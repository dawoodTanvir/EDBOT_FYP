{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1f711004dbf4764a6193fda5a363a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cf72d68eeea48faa628a2ec262114e3",
              "IPY_MODEL_7a3a318c6dec45b9951154884fa46531",
              "IPY_MODEL_0043736bb5064ae0b3c1c1a66c0ff4b0"
            ],
            "layout": "IPY_MODEL_12a2b332441d4d7c9d730c16d623bc85"
          }
        },
        "2cf72d68eeea48faa628a2ec262114e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215b78d4fb0c45c585c6ac0748ac0777",
            "placeholder": "​",
            "style": "IPY_MODEL_98967ce5b06d4278a55f5ca569cc8b2b",
            "value": "modules.json: 100%"
          }
        },
        "7a3a318c6dec45b9951154884fa46531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66c2f44d1f34df492cb69aea2849000",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e8a9c2a938744ce9412d9b47ae954c8",
            "value": 349
          }
        },
        "0043736bb5064ae0b3c1c1a66c0ff4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49348966f4e1445d9447f44ccf760982",
            "placeholder": "​",
            "style": "IPY_MODEL_fbc598de7a2840cfb2e775214f96f14a",
            "value": " 349/349 [00:00&lt;00:00, 24.2kB/s]"
          }
        },
        "12a2b332441d4d7c9d730c16d623bc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215b78d4fb0c45c585c6ac0748ac0777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98967ce5b06d4278a55f5ca569cc8b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66c2f44d1f34df492cb69aea2849000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8a9c2a938744ce9412d9b47ae954c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49348966f4e1445d9447f44ccf760982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc598de7a2840cfb2e775214f96f14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "309e5805a8e642f490cf1947e0c6029c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d2397110d6848d08d3bc1a912bdca21",
              "IPY_MODEL_47f93f728f00485ea35730248370440f",
              "IPY_MODEL_e679faccae7843c88967ca2991ad1b55"
            ],
            "layout": "IPY_MODEL_4902196d6a8649f987bb2c5daf9181d3"
          }
        },
        "9d2397110d6848d08d3bc1a912bdca21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aa6a676e0f9472a9e4902a8541755ff",
            "placeholder": "​",
            "style": "IPY_MODEL_5fb423d118824557999f775a504ec907",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "47f93f728f00485ea35730248370440f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6120b2b3d46c4600a14f15b219284ce4",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_203c42f4274e4166b3ecda8cf9c585a4",
            "value": 116
          }
        },
        "e679faccae7843c88967ca2991ad1b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270d03e97a7f41879cbe9f9aa7879129",
            "placeholder": "​",
            "style": "IPY_MODEL_806bdfc9c68d435dac3bf52806b3cfe5",
            "value": " 116/116 [00:00&lt;00:00, 9.48kB/s]"
          }
        },
        "4902196d6a8649f987bb2c5daf9181d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aa6a676e0f9472a9e4902a8541755ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb423d118824557999f775a504ec907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6120b2b3d46c4600a14f15b219284ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203c42f4274e4166b3ecda8cf9c585a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "270d03e97a7f41879cbe9f9aa7879129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806bdfc9c68d435dac3bf52806b3cfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f152067103804bfc85351b21bf53b0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fcfec5335af45cba1fc675111d056e8",
              "IPY_MODEL_a20d910f383042639bb3e575d5866c47",
              "IPY_MODEL_105a7bc0f45644bc86e75264c4dea2cf"
            ],
            "layout": "IPY_MODEL_357b0ff2da5f464c89e27a99c80d36fb"
          }
        },
        "2fcfec5335af45cba1fc675111d056e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd374ca946d478296dced82e10e50ff",
            "placeholder": "​",
            "style": "IPY_MODEL_899d4a3edffe47feb54efba947f15340",
            "value": "README.md: 100%"
          }
        },
        "a20d910f383042639bb3e575d5866c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ddf560e7ea046ceabe3cf054146c938",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08ddc522129240629e381a1d7053d6fb",
            "value": 10659
          }
        },
        "105a7bc0f45644bc86e75264c4dea2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d256c034a94a489939b2faea7abf90",
            "placeholder": "​",
            "style": "IPY_MODEL_da9afdbd14f442228f4afdad78398c73",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 789kB/s]"
          }
        },
        "357b0ff2da5f464c89e27a99c80d36fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd374ca946d478296dced82e10e50ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899d4a3edffe47feb54efba947f15340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ddf560e7ea046ceabe3cf054146c938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ddc522129240629e381a1d7053d6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44d256c034a94a489939b2faea7abf90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9afdbd14f442228f4afdad78398c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9853202c2954707811a68b34b63d063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d3d1ecbf7e64732a9199ebef9746aef",
              "IPY_MODEL_bfee9fcbef6a45d1a8d67fe20d9b26c7",
              "IPY_MODEL_4a216f4e082e4dabad730d8c523f0508"
            ],
            "layout": "IPY_MODEL_c8651b00881a4ee4b3400c519a1e5a00"
          }
        },
        "9d3d1ecbf7e64732a9199ebef9746aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98985e5d21404f8cb51738e47749b451",
            "placeholder": "​",
            "style": "IPY_MODEL_47ce99461b54450f90c0d874bd578d5b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "bfee9fcbef6a45d1a8d67fe20d9b26c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f40a7c03944e98924ae4c804a355aa",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05f3e5e319c24e278c31dbe470cf131f",
            "value": 53
          }
        },
        "4a216f4e082e4dabad730d8c523f0508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf10070b51c4a95a5d93c56df5e911a",
            "placeholder": "​",
            "style": "IPY_MODEL_3a7c20fe1aeb40118aa5ba29f9cd4208",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.33kB/s]"
          }
        },
        "c8651b00881a4ee4b3400c519a1e5a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98985e5d21404f8cb51738e47749b451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ce99461b54450f90c0d874bd578d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7f40a7c03944e98924ae4c804a355aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f3e5e319c24e278c31dbe470cf131f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcf10070b51c4a95a5d93c56df5e911a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7c20fe1aeb40118aa5ba29f9cd4208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b2dc31597a414c91e2229e2702de24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0521573a21f44558bd42c9ea3c5eddc5",
              "IPY_MODEL_59981e2dca1a4ed3a4eee56930c476e4",
              "IPY_MODEL_fcdb0b6698754dccb2b086ee0d631779"
            ],
            "layout": "IPY_MODEL_3df8f3ba62314c55a29408e867546334"
          }
        },
        "0521573a21f44558bd42c9ea3c5eddc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38c0da6a030b475db60ea15d69f19241",
            "placeholder": "​",
            "style": "IPY_MODEL_7f47440c215640bbaee1e4c176f218ee",
            "value": "config.json: 100%"
          }
        },
        "59981e2dca1a4ed3a4eee56930c476e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1006c1af4484574887499ab69c8173e",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7642e6513aa64ebdaa33f39861f17838",
            "value": 612
          }
        },
        "fcdb0b6698754dccb2b086ee0d631779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21507884d8a44402ad3c071a38f203b7",
            "placeholder": "​",
            "style": "IPY_MODEL_f716ebcaab23478987e8d61c9391ecc4",
            "value": " 612/612 [00:00&lt;00:00, 32.5kB/s]"
          }
        },
        "3df8f3ba62314c55a29408e867546334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c0da6a030b475db60ea15d69f19241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f47440c215640bbaee1e4c176f218ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1006c1af4484574887499ab69c8173e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7642e6513aa64ebdaa33f39861f17838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21507884d8a44402ad3c071a38f203b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f716ebcaab23478987e8d61c9391ecc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37db0e69cb014e09a7c443d966c80640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22917e29a70e42888aab87696f57d1ba",
              "IPY_MODEL_f66b6daff41a4134abae682318885c19",
              "IPY_MODEL_9cffa5a9871344bd85ef1166e8c3f835"
            ],
            "layout": "IPY_MODEL_880f1cee9baa4c49a85d67ba7b5894ed"
          }
        },
        "22917e29a70e42888aab87696f57d1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11ff6abf8d9c470d9d93516869fdcc6c",
            "placeholder": "​",
            "style": "IPY_MODEL_12f5a25fbd38445386a23e67da9b2fbc",
            "value": "model.safetensors: 100%"
          }
        },
        "f66b6daff41a4134abae682318885c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a21778d0cf942cc8fdee8600ae6c3d5",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68c1b23eb950498f918af3cd56e354d1",
            "value": 90868376
          }
        },
        "9cffa5a9871344bd85ef1166e8c3f835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a3d0ce106cc4177a6c8a1dc4491a8fe",
            "placeholder": "​",
            "style": "IPY_MODEL_0e06dd1fa91141b3bc5b580965273c8b",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 185MB/s]"
          }
        },
        "880f1cee9baa4c49a85d67ba7b5894ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11ff6abf8d9c470d9d93516869fdcc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f5a25fbd38445386a23e67da9b2fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a21778d0cf942cc8fdee8600ae6c3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68c1b23eb950498f918af3cd56e354d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a3d0ce106cc4177a6c8a1dc4491a8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e06dd1fa91141b3bc5b580965273c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "490bd349289e476ea9a3bbcac8253a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff57f0c619af43d7b43f3f45b46b51ca",
              "IPY_MODEL_e63c846a45784507bec41419a6830dce",
              "IPY_MODEL_964cdb1a4da24cd2b6f94621d38463ed"
            ],
            "layout": "IPY_MODEL_971c5a9ab747433b90e56699a2d70934"
          }
        },
        "ff57f0c619af43d7b43f3f45b46b51ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0328eb14e3734118b01bf770f46a73ba",
            "placeholder": "​",
            "style": "IPY_MODEL_df698bf0ada0436283d0354a1670f604",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e63c846a45784507bec41419a6830dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897dfe4e83e243aeb2d6dce96f5b4059",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ee5de77ffdf4b6e92b87522607aee82",
            "value": 350
          }
        },
        "964cdb1a4da24cd2b6f94621d38463ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7da91d7d3714507b64dfbc9ff36000e",
            "placeholder": "​",
            "style": "IPY_MODEL_2c72cee3b19a49689bfc3ad209307a5d",
            "value": " 350/350 [00:00&lt;00:00, 17.5kB/s]"
          }
        },
        "971c5a9ab747433b90e56699a2d70934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0328eb14e3734118b01bf770f46a73ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df698bf0ada0436283d0354a1670f604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "897dfe4e83e243aeb2d6dce96f5b4059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee5de77ffdf4b6e92b87522607aee82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7da91d7d3714507b64dfbc9ff36000e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c72cee3b19a49689bfc3ad209307a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0b82cc5b7f442bbc45556f8d8a2113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3c62670924e4015ab7b07e62e29a3c4",
              "IPY_MODEL_21b700d8a0694284a3d1e934f660d461",
              "IPY_MODEL_36e8a8c9b6c640a085684a4115d3cdeb"
            ],
            "layout": "IPY_MODEL_2c0367d19a6443c0a9c8bf81ffbe8011"
          }
        },
        "e3c62670924e4015ab7b07e62e29a3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f97afedfdc4049901a7787f85692f2",
            "placeholder": "​",
            "style": "IPY_MODEL_e718625da1474f7ca14bc0701d439f18",
            "value": "vocab.txt: 100%"
          }
        },
        "21b700d8a0694284a3d1e934f660d461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9f7f09ce1448cd8749f51a71945b45",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a013ce4448334b6a838bf59740e4a4b0",
            "value": 231508
          }
        },
        "36e8a8c9b6c640a085684a4115d3cdeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b975fce18117400f8a52d527516e3fc3",
            "placeholder": "​",
            "style": "IPY_MODEL_9193735399ce42368fc2bf9333b40340",
            "value": " 232k/232k [00:00&lt;00:00, 3.61MB/s]"
          }
        },
        "2c0367d19a6443c0a9c8bf81ffbe8011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f97afedfdc4049901a7787f85692f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e718625da1474f7ca14bc0701d439f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d9f7f09ce1448cd8749f51a71945b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a013ce4448334b6a838bf59740e4a4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b975fce18117400f8a52d527516e3fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9193735399ce42368fc2bf9333b40340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4b3409a5614b059fa9e9c97b98af2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7383d84003ce427b83ac72137ad9cc06",
              "IPY_MODEL_aaf49df2a65e4adbbd9fdfb868d6881c",
              "IPY_MODEL_143591e31903455aa3a0cf50aa7366ae"
            ],
            "layout": "IPY_MODEL_d18fc4f90dc043209da08c8281e11699"
          }
        },
        "7383d84003ce427b83ac72137ad9cc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_072781b3340945fb817a7140007fd54b",
            "placeholder": "​",
            "style": "IPY_MODEL_0e7758e40230485fad63fb2f14bb1e8e",
            "value": "tokenizer.json: 100%"
          }
        },
        "aaf49df2a65e4adbbd9fdfb868d6881c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3d6f72950d495d83e72db2e9fa50d2",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_868978f73bab462692a9d47b078cf72d",
            "value": 466247
          }
        },
        "143591e31903455aa3a0cf50aa7366ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285eca17d1104c318233ba769d2a9e17",
            "placeholder": "​",
            "style": "IPY_MODEL_ab268a7ae6df4d4398efa91a489b6be1",
            "value": " 466k/466k [00:00&lt;00:00, 4.72MB/s]"
          }
        },
        "d18fc4f90dc043209da08c8281e11699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "072781b3340945fb817a7140007fd54b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7758e40230485fad63fb2f14bb1e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3d6f72950d495d83e72db2e9fa50d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868978f73bab462692a9d47b078cf72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "285eca17d1104c318233ba769d2a9e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab268a7ae6df4d4398efa91a489b6be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "802719359172416782073ab7d616dba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a6b712ae65342deb40b997ac742ba05",
              "IPY_MODEL_fccf75af54e5492480fe5ad53372f62c",
              "IPY_MODEL_eea91b0b5f2c4926b2ad68d264a6e8ee"
            ],
            "layout": "IPY_MODEL_7661d6e9c3ad4144b94fc4b2163792fb"
          }
        },
        "2a6b712ae65342deb40b997ac742ba05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c28f5cb5096f4924b8e94785d4866d55",
            "placeholder": "​",
            "style": "IPY_MODEL_e5ab1e88d9c241439164535a6835647c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "fccf75af54e5492480fe5ad53372f62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4dd2925495c470981f4bd9e76523295",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94b647cec58c42a683c655a0a0fbdef4",
            "value": 112
          }
        },
        "eea91b0b5f2c4926b2ad68d264a6e8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9830f20fcf749caa94c278cbcc2ef49",
            "placeholder": "​",
            "style": "IPY_MODEL_a0fe31fe3ae74fb3a49f2db70eddf7fb",
            "value": " 112/112 [00:00&lt;00:00, 9.19kB/s]"
          }
        },
        "7661d6e9c3ad4144b94fc4b2163792fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28f5cb5096f4924b8e94785d4866d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ab1e88d9c241439164535a6835647c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4dd2925495c470981f4bd9e76523295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b647cec58c42a683c655a0a0fbdef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9830f20fcf749caa94c278cbcc2ef49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fe31fe3ae74fb3a49f2db70eddf7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64ff6a5b881b412a9b6a6a6003ba5229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb3b561a5f0d4d0a94125ee57307689c",
              "IPY_MODEL_71e23df4cbd641feb6caf207e53a2407",
              "IPY_MODEL_73b07e3fc0f1402996799d0da54e7cec"
            ],
            "layout": "IPY_MODEL_2cc352e861624ec2b67a83c7f7753068"
          }
        },
        "cb3b561a5f0d4d0a94125ee57307689c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad6067e6a234c059796f3df40675d46",
            "placeholder": "​",
            "style": "IPY_MODEL_2b67b0056ff7457ab200684319ee5290",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "71e23df4cbd641feb6caf207e53a2407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14556b1d828c499a90d662244413c333",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6f8951c6a0148269a66da025e7f1ccb",
            "value": 190
          }
        },
        "73b07e3fc0f1402996799d0da54e7cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a0e0dc227e24a6c95b2c3b9fb5a1df7",
            "placeholder": "​",
            "style": "IPY_MODEL_277718703432423995609c0b085488f0",
            "value": " 190/190 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "2cc352e861624ec2b67a83c7f7753068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad6067e6a234c059796f3df40675d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b67b0056ff7457ab200684319ee5290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14556b1d828c499a90d662244413c333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f8951c6a0148269a66da025e7f1ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a0e0dc227e24a6c95b2c3b9fb5a1df7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "277718703432423995609c0b085488f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dawoodTanvir/EDBOT_FYP/blob/main/FYP_Content_Slides_Quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e1f711004dbf4764a6193fda5a363a7f",
            "2cf72d68eeea48faa628a2ec262114e3",
            "7a3a318c6dec45b9951154884fa46531",
            "0043736bb5064ae0b3c1c1a66c0ff4b0",
            "12a2b332441d4d7c9d730c16d623bc85",
            "215b78d4fb0c45c585c6ac0748ac0777",
            "98967ce5b06d4278a55f5ca569cc8b2b",
            "f66c2f44d1f34df492cb69aea2849000",
            "7e8a9c2a938744ce9412d9b47ae954c8",
            "49348966f4e1445d9447f44ccf760982",
            "fbc598de7a2840cfb2e775214f96f14a",
            "309e5805a8e642f490cf1947e0c6029c",
            "9d2397110d6848d08d3bc1a912bdca21",
            "47f93f728f00485ea35730248370440f",
            "e679faccae7843c88967ca2991ad1b55",
            "4902196d6a8649f987bb2c5daf9181d3",
            "9aa6a676e0f9472a9e4902a8541755ff",
            "5fb423d118824557999f775a504ec907",
            "6120b2b3d46c4600a14f15b219284ce4",
            "203c42f4274e4166b3ecda8cf9c585a4",
            "270d03e97a7f41879cbe9f9aa7879129",
            "806bdfc9c68d435dac3bf52806b3cfe5",
            "f152067103804bfc85351b21bf53b0c4",
            "2fcfec5335af45cba1fc675111d056e8",
            "a20d910f383042639bb3e575d5866c47",
            "105a7bc0f45644bc86e75264c4dea2cf",
            "357b0ff2da5f464c89e27a99c80d36fb",
            "2bd374ca946d478296dced82e10e50ff",
            "899d4a3edffe47feb54efba947f15340",
            "0ddf560e7ea046ceabe3cf054146c938",
            "08ddc522129240629e381a1d7053d6fb",
            "44d256c034a94a489939b2faea7abf90",
            "da9afdbd14f442228f4afdad78398c73",
            "e9853202c2954707811a68b34b63d063",
            "9d3d1ecbf7e64732a9199ebef9746aef",
            "bfee9fcbef6a45d1a8d67fe20d9b26c7",
            "4a216f4e082e4dabad730d8c523f0508",
            "c8651b00881a4ee4b3400c519a1e5a00",
            "98985e5d21404f8cb51738e47749b451",
            "47ce99461b54450f90c0d874bd578d5b",
            "f7f40a7c03944e98924ae4c804a355aa",
            "05f3e5e319c24e278c31dbe470cf131f",
            "bcf10070b51c4a95a5d93c56df5e911a",
            "3a7c20fe1aeb40118aa5ba29f9cd4208",
            "04b2dc31597a414c91e2229e2702de24",
            "0521573a21f44558bd42c9ea3c5eddc5",
            "59981e2dca1a4ed3a4eee56930c476e4",
            "fcdb0b6698754dccb2b086ee0d631779",
            "3df8f3ba62314c55a29408e867546334",
            "38c0da6a030b475db60ea15d69f19241",
            "7f47440c215640bbaee1e4c176f218ee",
            "a1006c1af4484574887499ab69c8173e",
            "7642e6513aa64ebdaa33f39861f17838",
            "21507884d8a44402ad3c071a38f203b7",
            "f716ebcaab23478987e8d61c9391ecc4",
            "37db0e69cb014e09a7c443d966c80640",
            "22917e29a70e42888aab87696f57d1ba",
            "f66b6daff41a4134abae682318885c19",
            "9cffa5a9871344bd85ef1166e8c3f835",
            "880f1cee9baa4c49a85d67ba7b5894ed",
            "11ff6abf8d9c470d9d93516869fdcc6c",
            "12f5a25fbd38445386a23e67da9b2fbc",
            "1a21778d0cf942cc8fdee8600ae6c3d5",
            "68c1b23eb950498f918af3cd56e354d1",
            "6a3d0ce106cc4177a6c8a1dc4491a8fe",
            "0e06dd1fa91141b3bc5b580965273c8b",
            "490bd349289e476ea9a3bbcac8253a3b",
            "ff57f0c619af43d7b43f3f45b46b51ca",
            "e63c846a45784507bec41419a6830dce",
            "964cdb1a4da24cd2b6f94621d38463ed",
            "971c5a9ab747433b90e56699a2d70934",
            "0328eb14e3734118b01bf770f46a73ba",
            "df698bf0ada0436283d0354a1670f604",
            "897dfe4e83e243aeb2d6dce96f5b4059",
            "5ee5de77ffdf4b6e92b87522607aee82",
            "c7da91d7d3714507b64dfbc9ff36000e",
            "2c72cee3b19a49689bfc3ad209307a5d",
            "df0b82cc5b7f442bbc45556f8d8a2113",
            "e3c62670924e4015ab7b07e62e29a3c4",
            "21b700d8a0694284a3d1e934f660d461",
            "36e8a8c9b6c640a085684a4115d3cdeb",
            "2c0367d19a6443c0a9c8bf81ffbe8011",
            "04f97afedfdc4049901a7787f85692f2",
            "e718625da1474f7ca14bc0701d439f18",
            "4d9f7f09ce1448cd8749f51a71945b45",
            "a013ce4448334b6a838bf59740e4a4b0",
            "b975fce18117400f8a52d527516e3fc3",
            "9193735399ce42368fc2bf9333b40340",
            "2c4b3409a5614b059fa9e9c97b98af2f",
            "7383d84003ce427b83ac72137ad9cc06",
            "aaf49df2a65e4adbbd9fdfb868d6881c",
            "143591e31903455aa3a0cf50aa7366ae",
            "d18fc4f90dc043209da08c8281e11699",
            "072781b3340945fb817a7140007fd54b",
            "0e7758e40230485fad63fb2f14bb1e8e",
            "0b3d6f72950d495d83e72db2e9fa50d2",
            "868978f73bab462692a9d47b078cf72d",
            "285eca17d1104c318233ba769d2a9e17",
            "ab268a7ae6df4d4398efa91a489b6be1",
            "802719359172416782073ab7d616dba6",
            "2a6b712ae65342deb40b997ac742ba05",
            "fccf75af54e5492480fe5ad53372f62c",
            "eea91b0b5f2c4926b2ad68d264a6e8ee",
            "7661d6e9c3ad4144b94fc4b2163792fb",
            "c28f5cb5096f4924b8e94785d4866d55",
            "e5ab1e88d9c241439164535a6835647c",
            "a4dd2925495c470981f4bd9e76523295",
            "94b647cec58c42a683c655a0a0fbdef4",
            "a9830f20fcf749caa94c278cbcc2ef49",
            "a0fe31fe3ae74fb3a49f2db70eddf7fb",
            "64ff6a5b881b412a9b6a6a6003ba5229",
            "cb3b561a5f0d4d0a94125ee57307689c",
            "71e23df4cbd641feb6caf207e53a2407",
            "73b07e3fc0f1402996799d0da54e7cec",
            "2cc352e861624ec2b67a83c7f7753068",
            "8ad6067e6a234c059796f3df40675d46",
            "2b67b0056ff7457ab200684319ee5290",
            "14556b1d828c499a90d662244413c333",
            "e6f8951c6a0148269a66da025e7f1ccb",
            "6a0e0dc227e24a6c95b2c3b9fb5a1df7",
            "277718703432423995609c0b085488f0"
          ]
        },
        "id": "4MIwLXoUjga8",
        "outputId": "2abf2aef-8de8-4104-e5f7-a9a244f11e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/460.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 1s (215 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,843 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124956 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.2 python-pptx-1.0.2\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.11/dist-packages (from gensim==3.8.3) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from gensim==3.8.3) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from gensim==3.8.3) (1.17.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==3.8.3) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open>=1.8.1->gensim==3.8.3) (1.17.2)\n",
            "Building wheels for collected packages: gensim\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.12.14)\n",
            "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: keybert\n",
            "Successfully installed keybert-0.8.5\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Collecting google_images_download\n",
            "  Downloading google_images_download-2.8.0.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting selenium (from google_images_download)\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google_images_download) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium->google_images_download)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->google_images_download)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google_images_download) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google_images_download) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google_images_download) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google_images_download) (25.1.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->google_images_download)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google_images_download) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium->google_images_download)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google_images_download) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->google_images_download)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google_images_download) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google_images_download) (0.14.0)\n",
            "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: google_images_download\n",
            "  Building wheel for google_images_download (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google_images_download: filename=google_images_download-2.8.0-py2.py3-none-any.whl size=14537 sha256=9b06433f790f4971766e2ef1c6d16778e303d5a3d827656033e785c42a5c5a65\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/83/37/7303b15f3e8a5bfbd5c7ebbfe13f0c666ada6f8efecc6d77ec\n",
            "Successfully built google_images_download\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium, google_images_download\n",
            "Successfully installed google_images_download-2.8.0 outcome-1.3.0.post0 selenium-4.28.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2024.12.14)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32009 sha256=26fd0e2492edc58a6879e6d280586969d99d938df7fa04f217991e40a81179d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1f711004dbf4764a6193fda5a363a7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "309e5805a8e642f490cf1947e0c6029c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f152067103804bfc85351b21bf53b0c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9853202c2954707811a68b34b63d063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04b2dc31597a414c91e2229e2702de24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37db0e69cb014e09a7c443d966c80640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "490bd349289e476ea9a3bbcac8253a3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df0b82cc5b7f442bbc45556f8d8a2113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c4b3409a5614b059fa9e9c97b98af2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "802719359172416782073ab7d616dba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64ff6a5b881b412a9b6a6a6003ba5229"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "<ipython-input-1-f1bbd73c38d2>:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain openai  -q\n",
        "!pip install sentence_transformers -q\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install -U langchain-community -q\n",
        "!pip install pillow\n",
        "!pip install requests\n",
        "!pip install python-pptx\n",
        "!pip install gensim==3.8.3\n",
        "!pip install keybert\n",
        "!pip install requests Pillow\n",
        "!pip install google_images_download\n",
        "!pip install google-search-results\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt, Inches\n",
        "from PIL import Image\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import requests\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize KeyBERT\n",
        "from keybert import KeyBERT\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "!pip install pinecone-client -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import getpass\n",
        "import os\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
        "\n",
        "#074e0d9a-ab5e-48bf-8eae-9effae335521              This is the API to MYDB Insert this\n",
        "\n",
        "# Prompt for Pinecone API key if not set in the environment\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "# Retrieve the Pinecone API key from environment variables\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Define your index name\n",
        "index_name = \"newdata\"  # Change if desired\n",
        "\n",
        "# Check for existing indexes\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "# Create the index if it does not exist\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # Adjust this to match your embeddings' dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "    # Wait until the index is ready\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        print(\"Waiting for the index to be ready...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Print connection success message\n",
        "print(f\"Successfully connected to the index: {index_name}\")\n",
        "\n",
        "\n",
        "# Now create a Pinecone index for Langchain using the existing index\n",
        "langchain_index = LangchainPinecone.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        "\n",
        ")\n",
        "\n",
        "# Output to verify the index creation\n",
        "print(f\"Successfully created or connected to the Langchain index: {langchain_index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYu3NxcikRHQ",
        "outputId": "06104075-0c0a-45a2-88fc-dfc6d4418287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Pinecone API key: ··········\n",
            "Successfully connected to the index: newdata\n",
            "Successfully created or connected to the Langchain index: <langchain_community.vectorstores.pinecone.Pinecone object at 0x7e6b8aae1ad0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_docs(query, k=20, score=True):\n",
        "    if score:\n",
        "        similar_docs = langchain_index.similarity_search_with_score(query, k=k)  # Use langchain_index\n",
        "    else:\n",
        "        similar_docs = langchain_index.similarity_search(query, k=k)  # Use langchain_index\n",
        "    return similar_docs"
      ],
      "metadata": {
        "id": "-TBvgxFWkVR6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "aNic89lFgYnu",
        "outputId": "710f4b39-480e-46e2-b8e6-8cd76352aabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Answer from LLM"
      ],
      "metadata": {
        "id": "U3dDS6B-RNrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################  WITH GROQ #############################################################\n",
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "class GroqClient:\n",
        "    def __init__(self, api_key):\n",
        "        self.client = Groq(api_key=api_key)\n",
        "\n",
        "    def get_completion(self, query, max_tokens=2500):\n",
        "        prompt = self._construct_prompt(query)\n",
        "\n",
        "        try:\n",
        "            chat_completion = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a helpful AI assistant specialized in providing detailed technical explanations.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                model=\"mixtral-8x7b-32768\",\n",
        "                temperature=0.5,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=0.7,\n",
        "                stream=False\n",
        "            )\n",
        "            return chat_completion.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"API request failed: {str(e)}\")\n",
        "\n",
        "    def _construct_prompt(self, query):\n",
        "        return f'''\n",
        "Question: {query}\n",
        "\n",
        "Provide a comprehensive explanation strictly following this structure. Always use \"##\" to introduce each section:\n",
        "\n",
        "## 1. *Concept Overview*\n",
        "  • Core definition and purpose\n",
        "  • Key principles\n",
        "  • Relationship to broader computing concepts\n",
        "\n",
        "## 2. *Technical Components*\n",
        "  • Primary elements and their roles\n",
        "  • Relationships and interactions\n",
        "  • Implementation details\n",
        "  • Core algorithms/procedures (if applicable)\n",
        "\n",
        "## 3. *Working Mechanism*\n",
        "  • Step-by-step operational flow\n",
        "  • Critical processes and transformations\n",
        "  • Resource management (if applicable)\n",
        "  • Exception handling (if applicable)\n",
        "\n",
        "## 4. *Implementation Example*\n",
        "  • Use case scenario\n",
        "  • Code implementation or technical design\n",
        "  • Step-by-step execution\n",
        "  • Output analysis\n",
        "\n",
        "## 5. *Best Practices*\n",
        "  • Design considerations\n",
        "  • Optimization techniques\n",
        "  • Common pitfalls\n",
        "  • Performance implications\n",
        "\n",
        "## 6. *Applications*\n",
        "  • Real-world use cases\n",
        "  • Industry applications\n",
        "  • Integration patterns\n",
        "  • Variations and alternatives\n",
        "\n",
        "## 7. *Evaluation*\n",
        "  • Performance metrics\n",
        "  • Testing approaches\n",
        "  • Debugging strategies\n",
        "  • Optimization opportunities\n",
        "\n",
        "## 8. *Practice Problems*\n",
        "  • Concept verification questions\n",
        "  • Implementation challenges\n",
        "  • Problem-solving scenarios\n",
        "  • Solutions with explanations\n",
        "\n",
        "### Format Requirements:\n",
        "- Always use \"##\" to introduce each section header. Avoid using any other format.\n",
        "- Use bullet points for clarity\n",
        "- Show all mathematical steps using proper equation formatting ($...$)\n",
        "- Include clear variable definitions after each equation\n",
        "- Write formulas using LaTeX formatting inside $...$\n",
        "- For matrices use: $\\begin{{bmatrix}} a & b \\\\ c & d \\end{{bmatrix}}$\n",
        "- For fractions use: $\\frac{{numerator}}{{denominator}}$\n",
        "- Use ■ for numbered equations and • for regular points\n",
        "- Demonstrate practical interpretation\n",
        "- Connect to real applications\n",
        "\n",
        "### Equation Guidelines:\n",
        "- Enclose equations in $...$ format\n",
        "- Use proper LaTeX notation for mathematical expressions\n",
        "- Use $\\sum$ for summations\n",
        "- Define each variable after presenting equations\n",
        "- Number important equations using ■\n",
        "- Show step-by-step derivations with clear explanations\n",
        "\n",
        "### Code Guidelines:\n",
        "- Use LaTeX verbatim environment for code blocks:\n",
        "  \\begin{{verbatim}}\n",
        "  code here\n",
        "  \\end{{verbatim}}\n",
        "\n",
        "- For inline code use \\texttt{{code}}\n",
        "- For syntax highlighting:\n",
        "  \\begin{{lstlisting}}[language=Python]\n",
        "  code here\n",
        "  \\end{{lstlisting}}\n",
        "- Include comments explaining code functionality\n",
        "- Show output examples where applicable\n",
        "'''\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # api_key = os.environ.get(\"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\")\n",
        "    # client = GroqClient(api_key)\n",
        "\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    client = GroqClient(api_key)\n",
        "    query = \"LSTM\"\n",
        "    try:\n",
        "        answer = client.get_completion(query)\n",
        "        print(\"Answer:\", answer)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", str(e))"
      ],
      "metadata": {
        "id": "zCZoCFqmhMLn",
        "outputId": "ec5592ee-1bb1-4074-d207-98555aa41302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ## 1. Concept Overview\n",
            "  • Core definition and purpose: Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to handle sequential data by learning long-term dependencies between input elements. LSTM addresses the vanishing gradient problem in traditional RNNs, allowing them to capture complex temporal patterns.\n",
            "\n",
            "  • Key principles: LSTM maintains a cell state, a vector of information that flows through the entire chain of LSTM units. Gates, which are neural network layers with sigmoid activation functions, control the flow of information into and out of the cell state. These gates include the input, forget, and output gates.\n",
            "\n",
            "  • Relationship to broader computing concepts: LSTM is a specialized form of RNN, which is a type of neural network for processing sequential data. RNNs are particularly useful for tasks like time series prediction, natural language processing, and speech recognition.\n",
            "\n",
            "## 2. Technical Components\n",
            "  • Primary elements and their roles:\n",
            "    - Cell state: A vector of information that flows through the LSTM chain.\n",
            "    - Input gate: Controls the input flow into the cell state.\n",
            "    - Forget gate: Controls the removal of information from the cell state.\n",
            "    - Output gate: Controls the output flow from the cell state.\n",
            "\n",
            "  • Relationships and interactions: The input, forget, and output gates determine which information to store, discard, or use from the cell state. These gates are learned during training.\n",
            "\n",
            "  • Implementation details: LSTM units typically have a tanh activation function for the cell state and sigmoid activation functions for the gates. The weights and biases for these functions are learned during training.\n",
            "\n",
            "  • Core algorithms/procedures: LSTM training involves backpropagation through time (BPTT), a variant of gradient descent that unrolls the RNN over time and calculates gradients for each time step.\n",
            "\n",
            "## 3. Working Mechanism\n",
            "  • Step-by-step operational flow:\n",
            "    1. Calculate input, forget, and output gate activations using sigmoid functions.\n",
            "    2. Calculate candidate values for the cell state using a tanh function.\n",
            "    3. Update the cell state by multiplying it with the forget gate activation, adding the candidate values multiplied by the input gate activation.\n",
            "    4. Calculate the hidden state by multiplying the cell state with the output gate activation and passing it through a tanh function.\n",
            "\n",
            "  • Critical processes and transformations: The primary transformation in LSTM is the update of the cell state, which is controlled by the input and forget gates.\n",
            "\n",
            "  • Resource management (if applicable): LSTM units manage their own internal state, the cell state, and the gate activations.\n",
            "\n",
            "  • Exception handling (if applicable): LSTM does not explicitly handle exceptions. However, it can learn to ignore or emphasize certain inputs based on the task at hand.\n",
            "\n",
            "## 4. Implementation Example\n",
            "  • Use case scenario: Predicting the next word in a sentence.\n",
            "\n",
            "  • Code implementation or technical design:\n",
            "    \begin{verbatim}\n",
            "    class LSTMCell(nn.Module):\n",
            "        def __init__(self, input_size, hidden_size):\n",
            "            super(LSTMCell, self).__init__()\n",
            "            self.input_size = input_size\n",
            "            self.hidden_size = hidden_size\n",
            "            self.i_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
            "            self.f_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
            "            self.o_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
            "            self.c_candidate = nn.Linear(input_size + hidden_size, hidden_size)\n",
            "    \n",
            "        def forward(self, input, hidden):\n",
            "            combined = torch.cat((input, hidden), dim=1)\n",
            "            i_gate = torch.sigmoid(self.i_gate(combined))\n",
            "            f_gate = torch.sigmoid(self.f_gate(combined))\n",
            "            o_gate = torch.sigmoid(self.o_gate(combined))\n",
            "            c_candidate = torch.tanh(self.c_candidate(combined))\n",
            "            c_new = f_gate * hidden + i_gate * c_candidate\n",
            "            h_new = o_gate * torch.tanh(c_new)\n",
            "            return h_new, c_new\n",
            "    \n",
            "    class LSTM(nn.Module):\n",
            "        def __init__(self, input_size, hidden_size, num_layers):\n",
            "            super(LSTM, self).__init__()\n",
            "            self.hidden_size = hidden_size\n",
            "            self.num_layers = num_layers\n",
            "            self.cells = nn.ModuleList([LSTMCell(input_size, hidden_size) for _ in range(num_layers)])\n",
            "    \n",
            "        def forward(self, input):\n",
            "            hiddens = []\n",
            "            c_t = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
            "            for cell in self.cells:\n",
            "                h_t, c_t = cell(input, c_t)\n",
            "                hiddens.append(h_t)\n",
            "                input = h_t\n",
            "            return hiddens\n",
            "    \n",
            "    input_size = 10\n",
            "    hidden_size = 20\n",
            "    num_layers = 2\n",
            "    lstm = LSTM(input_size, hidden_size, num_layers)\n",
            "    \n",
            "    input = torch.randn(5, input_size)\n",
            "    hiddens = lstm(input)\n",
            "    \n",
            "    print(hiddens)\n",
            "    \n",
            "    \\end{verbatim}\n",
            "\n",
            "  • Step-by-step execution: The LSTM model takes an input tensor and calculates the hidden state for each LSTM cell.\n",
            "\n",
            "  • Output analysis: The output is a list of hidden states for each LSTM cell.\n",
            "\n",
            "## 5. Best Practices\n",
            "  • Design considerations: Choose the number of LSTM layers and hidden units based on the complexity of the task and the available data.\n",
            "\n",
            "  • Optimization techniques: Use learning rate scheduling, weight decay, and gradient clipping during training.\n",
            "\n",
            "  • Common pitfalls: Avoid overfitting by using regularization techniques and monitoring the validation loss.\n",
            "\n",
            "  • Performance implications: LSTM can be computationally expensive for large sequences and hidden state sizes.\n",
            "\n",
            "## 6. Applications\n",
            "  • Real-world use cases: Time series forecasting, natural language processing, speech recognition.\n",
            "\n",
            "  • Industry applications: Finance, healthcare, manufacturing, and transportation.\n",
            "\n",
            "  • Integration patterns: LSTM can be integrated into deep learning models as a building block for processing sequential data.\n",
            "\n",
            "  • Variations and alternatives: Gated Recurrent Units (GRU), Simple Recurrent Units (SRU), and other variants of RNNs.\n",
            "\n",
            "## 7. Evaluation\n",
            "  • Performance metrics: Accuracy, precision, recall, F1 score, perplexity, and mean squared error.\n",
            "\n",
            "  • Testing approaches: Cross-validation, time series split, and Monte Carlo cross-validation.\n",
            "\n",
            "  • Debugging strategies: Visualize the cell state and gate activations, monitor the training and validation loss.\n",
            "\n",
            "  • Optimization opportunities: Use parallel processing, GPU acceleration, and model pruning.\n",
            "\n",
            "## 8. Practice Problems\n",
            "  • Concept verification questions: What is the purpose of the cell state in LSTM? How do gates control the flow of information in LSTM?\n",
            "\n",
            "  • Implementation challenges: Implement a multi-layered LSTM model for time series forecasting.\n",
            "\n",
            "  • Problem-solving scenarios: Train an LSTM model for sentiment analysis on a movie review dataset.\n",
            "\n",
            "  • Solutions with explanations: Implement a unidirectional and bidirectional LSTM model for sequence tagging tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Set the Hugging Face API key directly inside the script\n",
        "HUGGINGFACE_API_TOKEN = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Replace with your chosen model\n",
        "\n",
        "# Construct the API URL\n",
        "HUGGINGFACE_API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "\n",
        "# Set up the headers with the authorization token\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HUGGINGFACE_API_TOKEN}\"\n",
        "}\n",
        "\n",
        "# Example query\n",
        "query = \"LSTM\"\n",
        "\n",
        "# Assuming you have a function `get_similar_docs` defined\n",
        "similar_docs = get_similar_docs(query)\n",
        "\n",
        "# Prepare the context from similar_docs\n",
        "context = \"\\n\\n\".join([doc[0].page_content for doc in similar_docs])\n",
        "\n",
        "prompt = f'''\n",
        "Context: {context}\n",
        "Question: {query}\n",
        "\n",
        "Provide a comprehensive explanation strictly following this structure. Always use \"##\" to introduce each section:\n",
        "\n",
        "## 1. *Concept Overview*\n",
        "  • Core definition and purpose\n",
        "  • Key principles\n",
        "  • Relationship to broader computing concepts\n",
        "\n",
        "## 2. *Technical Components*\n",
        "  • Primary elements and their roles\n",
        "  • Relationships and interactions\n",
        "  • Implementation details\n",
        "  • Core algorithms/procedures (if applicable)\n",
        "\n",
        "## 3. *Working Mechanism*\n",
        "  • Step-by-step operational flow\n",
        "  • Critical processes and transformations\n",
        "  • Resource management (if applicable)\n",
        "  • Exception handling (if applicable)\n",
        "\n",
        "## 4. *Implementation Example*\n",
        "  • Use case scenario\n",
        "  • Code implementation or technical design\n",
        "  • Step-by-step execution\n",
        "  • Output analysis\n",
        "\n",
        "## 5. *Best Practices*\n",
        "  • Design considerations\n",
        "  • Optimization techniques\n",
        "  • Common pitfalls\n",
        "  • Performance implications\n",
        "\n",
        "## 6. *Applications*\n",
        "  • Real-world use cases\n",
        "  • Industry applications\n",
        "  • Integration patterns\n",
        "  • Variations and alternatives\n",
        "\n",
        "## 7. *Evaluation*\n",
        "  • Performance metrics\n",
        "  • Testing approaches\n",
        "  • Debugging strategies\n",
        "  • Optimization opportunities\n",
        "\n",
        "## 8. *Practice Problems*\n",
        "  • Concept verification questions\n",
        "  • Implementation challenges\n",
        "  • Problem-solving scenarios\n",
        "  • Solutions with explanations\n",
        "\n",
        "### Format Requirements:\n",
        "- Always use \"##\" to introduce each section header. Avoid using any other format.\n",
        "- Use bullet points for clarity\n",
        "- Show all mathematical steps using proper equation formatting ($...$)\n",
        "- Include clear variable definitions after each equation\n",
        "- Write formulas using LaTeX formatting inside $...$\n",
        "- For matrices use: $\\begin{{bmatrix}} a & b \\\\ c & d \\end{{bmatrix}}$\n",
        "- For fractions use: $\\frac{{numerator}}{{denominator}}$\n",
        "- Use ■ for numbered equations and • for regular points\n",
        "- Demonstrate practical interpretation\n",
        "- Connect to real applications\n",
        "\n",
        "### Equation Guidelines:\n",
        "- Enclose equations in $...$ format\n",
        "- Use proper LaTeX notation for mathematical expressions\n",
        "- Use $\\sum$ for summations\n",
        "- Define each variable after presenting equations\n",
        "- Number important equations using ■\n",
        "- Show step-by-step derivations with clear explanations\n",
        "\n",
        "### Code Guidelines:\n",
        "- Use LaTeX verbatim environment for code blocks:\n",
        "  \\begin{{verbatim}}\n",
        "  code here\n",
        "  \\end{{verbatim}}\n",
        "\n",
        "- For inline code use \\texttt{{code}}\n",
        "- For syntax highlighting:\n",
        "  \\begin{{lstlisting}}[language=Python]\n",
        "  code here\n",
        "  \\end{{lstlisting}}\n",
        "- Include comments explaining code functionality\n",
        "- Show output examples where applicable\n",
        "'''\n",
        "\n",
        "\n",
        "# Function to query Hugging Face Inference API\n",
        "def query_huggingface_api(prompt, max_length=25000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.5,\n",
        "            \"top_p\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "     }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        #print(response.json())\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "# Function to extract answer from API response\n",
        "def extract_answer(api_response):\n",
        "    if isinstance(api_response, list) and len(api_response) > 0:\n",
        "        generated_text = api_response[0].get('generated_text', '')\n",
        "\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip() if \"Answer:\" in generated_text else generated_text.strip()\n",
        "        if len(answer) > 30000:\n",
        "            answer = answer[:30000] + \"...\"\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No answer generated.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate the answer using Hugging Face Inference API\n",
        "try:\n",
        "    api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "    answer = extract_answer(api_response)\n",
        "    print(\"Answer:\", answer)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIe7L4Okkdrd",
        "outputId": "dc800bf1-cc71-418b-d637-6e3d15d4a6b2",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Context: BIBLIOGRAPHY  Graves, . . Sequence transduction  recurrent neural networks.   . Graves, .  . Jaitly . Towards  speech recognition  recurrent neural networks.   , . . Graves, .  . Schmidhuber . Framewise phoneme classiﬁcation  bidirec tional   other neural network architectures. Neural Networks  , .\n",
            "\n",
            "  vector hmthis information  represented.   limitation   informa   attenuated  repeated application   squashing function . short memories LSTMs, described below,   variant    address  issue,   memory cells  propagate information through  sequence without applying  linearities Hochreiter  Schmidhuber, .  denominator  Equation .   computational bottleneck, because  involves\n",
            "\n",
            "Recurrent neural networks   introduced     language model  technique,  which  context  token  summarized   recurrentlyupdated vector, ,,   ,,..., wherexmis  vector embedding   token wmand  function gdeﬁnes  recur rence.  starting condition   additional parameter   model.   short  memory     complex recurrence,  which  memory   through \n",
            "\n",
            " . LANGUAGE MODELS              Figure .   short memory  architecture. Gates  shown  boxes  dotted edges.    language model,  hmwould    predict   wordwm.  gates  functions   input  previous hidden state.   computed  elementwise sigmoid activations,   , ensuring  their values\n",
            "\n",
            "BIBLIOGRAPHY  , .  . Bansal .  relation extraction using lstms  sequences   structures.   , . . , .  . Hinton . Three  graphical models  statistical language  elling.  Proceedings   International Conference  Machine Learning  , .  . , .  . . Hinton .  scalable hierarchical distributed language model.  Neural Information Processing Systems  , . .\n",
            "\n",
            "sentations  vector space.  Proceedings  International Conference  Learning Represen tations . Mikolov, ., . Deoras, . Povey, . Burget,  . Cernocky . Strategies  training large scale neural network language models.  Proceedings   Workshop  Automatic Speech Recognition  Understanding  , . . Mikolov, ., . Karaﬁ , . Burget, . Cernock ,  . Khudanpur . Recurrent neural network based language model.  INTERSPEECH , . .\n",
            "\n",
            ".. Convolutional Neural Networks  Sequence Labeling  disadvantage  recurrent neural networks    architecture requires iterating through  sequence  inputs  predictions  hidden vector hmmust   puted   previous hidden vector , before predicting   . These iterative computations  difﬁcult  parallelize,    exploit  speedups offered  graph  processing units   operations   matrix multiplication. Convolutional\n",
            "\n",
            " baseline   implemented   neural architecture, using  attention  anism .., which scores  similarity   query      source    ., .      encode  passage   query , using  bidirectional LSTMs  .. BiLSTM   . BiLSTM  . .  query  represented  vertically concatenating   states   right  right passes \n",
            "\n",
            ".. RECURRENT NEURAL NETWORK LANGUAGE MODELS  derivatives automatically,  cache   future .  important distinction   feedforward neural networks considered         computa  graph   ,  varies   length   input.  poses difﬁculties  toolkits   designed around static computation graphs,   TensorFlow Abadi  ., . .. Hyperparameters\n",
            "\n",
            " incorporating  inner product   approximation   likelihood    ,   possible  estimate  parameters  backpropagation.   Mikolov  .,  includes   approximations continuous words   skipgrams. .. Continuous words   recurrent neural network language models,    conditioned   recurrently updated state vector, which  based   representations going      \n",
            "\n",
            ".. RECURRENT NEURAL NETWORK LANGUAGE MODELS    sequence.  language models  deﬁned,  .  , . ,,...,  , . whereφis  matrix   embeddings , andxmdenotes  embedding   .  conversion  wmtoxmis sometimes known   lookup layer , because  simply lookup  embeddings      table  ...  Elman  deﬁnes  simple recurrent operation Elman, ,\n",
            "\n",
            "whereδ   indicator function, taking  value      record   identical   target  .  probability  copying record rfrom  source ,  product    probability   local attention.     model,  attention weights αmare computed   previous decoder state .  computation graph therefore remains  feedforward network,  recurrent paths      .\n",
            "\n",
            " operator  elementwise Hadamard product.    controlled      weights, which parametrize  previous hidden state ..,    current input .., ,   vector offset .., .  overall operation   infor mally summarized  ,   ,,,  ,representing   state after reading token .   outperforms standard recurrent neural networks across   range \n",
            "\n",
            " . APPLICATIONS  SEQUENCE LABELING   predict labels      ONSTART   character.  recent   employed neural network architectures.  example,   .      architecture,  described  .  construct  trellis,  which    scored according   hidden state   ,   transitions  scored according  learned transition weights.  scoring segmentation   computed  \n",
            "\n",
            "beginning   source   greatest impact   encoding ,  therefore impact  words   beginning   target sentence. Later     vanced encoding models,   neural attention ..,  eliminated    reversing  source sentence.  encoder  decoder   implemented   LSTMs ,  multiple layers  hidden states.  shown  Figure .,  hidden state ,  layeriis treated\n",
            "\n",
            "epochs batches   sentences, chosen   similar length    sentence   batch   roughly   amount    process gradi  clipping ..  ensure      gradient never exceeds  predeﬁned value. .. Neural attention  sequencesequence model discussed   previous section   radical depar   statistical machine translation,  which    phrase   target \n",
            "\n",
            "brenner  Blunsom     . ,  strong empirical results.  models  recurrent   utterance level,    complete utterance updates  hidden state.  recurrentconvolutional network  Kalchbrenner  Blunsom   convolu   obtain  representation   individual utterance, while   .    second level  recurrence,  individual words.  enables their method   \n",
            "\n",
            ".. NEURAL SEQUENCE LABELING   practice, numerical stability demands       domain, logαm    logsm,  logαm . logβm    logsm,  logβm . .  application   forward  backward probabilities  shown  Figure ..   forward  backward recurrences operate   trellis, which implies  space complexityO. Because  recurrences require computing    Kterms \n",
            "\n",
            "tentional encoderdecoder translation model discussed  ..   ., .  longer sentence  encoded   sequence  vectors,    token.  decoder  computes attention  these vectors  updating   recurrent state.    generation,    useful  augment  encoderdecoder model   ability   words directly   source.   .  train  model \n",
            "\n",
            " Morphology  Syntax , Volume   Synthesis Lectures  Human Language Technolo . Morgan  Claypool Publishers. Bengio, ., . Vinyals, . Jaitly,  . Shazeer . Scheduled sampling  sequence prediction  recurrent neural networks.   , . . Bengio, ., . Ducharme,  . Vincent,  . Janvin .  neural probabilistic language model.  Journal  Machine Learning Research  , .\n",
            "Question: LSTM\n",
            "\n",
            "Provide a comprehensive explanation strictly following this structure. Always use \"##\" to introduce each section:\n",
            "\n",
            "## 1. *Concept Overview*\n",
            "  • Core definition and purpose\n",
            "  • Key principles\n",
            "  • Relationship to broader computing concepts\n",
            "\n",
            "## 2. *Technical Components*\n",
            "  • Primary elements and their roles\n",
            "  • Relationships and interactions\n",
            "  • Implementation details\n",
            "  • Core algorithms/procedures (if applicable)\n",
            "\n",
            "## 3. *Working Mechanism*\n",
            "  • Step-by-step operational flow\n",
            "  • Critical processes and transformations\n",
            "  • Resource management (if applicable)\n",
            "  • Exception handling (if applicable)\n",
            "\n",
            "## 4. *Implementation Example*\n",
            "  • Use case scenario\n",
            "  • Code implementation or technical design\n",
            "  • Step-by-step execution\n",
            "  • Output analysis\n",
            "\n",
            "## 5. *Best Practices*\n",
            "  • Design considerations\n",
            "  • Optimization techniques\n",
            "  • Common pitfalls\n",
            "  • Performance implications\n",
            "\n",
            "## 6. *Applications*\n",
            "  • Real-world use cases\n",
            "  • Industry applications\n",
            "  • Integration patterns\n",
            "  • Variations and alternatives\n",
            "\n",
            "## 7. *Evaluation*\n",
            "  • Performance metrics\n",
            "  • Testing approaches\n",
            "  • Debugging strategies\n",
            "  • Optimization opportunities\n",
            "\n",
            "## 8. *Practice Problems*\n",
            "  • Concept verification questions\n",
            "  • Implementation challenges\n",
            "  • Problem-solving scenarios\n",
            "  • Solutions with explanations\n",
            "\n",
            "### Format Requirements:\n",
            "- Always use \"##\" to introduce each section header. Avoid using any other format.\n",
            "- Use bullet points for clarity\n",
            "- Show all mathematical steps using proper equation formatting ($...$)\n",
            "- Include clear variable definitions after each equation\n",
            "- Write formulas using LaTeX formatting inside $...$\n",
            "- For matrices use: $\begin{bmatrix} a & b \\ c & d \\end{bmatrix}$\n",
            "- For fractions use: $\frac{numerator}{denominator}$\n",
            "- Use ■ for numbered equations and • for regular points\n",
            "- Demonstrate practical interpretation\n",
            "- Connect to real applications\n",
            "\n",
            "### Equation Guidelines:\n",
            "- Enclose equations in $...$ format\n",
            "- Use proper LaTeX notation for mathematical expressions\n",
            "- Use $\\sum$ for summations\n",
            "- Define each variable after presenting equations\n",
            "- Number important equations using ■\n",
            "- Show step-by-step derivations with clear explanations\n",
            "\n",
            "### Code Guidelines:\n",
            "- Use LaTeX verbatim environment for code blocks:\n",
            "  \begin{verbatim}\n",
            "  code here\n",
            "  \\end{verbatim}\n",
            "\n",
            "- For inline code use \texttt{code}\n",
            "- For syntax highlighting:\n",
            "  \begin{lstlisting}[language=Python]\n",
            "  code here\n",
            "  \\end{lstlisting}\n",
            "- Include comments explaining code functionality\n",
            "- Show output examples where applicable\n",
            "\n",
            "---\n",
            "\n",
            "## 1. **Concept Overview**\n",
            "\n",
            "### Core Definition and Purpose\n",
            "\n",
            "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem by introducing a memory cell and three types of gates: input gate, forget gate, and output gate. The primary goal of LSTM is to maintain long-term dependencies in sequential data, enabling it to learn and remember relevant information from distant parts of the sequence.\n",
            "\n",
            "### Key Principles\n",
            "\n",
            "The core idea behind LSTM is to control the flow of information into, within, and out of the memory cell. This is achieved through the use of gates, which are essentially nonlinear activation functions that determine whether certain information should be stored, forgotten, or passed on.\n",
            "\n",
            "### Relationship to Broader Computing Concepts\n",
            "\n",
            "LSTM falls under the category of artificial neural networks (ANNs), a subset of machine learning algorithms inspired by biological neural systems. It is specifically designed to handle sequential data, such as time series, natural language, and music, where the order of the data is crucial for understanding and making accurate predictions.\n",
            "\n",
            "## 2. **Technical Components**\n",
            "\n",
            "### Primary Elements and Their Roles\n",
            "\n",
            "- **Memory Cell**: A central component that stores the long-term information in the network.\n",
            "- **Input Gate**: Controls the flow of new information into the memory cell.\n",
            "- **Forget Gate**: Determines what information should be forgotten from the memory cell.\n",
            "- **Output Gate**: Decides what information should be passed on to the next time step.\n",
            "- **Cell State**: The internal state of the memory cell, which represents the accumulated information over time.\n",
            "- **Hidden State**: The output of the LSTM at a given time step, used as input for the next time step.\n",
            "\n",
            "### Relationships and Interactions\n",
            "\n",
            "The interactions between these components occur during the update process, where the input gate, forget gate, and output gate work together to modify the cell state and hidden state based on the current input and previous hidden state.\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "The LSTM update process can be broken down into several steps, including calculating the input gate, forget gate, and output gate activations, adding the new information to the cell state, and finally passing the updated cell state and hidden state to the next time step.\n",
            "\n",
            "### Core Algorithms/Procedures\n",
            "\n",
            "The LSTM update process can be mathematically represented as follows:\n",
            "\n",
            "1. Calculate input gate activation: $i\\_t = sigmoid(W\\_xi . x\\_t + U\\_hi . h\\_{t-1} + b\\_i)$\n",
            "2. Calculate forget gate activation: $f\\_t = sigmoid(W\\_xf . x\\_t + U\\_hf . h\\_{t-1} + b\\_f)$\n",
            "3. Calculate candidate cell state: $C\\_t = tanh(W\\_xc . x\\_t + U\\_hc . h\\_{t-1} + b\\_c)$\n",
            "4. Update cell state: $C\\_{t-1} = f\\_t * C\\_{t-1} + i\\_t * C\\_t$\n",
            "5. Calculate output gate activation: $o\\_t = sigmoid(W\\_xo . x\\_t + U\\_ho . h\\_{t-1} + b\\_o)$\n",
            "6. Calculate hidden state: $h\\_t = o\\_t * tanh(C\\_t)$\n",
            "\n",
            "Here, $W\\_xi$, $W\\_xf$, $W\\_xc$, $U\\_hi$, $U\\_hf$, $U\\_hc$, $b\\_i$, $b\\_f$, $b\\_c$, and $b\\_o$ are weight matrices and biases, and $sigmoid$ denotes the sigmoid activation function.\n",
            "\n",
            "## 3. **Working Mechanism**\n",
            "\n",
            "### Step-by-Step Operational Flow\n",
            "\n",
            "1. At each time step, the input gate determines how much new information should be added to the cell state.\n",
            "2. The forget gate decides what information should be forgotten from the current cell state.\n",
            "3. A candidate cell state is calculated based on the current input and previous hidden state.\n",
            "4. The cell state is updated by combining the old cell state, new information, and the candidate cell state.\n",
            "5. The output gate determines what information should be passed on to the next hidden state.\n",
            "6. The updated cell state is passed through a hyperbolic tangent function to produce the final hidden state.\n",
            "\n",
            "### Critical Processes and Transformations\n",
            "\n",
            "The critical processes involve controlling the flow of information into, within, and out of the memory cell using the input gate, forget gate, and output gate. The transformation of the cell state occurs through the addition of new information and the application of the hyperbolic tangent function.\n",
            "\n",
            "### Resource Management (if applicable)\n",
            "\n",
            "LSTM requires significant computational resources due to its recurrent nature and the presence of multiple gates. Efficient implementations often rely on optimizations like pruning connections, using efficient matrix multiplication libraries, and parallelizing computations.\n",
            "\n",
            "### Exception Handling (if applicable)\n",
            "\n",
            "Exception handling is not typically a concern in the working mechanism of LSTM, as it operates purely as a computational model without explicit error handling logic. However, during training, issues may arise related to convergence, vanishing or exploding gradients, and overfitting, which need to be addressed separately.\n",
            "\n",
            "## 4. **Implementation Example**\n",
            "\n",
            "### Use Case Scenario\n",
            "\n",
            "Consider an LSTM network trained to predict the next word in a sentence given the previous words.\n",
            "\n",
            "### Code Implementation or Technical Design\n",
            "\n",
            "Here's a simplified Python implementation using the Keras library:\n",
            "\n",
            "```python\n",
            "from keras.models import Sequential\n",
            "from keras.layers import LSTM, Dense\n",
            "\n",
            "# Define the model\n",
            "model = Sequential()\n",
            "model.add(LSTM(units=128, return_sequences=True, input_shape=(timesteps, vocab_size)))\n",
            "model.add(Dense(vocab_size))\n",
            "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
            "\n",
            "# Train the model\n",
            "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
            "```\n",
            "\n",
            "### Step-by-Step Execution\n",
            "\n",
            "1. Instantiate a Sequential model and add an LSTM layer with the desired number of units, set `return_sequences` to True if you want the layer to return sequences instead of a single vector, and specify the input shape.\n",
            "2. Add a dense layer for the output layer.\n",
            "3. Compile the model with the appropriate loss function and optimizer.\n",
            "4. Train the model using your training data.\n",
            "\n",
            "### Output Analysis\n",
            "\n",
            "After training, the model will have learned to predict the next word in a sentence given the previous words. You can evaluate its performance on test data and fine-tune the model further to improve its accuracy.\n",
            "\n",
            "## 5. **Best Practices**\n",
            "\n",
            "### Design Considerations\n",
            "\n",
            "- Choose an appropriate number of LSTM units based on the complexity of the task.\n",
            "- Experiment with different architectures, such as stacking multiple LSTM layers or using bidirectional LSTMs.\n",
            "- Regularize the model to prevent overfitting.\n",
            "\n",
            "### Optimization Techniques\n",
            "\n",
            "- Use dropout to randomly disable some neurons during training, reducing co-dependencies among them.\n",
            "- Gradient clipping to prevent the gradients from becoming excessively large.\n",
            "- Use batch normalization to stabilize the network during training.\n",
            "\n",
            "### Common Pitfalls\n",
            "\n",
            "- Ignoring the vanishing gradient problem by using too few LSTM units or not properly initializing the weights.\n",
            "- Training the model for insufficient epochs or using a small batch size.\n",
            "- Not properly handling the sequence length when feeding data to the model.\n",
            "\n",
            "### Performance Implications\n",
            "\n",
            "Performance improvements can be achieved through optimization techniques like those mentioned above, as well as through hardware acceleration like GPUs and TPUs. Additionally, carefully selecting the hyperparameters, such as learning rate, batch size, and number of epochs, can significantly impact the model's performance.\n",
            "\n",
            "## 6. **Applications**\n",
            "\n",
            "### Real-World Use Cases\n",
            "\n",
            "- Natural Language Processing (NLP): Text generation, sentiment analysis, machine translation, and speech recognition.\n",
            "- Time Series Forecasting: Stock price prediction, weather forecasting, and traffic prediction.\n",
            "- Music Generation: Creating new melodies and harmonies based on existing ones.\n",
            "\n",
            "### Industry Applications\n",
            "\n",
            "- Finance: Predicting stock prices, detecting fraudulent transactions, and risk assessment.\n",
            "- Healthcare: Diagnosing diseases, monitoring patient health, and drug discovery.\n",
            "- Entertainment: Generating music, movies, and games.\n",
            "\n",
            "### Integration Patterns\n",
            "\n",
            "LSTMs can be integrated into various software frameworks and libraries, such as TensorFlow, PyTorch, and Keras, making it easy to build and train models for a wide range of applications.\n",
            "\n",
            "### Variations and Alternatives\n",
            "\n",
            "Variants of LSTM include GRU (Gated Recurrent Unit), Echo State Networks, and Simple Recurrent Units. Each has its own strengths and weaknesses, and choosing the best one depends on the specific requirements of the task at hand.\n",
            "\n",
            "## 7. **Evaluation**\n",
            "\n",
            "### Performance Metrics\n",
            "\n",
            "- Accuracy: The proportion of correct predictions made by the model.\n",
            "- Loss: A measure of the difference between the predicted and actual outputs.\n",
            "- Precision: The proportion of true positives among the predicted positives.\n",
            "- Recall: The proportion of true positives among the actual positives.\n",
            "- F1 Score: The harmonic mean of precision and recall.\n",
            "\n",
            "### Testing Approaches\n",
            "\n",
            "- Cross-validation: Splitting the dataset into training, validation, and testing sets to evaluate the model's generalization capability.\n",
            "- Grid Search: Systematically searching for the optimal hyperparameters by trying different combinations.\n",
            "\n",
            "### Debugging Strategies\n",
            "\n",
            "- Visualizing the activations and gradients to identify potential issues.\n",
            "- Monitoring the loss and accuracy during training to detect overfitting or underfitting.\n",
            "- Analyzing the predictions made by the model to understand its behavior.\n",
            "\n",
            "### Optimization Opportunities\n",
            "\n",
            "- Increasing the number of epochs or the batch size can potentially improve the model's performance.\n",
            "- Using more powerful hardware, such as GPUs or TPUs, can accelerate the training process.\n",
            "- Fine-tuning the hyperparameters, such as learning rate and dropout rate, can help the model converge faster and perform better.\n",
            "\n",
            "## 8. **Practice Problems**\n",
            "\n",
            "### Concept Verification Questions\n",
            "\n",
            "1. What is the main advantage of LSTM over traditional RNNs?\n",
            "2. Explain the role of the input gate, forget gate, and output gate in an LSTM unit.\n",
            "3. How does the cell state in an LSTM unit differ from the hidden state?\n",
            "\n",
            "### Implementation Challenges\n",
            "\n",
            "1. Implement an LSTM model from scratch using NumPy.\n",
            "2. Modify the LSTM model to handle variable-length sequences.\n",
            "3. Train an LSTM model to predict the next word in a sentence given the previous words.\n",
            "\n",
            "### Problem-Solving Scenarios\n",
            "\n",
            "1. A model is underfitting during training. What could be the reasons, and how might they be addressed?\n",
            "2. The model is producing incorrect predictions for long sequences. How might this be resolved?\n",
            "3. The model is slow to train due to limited computational resources. What strategies could be employed to speed up the training process?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the answer from api response"
      ],
      "metadata": {
        "id": "GqWZo8EbRSmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "n-1etPIcHaMC",
        "outputId": "5ed3e19c-8e4c-4b4a-985e-00d1e286d9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Context: BIBLIOGRAPHY  Graves, . . Sequence transduction  recurrent neural networks.   . Graves, .  . Jaitly . Towards  speech recognition  recurrent neural networks.   , . . Graves, .  . Schmidhuber . Framewise phoneme classiﬁcation  bidirec tional   other neural network architectures. Neural Networks  , .\n",
            "\n",
            "  vector hmthis information  represented.   limitation   informa   attenuated  repeated application   squashing function . short memories LSTMs, described below,   variant    address  issue,   memory cells  propagate information through  sequence without applying  linearities Hochreiter  Schmidhuber, .  denominator  Equation .   computational bottleneck, because  involves\n",
            "\n",
            "Recurrent neural networks   introduced     language model  technique,  which  context  token  summarized   recurrentlyupdated vector, ,,   ,,..., wherexmis  vector embedding   token wmand  function gdeﬁnes  recur rence.  starting condition   additional parameter   model.   short  memory     complex recurrence,  which  memory   through \n",
            "\n",
            " . LANGUAGE MODELS              Figure .   short memory  architecture. Gates  shown  boxes  dotted edges.    language model,  hmwould    predict   wordwm.  gates  functions   input  previous hidden state.   computed  elementwise sigmoid activations,   , ensuring  their values\n",
            "\n",
            "BIBLIOGRAPHY  , .  . Bansal .  relation extraction using lstms  sequences   structures.   , . . , .  . Hinton . Three  graphical models  statistical language  elling.  Proceedings   International Conference  Machine Learning  , .  . , .  . . Hinton .  scalable hierarchical distributed language model.  Neural Information Processing Systems  , . .\n",
            "\n",
            "sentations  vector space.  Proceedings  International Conference  Learning Represen tations . Mikolov, ., . Deoras, . Povey, . Burget,  . Cernocky . Strategies  training large scale neural network language models.  Proceedings   Workshop  Automatic Speech Recognition  Understanding  , . . Mikolov, ., . Karaﬁ , . Burget, . Cernock ,  . Khudanpur . Recurrent neural network based language model.  INTERSPEECH , . .\n",
            "\n",
            ".. Convolutional Neural Networks  Sequence Labeling  disadvantage  recurrent neural networks    architecture requires iterating through  sequence  inputs  predictions  hidden vector hmmust   puted   previous hidden vector , before predicting   . These iterative computations  difﬁcult  parallelize,    exploit  speedups offered  graph  processing units   operations   matrix multiplication. Convolutional\n",
            "\n",
            " baseline   implemented   neural architecture, using  attention  anism .., which scores  similarity   query      source    ., .      encode  passage   query , using  bidirectional LSTMs  .. BiLSTM   . BiLSTM  . .  query  represented  vertically concatenating   states   right  right passes \n",
            "\n",
            ".. RECURRENT NEURAL NETWORK LANGUAGE MODELS  derivatives automatically,  cache   future .  important distinction   feedforward neural networks considered         computa  graph   ,  varies   length   input.  poses difﬁculties  toolkits   designed around static computation graphs,   TensorFlow Abadi  ., . .. Hyperparameters\n",
            "\n",
            " incorporating  inner product   approximation   likelihood    ,   possible  estimate  parameters  backpropagation.   Mikolov  .,  includes   approximations continuous words   skipgrams. .. Continuous words   recurrent neural network language models,    conditioned   recurrently updated state vector, which  based   representations going      \n",
            "\n",
            ".. RECURRENT NEURAL NETWORK LANGUAGE MODELS    sequence.  language models  deﬁned,  .  , . ,,...,  , . whereφis  matrix   embeddings , andxmdenotes  embedding   .  conversion  wmtoxmis sometimes known   lookup layer , because  simply lookup  embeddings      table  ...  Elman  deﬁnes  simple recurrent operation Elman, ,\n",
            "\n",
            "whereδ   indicator function, taking  value      record   identical   target  .  probability  copying record rfrom  source ,  product    probability   local attention.     model,  attention weights αmare computed   previous decoder state .  computation graph therefore remains  feedforward network,  recurrent paths      .\n",
            "\n",
            " operator  elementwise Hadamard product.    controlled      weights, which parametrize  previous hidden state ..,    current input .., ,   vector offset .., .  overall operation   infor mally summarized  ,   ,,,  ,representing   state after reading token .   outperforms standard recurrent neural networks across   range \n",
            "\n",
            " . APPLICATIONS  SEQUENCE LABELING   predict labels      ONSTART   character.  recent   employed neural network architectures.  example,   .      architecture,  described  .  construct  trellis,  which    scored according   hidden state   ,   transitions  scored according  learned transition weights.  scoring segmentation   computed  \n",
            "\n",
            "beginning   source   greatest impact   encoding ,  therefore impact  words   beginning   target sentence. Later     vanced encoding models,   neural attention ..,  eliminated    reversing  source sentence.  encoder  decoder   implemented   LSTMs ,  multiple layers  hidden states.  shown  Figure .,  hidden state ,  layeriis treated\n",
            "\n",
            "epochs batches   sentences, chosen   similar length    sentence   batch   roughly   amount    process gradi  clipping ..  ensure      gradient never exceeds  predeﬁned value. .. Neural attention  sequencesequence model discussed   previous section   radical depar   statistical machine translation,  which    phrase   target \n",
            "\n",
            "brenner  Blunsom     . ,  strong empirical results.  models  recurrent   utterance level,    complete utterance updates  hidden state.  recurrentconvolutional network  Kalchbrenner  Blunsom   convolu   obtain  representation   individual utterance, while   .    second level  recurrence,  individual words.  enables their method   \n",
            "\n",
            ".. NEURAL SEQUENCE LABELING   practice, numerical stability demands       domain, logαm    logsm,  logαm . logβm    logsm,  logβm . .  application   forward  backward probabilities  shown  Figure ..   forward  backward recurrences operate   trellis, which implies  space complexityO. Because  recurrences require computing    Kterms \n",
            "\n",
            "tentional encoderdecoder translation model discussed  ..   ., .  longer sentence  encoded   sequence  vectors,    token.  decoder  computes attention  these vectors  updating   recurrent state.    generation,    useful  augment  encoderdecoder model   ability   words directly   source.   .  train  model \n",
            "\n",
            " Morphology  Syntax , Volume   Synthesis Lectures  Human Language Technolo . Morgan  Claypool Publishers. Bengio, ., . Vinyals, . Jaitly,  . Shazeer . Scheduled sampling  sequence prediction  recurrent neural networks.   , . . Bengio, ., . Ducharme,  . Vincent,  . Janvin .  neural probabilistic language model.  Journal  Machine Learning Research  , .\n",
            "Question: Lstm\n",
            "\n",
            "Provide a comprehensive explanation strictly following this structure. Always use \"##\" to introduce each section:\n",
            "\n",
            "## 1. *Concept Overview*\n",
            "  • Core definition and purpose\n",
            "  • Key principles\n",
            "  • Relationship to broader computing concepts\n",
            "\n",
            "## 2. *Technical Components*\n",
            "  • Primary elements and their roles\n",
            "  • Relationships and interactions\n",
            "  • Implementation details\n",
            "  • Core algorithms/procedures (if applicable)\n",
            "\n",
            "## 3. *Working Mechanism*\n",
            "  • Step-by-step operational flow\n",
            "  • Critical processes and transformations\n",
            "  • Resource management (if applicable)\n",
            "  • Exception handling (if applicable)\n",
            "\n",
            "## 4. *Implementation Example*\n",
            "  • Use case scenario\n",
            "  • Code implementation or technical design\n",
            "  • Step-by-step execution\n",
            "  • Output analysis\n",
            "\n",
            "## 5. *Best Practices*\n",
            "  • Design considerations\n",
            "  • Optimization techniques\n",
            "  • Common pitfalls\n",
            "  • Performance implications\n",
            "\n",
            "## 6. *Applications*\n",
            "  • Real-world use cases\n",
            "  • Industry applications\n",
            "  • Integration patterns\n",
            "  • Variations and alternatives\n",
            "\n",
            "## 7. *Evaluation*\n",
            "  • Performance metrics\n",
            "  • Testing approaches\n",
            "  • Debugging strategies\n",
            "  • Optimization opportunities\n",
            "\n",
            "## 8. *Practice Problems*\n",
            "  • Concept verification questions\n",
            "  • Implementation challenges\n",
            "  • Problem-solving scenarios\n",
            "  • Solutions with explanations\n",
            "\n",
            "### Format Requirements:\n",
            "- Always use \"##\" to introduce each section header. Avoid using any other format.\n",
            "- Use bullet points for clarity\n",
            "- Show all mathematical steps using proper equation formatting ($...$)\n",
            "- Include clear variable definitions after each equation\n",
            "- Write formulas using LaTeX formatting inside $...$\n",
            "- For matrices use: $\begin{bmatrix} a & b \\ c & d \\end{bmatrix}$\n",
            "- For fractions use: $\frac{numerator}{denominator}$\n",
            "- Use ■ for numbered equations and • for regular points\n",
            "- Demonstrate practical interpretation\n",
            "- Connect to real applications\n",
            "\n",
            "### Equation Guidelines:\n",
            "- Enclose equations in $...$ format\n",
            "- Use proper LaTeX notation for mathematical expressions\n",
            "- Use $\\sum$ for summations\n",
            "- Define each variable after presenting equations\n",
            "- Number important equations using ■\n",
            "- Show step-by-step derivations with clear explanations\n",
            "\n",
            "### Code Guidelines:\n",
            "- Use LaTeX verbatim environment for code blocks:\n",
            "  \begin{verbatim}\n",
            "  code here\n",
            "  \\end{verbatim}\n",
            "\n",
            "- For inline code use \texttt{code}\n",
            "- For syntax highlighting:\n",
            "  \begin{lstlisting}[language=Python]\n",
            "  code here\n",
            "  \\end{lstlisting}\n",
            "- Include comments explaining code functionality\n",
            "- Show output examples where applicable\n",
            "\n",
            "## 1. *Concept Overview*\n",
            "  • **Core definition and purpose**: Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem by introducing memory cells, allowing it to learn long-term dependencies in sequential data. The primary purpose of LSTM is to model and generate sequences, such as text, music, and speech, by capturing the temporal relationships between elements in the sequence.\n",
            "\n",
            "  • **Key principles**: LSTM introduces three types of gates—the forget gate, the input gate, and the output gate—that control the flow of information into and out of memory cells. These gates are responsible for deciding what information to retain, update, and discard during the processing of a sequence. Additionally, LSTM uses a cell state, which acts as a temporary storage unit for information, allowing it to maintain relevant information over time.\n",
            "\n",
            "  • **Relationship to broader computing concepts**: LSTM is closely related to other RNN architectures, such as Elman networks and simple RNNs, but offers improved performance due to its ability to handle long-term dependencies more effectively. LSTM shares many similarities with traditional computer memory systems, as both store and retrieve information based on specific addressing mechanisms. However, unlike traditional memory systems, LSTM does not have a fixed size and can dynamically adjust its memory capacity depending on the sequence length.\n",
            "\n",
            "## 2. *Technical Components*\n",
            "  • **Primary elements and their roles**: The main components of an LSTM include the forget gate, input gate, output gate, and cell state. Each component plays a crucial role in managing the flow of information within the network.\n",
            "\n",
            "  - **Forget Gate**: Determines which information from the previous cell state should be forgotten. It takes the previous cell state and the current input as inputs and produces a weighted sum that represents the importance of each piece of information.\n",
            "\n",
            "  - **Input Gate**: Decides which new information should be written into the cell state. It also takes the previous cell state and the current input as inputs and produces a weighted sum that represents the importance of each piece of information to be added to the cell state.\n",
            "\n",
            "  - **Output Gate**: Controls the flow of information from the cell state to the hidden state. It takes the current cell state and the current input as inputs and produces a weighted sum that represents the importance of each piece of information to be passed to the hidden state.\n",
            "\n",
            "  - **Cell State**: Temporarily stores information throughout the processing of a sequence. It is updated based on the decisions made by the forget gate, input gate, and output gate.\n",
            "\n",
            "  • **Relationships and interactions**: The interactions between the components of an LSTM occur during the forward pass, where the current hidden state is calculated based on the previous hidden state, the current input, and the outputs of the forget gate, input gate, and output gate. The cell state is updated based on the decisions made by the forget gate and input gate, while the hidden state is determined by the output gate and the updated cell state.\n",
            "\n",
            "  • **Implementation details**: LSTMs are typically implemented using software libraries such as TensorFlow, PyTorch, or Keras. These libraries provide pre-built LSTM modules that can be easily integrated into deep learning models. During training, the model learns the weights associated with each component, allowing it to make informed decisions about the flow of information.\n",
            "\n",
            "  • **Core algorithms/procedures**: The core algorithm used in LSTM is the forward pass, which calculates the current hidden state based on the previous hidden state, the current input, and the outputs of the forget gate, input gate, and output gate. The backward pass, on the other hand, computes the gradients required for backpropagation, enabling the model to adjust its weights during training.\n",
            "\n",
            "## 3. *Working Mechanism*\n",
            "  • **Step-by-step operational flow**: During the forward pass, the LSTM first calculates the forget gate, input gate, and output gate activations using sigmoid functions. Then, it calculates the candidate cell state and the new cell state using tanh functions. Finally, it updates the cell state and calculates the hidden state using the output gate activation.\n",
            "\n",
            "  - **Forget Gate**: Calculates the weighted sum of the previous cell state and the current input, passing them through a sigmoid function to produce a value between 0 and 1. This value determines how much of the previous cell state should be forgotten.\n",
            "\n",
            "  - **Input Gate**: Calculates the weighted sum of the previous cell state, the current input, and a bias term, passing them through a sigmoid function to produce a value between 0 and 1. This value determines how much new information should be written into the cell state.\n",
            "\n",
            "  - **Candidate Cell State**: Calculates the weighted sum of the previous cell state, the current input, and a bias term, passing them through a tanh function to produce a value between -1 and 1. This value represents the potential new cell state.\n",
            "\n",
            "  - **New Cell State**: Calculates the product of the input gate activation and the candidate cell state, then adds the result to the forget gate's output multiplied by the previous cell state. This calculation determines the final cell state update.\n",
            "\n",
            "  - **Hidden State**: Calculates the output gate activation, then passes the new cell state through a sigmoid function to determine which parts of the new cell state should be passed to the hidden state. Finally, it passes the remaining parts of the new cell state through a tanh function to normalize the hidden state values.\n",
            "\n",
            "  • **Critical processes and transformations**: The critical processes in an LSTM involve determining which information to forget, write, and retain, as well as updating the cell state and hidden state based on these decisions. The transformations performed by the LSTM include weighted sums, sigmoid and tanh activations, and multiplications.\n",
            "\n",
            "  • **Resource management**: LSTMs do not require explicit resource management, as they are typically implemented within deep learning frameworks that handle resource allocation. However, during training, the model may require significant computational resources, particularly when dealing with long sequences or large datasets.\n",
            "\n",
            "  • **Exception handling**: LSTMs do not explicitly handle exceptions, as they are primarily focused on processing sequential data. However, during training, common exceptions such as NaN (Not-a-Number) errors may occur due to numerical instability or incorrect initialization of weights. These exceptions can be handled using techniques such as gradient clipping or weight initialization schemes that minimize the occurrence of NaNs.\n",
            "\n",
            "## 4. *Implementation Example*\n",
            "  • **Use case scenario**: An example use case for LSTMs is language modeling, where the goal is to predict the next word in a sentence given the preceding words. In this scenario, an LSTM is trained on a large corpus of text, learning the probabilities of each word given the previous words in the sequence.\n",
            "\n",
            "  • **Code implementation or technical design**: Here is an example implementation of an LSTM language model using Python and TensorFlow:\n",
            "\n",
            "  \begin{verbatim}\n",
            "  import tensorflow as tf\n",
            "\n",
            "  # Define the LSTM cell\n",
            "  lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
            "\n",
            "  # Define the input placeholders\n",
            "  inputs = tf.placeholder(tf.int32, shape=[None, None])\n",
            "  targets = tf.placeholder(tf.int32, shape=[None])\n",
            "\n",
            "  # Define the embedding layer\n",
            "  embedding = tf.Variable(tf.random_uniform([vocab_size, num_units], minval=-1.0, maxval=1.0))\n",
            "  embedded_inputs = tf.nn.embedding_lookup(embedding, inputs)\n",
            "\n",
            "  # Initialize the initial hidden state\n",
            "  hidden_state = tf.zeros([batch_size, num_units])\n",
            "\n",
            "  # Perform the forward pass through the LSTM\n",
            "  outputs, _ = tf.nn.dynamic_rnn(lstm_cell, embedded_inputs, initial_state=hidden_state)\n",
            "\n",
            "  # Extract the last hidden state as the final representation\n",
            "  final_hidden_state = outputs[:, -1, :]\n",
            "\n",
            "  # Define the loss function (e.g., cross-entropy loss)\n",
            "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=final_hidden_state))\n",
            "\n",
            "  # Define the optimizer (e.g., Adam optimizer)\n",
            "  optimizer = tf.train.AdamOptimizer()\n",
            "\n",
            "  # Train the model\n",
            "  with tf.Session() as sess:\n",
            "      sess.run(tf.global_variables_initializer())\n",
            "      for epoch in range(num_epochs):\n",
            "          for batch_index in range(num_batches):\n",
            "              batch_inputs, batch_targets = get_next_batch(batch_size)\n",
            "              _, loss_value = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs, targets: batch_targets})\n",
            "              if batch_index % display_steps == 0:\n",
            "                  print(\"Epoch:\", epoch + 1, \", Loss:\", loss_value)\n",
            "   \\end{verbatim}\n",
            "\n",
            "  • **Step-by-step execution**: The code above defines an LSTM language model, initializes the necessary variables, performs the forward pass through the LSTM, calculates the loss, and trains the model using the Adam optimizer. During training, the model iterates over batches of data, updating its weights to minimize the loss.\n",
            "\n",
            "  • **Output analysis**: The output of the model is the predicted probability distribution over the vocabulary for each input sequence. The predicted word with the highest probability is selected as the output.\n",
            "\n",
            "## 5. *Best Practices*\n",
            "  • **Design considerations**: When designing an LSTM model, consider the sequence length, the number of layers, the number of hidden units, and the choice of activation functions. Experiment with different hyperparameter settings to find the optimal configuration for your specific task.\n",
            "\n",
            "  • **Optimization techniques**: To improve the performance of your LSTM model, consider using techniques such as dropout, batch normalization, and gradient clipping. These techniques help prevent overfitting and improve the model's generalization capabilities.\n",
            "\n",
            "  • **Common pitfalls**: One common pitfall when working with LSTMs is the vanishing or exploding gradient problem. This can be addressed by using techniques such as gradient clipping, weight initialization schemes, and normalizing the inputs. Another pitfall is choosing an inappropriate sequence length, leading to poor performance.\n",
            "\n",
            "  • **Performance implications**: The performance of an LSTM model depends on various factors, including the quality and quantity of the training data, the choice of hyperparameters, and the complexity of the task at hand. A well-designed and properly trained LSTM model can achieve state-of-the-art results on a wide range of sequential data tasks.\n",
            "\n",
            "## 6. *Applications*\n",
            "  • **Real-world use cases**: LSTMs have been successfully applied to various real-world problems, including natural language processing (such as language modeling, machine translation, and sentiment analysis), speech recognition, music generation, and video analysis.\n",
            "\n",
            "  • **Industry applications**: LSTMs are widely used in industries such as artificial intelligence, robotics, and autonomous vehicles for tasks like speech recognition, natural language understanding, and decision making. They are also used in entertainment industries for music composition and video game development.\n",
            "\n",
            "  • **Integration patterns**: LSTMs can be integrated into deep learning pipelines alongside other neural network layers, such as convolutional neural networks (CNNs) and fully connected layers. They can also be combined with attention mechanisms to further improve their performance on sequential data tasks.\n",
            "\n",
            "  • **Variations and alternatives**: There are several variations of LSTMs, such as GRUs (Gated Recurrent Units) and ESNs (Echo State Networks). These variants offer different trade-offs in terms of computational efficiency, memory requirements, and performance.\n",
            "\n",
            "## 7. *Evaluation*\n",
            "  • **Performance metrics**: Common performance metrics for evaluating LSTM models include accuracy, perplexity, and F1 score. Accuracy measures the proportion of correct predictions, while perplexity quantifies the average number of possible outcomes for a given prediction. The F1 score combines precision and recall to evaluate the model's overall performance.\n",
            "\n",
            "  • **Testing approaches**: To test an LSTM model, divide the available data into training, validation, and testing sets. Train the model on the training set, tune the hyperparameters on the validation set, and evaluate the model's performance on the testing set.\n",
            "\n",
            "  • **Debugging strategies**: When debugging an LSTM model, start by checking the model's architecture, hyperparameters, and training procedure for any inconsistencies. Visualize the learned weights and activations to gain insights into the model's behavior. If necessary, collect more data or adjust the data preprocessing steps.\n",
            "\n",
            "  • **Optimization opportunities**: To further optimize an LSTM model, experiment with different optimization algorithms, learning rates, and regularization techniques. Consider using techniques such as transfer learning or ensemble methods to leverage pre-trained models or combine multiple models for improved performance.\n",
            "\n",
            "## 8. *Practice Problems*\n",
            "  • **Concept verification questions**:\n",
            "    1. What is the primary difference between LSTMs and traditional RNNs?\n",
            "    2. How does the forget gate in an LSTM decide which information to forget?\n",
            "    3. What is the role of the cell state in an LSTM?\n",
            "    4. Why are LSTMs useful for handling long-term dependencies in sequential data?\n",
            "    5. What is the purpose of the output gate in an LSTM?\n",
            "\n",
            "  • **Implementation challenges**:\n",
            "    1. Implement an LSTM language model from scratch using numpy and backpropagation.\n",
            "    2. Extend the LSTM language model to handle character-level language modeling.\n",
            "    3. Modify the LSTM language model to incorporate attention mechanisms.\n",
            "    4. Train an LSTM model on a custom dataset (e.g., movie reviews or news articles) and compare its performance to a baseline model.\n",
            "\n",
            "  • **Problem-solving scenarios**:\n",
            "    1. A researcher is training an LSTM model for machine translation but observes that the model often generates nonsensical translations. Investigate possible causes and propose solutions.\n",
            "    2. A developer is working on a speech recognition system using LSTMs but finds that the model struggles with long audio clips. Suggest improvements to address this issue.\n",
            "    3. A data scientist is working on a sentiment analysis project using LSTMs but notices that the model's performance drops significantly when dealing with negative sentiments. Explain why this might happen and suggest ways to improve the model's performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(text):\n",
        "   \"\"\"Extract the answer, removing everything before 'Show output examples where applicable'\"\"\"\n",
        "   # Find the index of the end section\n",
        "   start_index = text.find(\"Show output examples where applicable\")\n",
        "\n",
        "   if start_index == -1:\n",
        "       return text.strip()\n",
        "\n",
        "   return text[start_index + len(\"Show output examples where applicable\"):].strip()\n",
        "\n",
        "# Usage\n",
        "clean_answer = extract_answer(answer)\n",
        "print(clean_answer)"
      ],
      "metadata": {
        "id": "LLopdHdAHPz4",
        "outputId": "2af42292-b47d-4736-f9da-7efd9001dc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "## 1. **Concept Overview**\n",
            "\n",
            "### Core Definition and Purpose\n",
            "\n",
            "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem by introducing a memory cell and three types of gates: input gate, forget gate, and output gate. The primary goal of LSTM is to maintain long-term dependencies in sequential data, enabling it to learn and remember relevant information from distant parts of the sequence.\n",
            "\n",
            "### Key Principles\n",
            "\n",
            "The core idea behind LSTM is to control the flow of information into, within, and out of the memory cell. This is achieved through the use of gates, which are essentially nonlinear activation functions that determine whether certain information should be stored, forgotten, or passed on.\n",
            "\n",
            "### Relationship to Broader Computing Concepts\n",
            "\n",
            "LSTM falls under the category of artificial neural networks (ANNs), a subset of machine learning algorithms inspired by biological neural systems. It is specifically designed to handle sequential data, such as time series, natural language, and music, where the order of the data is crucial for understanding and making accurate predictions.\n",
            "\n",
            "## 2. **Technical Components**\n",
            "\n",
            "### Primary Elements and Their Roles\n",
            "\n",
            "- **Memory Cell**: A central component that stores the long-term information in the network.\n",
            "- **Input Gate**: Controls the flow of new information into the memory cell.\n",
            "- **Forget Gate**: Determines what information should be forgotten from the memory cell.\n",
            "- **Output Gate**: Decides what information should be passed on to the next time step.\n",
            "- **Cell State**: The internal state of the memory cell, which represents the accumulated information over time.\n",
            "- **Hidden State**: The output of the LSTM at a given time step, used as input for the next time step.\n",
            "\n",
            "### Relationships and Interactions\n",
            "\n",
            "The interactions between these components occur during the update process, where the input gate, forget gate, and output gate work together to modify the cell state and hidden state based on the current input and previous hidden state.\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "The LSTM update process can be broken down into several steps, including calculating the input gate, forget gate, and output gate activations, adding the new information to the cell state, and finally passing the updated cell state and hidden state to the next time step.\n",
            "\n",
            "### Core Algorithms/Procedures\n",
            "\n",
            "The LSTM update process can be mathematically represented as follows:\n",
            "\n",
            "1. Calculate input gate activation: $i\\_t = sigmoid(W\\_xi . x\\_t + U\\_hi . h\\_{t-1} + b\\_i)$\n",
            "2. Calculate forget gate activation: $f\\_t = sigmoid(W\\_xf . x\\_t + U\\_hf . h\\_{t-1} + b\\_f)$\n",
            "3. Calculate candidate cell state: $C\\_t = tanh(W\\_xc . x\\_t + U\\_hc . h\\_{t-1} + b\\_c)$\n",
            "4. Update cell state: $C\\_{t-1} = f\\_t * C\\_{t-1} + i\\_t * C\\_t$\n",
            "5. Calculate output gate activation: $o\\_t = sigmoid(W\\_xo . x\\_t + U\\_ho . h\\_{t-1} + b\\_o)$\n",
            "6. Calculate hidden state: $h\\_t = o\\_t * tanh(C\\_t)$\n",
            "\n",
            "Here, $W\\_xi$, $W\\_xf$, $W\\_xc$, $U\\_hi$, $U\\_hf$, $U\\_hc$, $b\\_i$, $b\\_f$, $b\\_c$, and $b\\_o$ are weight matrices and biases, and $sigmoid$ denotes the sigmoid activation function.\n",
            "\n",
            "## 3. **Working Mechanism**\n",
            "\n",
            "### Step-by-Step Operational Flow\n",
            "\n",
            "1. At each time step, the input gate determines how much new information should be added to the cell state.\n",
            "2. The forget gate decides what information should be forgotten from the current cell state.\n",
            "3. A candidate cell state is calculated based on the current input and previous hidden state.\n",
            "4. The cell state is updated by combining the old cell state, new information, and the candidate cell state.\n",
            "5. The output gate determines what information should be passed on to the next hidden state.\n",
            "6. The updated cell state is passed through a hyperbolic tangent function to produce the final hidden state.\n",
            "\n",
            "### Critical Processes and Transformations\n",
            "\n",
            "The critical processes involve controlling the flow of information into, within, and out of the memory cell using the input gate, forget gate, and output gate. The transformation of the cell state occurs through the addition of new information and the application of the hyperbolic tangent function.\n",
            "\n",
            "### Resource Management (if applicable)\n",
            "\n",
            "LSTM requires significant computational resources due to its recurrent nature and the presence of multiple gates. Efficient implementations often rely on optimizations like pruning connections, using efficient matrix multiplication libraries, and parallelizing computations.\n",
            "\n",
            "### Exception Handling (if applicable)\n",
            "\n",
            "Exception handling is not typically a concern in the working mechanism of LSTM, as it operates purely as a computational model without explicit error handling logic. However, during training, issues may arise related to convergence, vanishing or exploding gradients, and overfitting, which need to be addressed separately.\n",
            "\n",
            "## 4. **Implementation Example**\n",
            "\n",
            "### Use Case Scenario\n",
            "\n",
            "Consider an LSTM network trained to predict the next word in a sentence given the previous words.\n",
            "\n",
            "### Code Implementation or Technical Design\n",
            "\n",
            "Here's a simplified Python implementation using the Keras library:\n",
            "\n",
            "```python\n",
            "from keras.models import Sequential\n",
            "from keras.layers import LSTM, Dense\n",
            "\n",
            "# Define the model\n",
            "model = Sequential()\n",
            "model.add(LSTM(units=128, return_sequences=True, input_shape=(timesteps, vocab_size)))\n",
            "model.add(Dense(vocab_size))\n",
            "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
            "\n",
            "# Train the model\n",
            "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
            "```\n",
            "\n",
            "### Step-by-Step Execution\n",
            "\n",
            "1. Instantiate a Sequential model and add an LSTM layer with the desired number of units, set `return_sequences` to True if you want the layer to return sequences instead of a single vector, and specify the input shape.\n",
            "2. Add a dense layer for the output layer.\n",
            "3. Compile the model with the appropriate loss function and optimizer.\n",
            "4. Train the model using your training data.\n",
            "\n",
            "### Output Analysis\n",
            "\n",
            "After training, the model will have learned to predict the next word in a sentence given the previous words. You can evaluate its performance on test data and fine-tune the model further to improve its accuracy.\n",
            "\n",
            "## 5. **Best Practices**\n",
            "\n",
            "### Design Considerations\n",
            "\n",
            "- Choose an appropriate number of LSTM units based on the complexity of the task.\n",
            "- Experiment with different architectures, such as stacking multiple LSTM layers or using bidirectional LSTMs.\n",
            "- Regularize the model to prevent overfitting.\n",
            "\n",
            "### Optimization Techniques\n",
            "\n",
            "- Use dropout to randomly disable some neurons during training, reducing co-dependencies among them.\n",
            "- Gradient clipping to prevent the gradients from becoming excessively large.\n",
            "- Use batch normalization to stabilize the network during training.\n",
            "\n",
            "### Common Pitfalls\n",
            "\n",
            "- Ignoring the vanishing gradient problem by using too few LSTM units or not properly initializing the weights.\n",
            "- Training the model for insufficient epochs or using a small batch size.\n",
            "- Not properly handling the sequence length when feeding data to the model.\n",
            "\n",
            "### Performance Implications\n",
            "\n",
            "Performance improvements can be achieved through optimization techniques like those mentioned above, as well as through hardware acceleration like GPUs and TPUs. Additionally, carefully selecting the hyperparameters, such as learning rate, batch size, and number of epochs, can significantly impact the model's performance.\n",
            "\n",
            "## 6. **Applications**\n",
            "\n",
            "### Real-World Use Cases\n",
            "\n",
            "- Natural Language Processing (NLP): Text generation, sentiment analysis, machine translation, and speech recognition.\n",
            "- Time Series Forecasting: Stock price prediction, weather forecasting, and traffic prediction.\n",
            "- Music Generation: Creating new melodies and harmonies based on existing ones.\n",
            "\n",
            "### Industry Applications\n",
            "\n",
            "- Finance: Predicting stock prices, detecting fraudulent transactions, and risk assessment.\n",
            "- Healthcare: Diagnosing diseases, monitoring patient health, and drug discovery.\n",
            "- Entertainment: Generating music, movies, and games.\n",
            "\n",
            "### Integration Patterns\n",
            "\n",
            "LSTMs can be integrated into various software frameworks and libraries, such as TensorFlow, PyTorch, and Keras, making it easy to build and train models for a wide range of applications.\n",
            "\n",
            "### Variations and Alternatives\n",
            "\n",
            "Variants of LSTM include GRU (Gated Recurrent Unit), Echo State Networks, and Simple Recurrent Units. Each has its own strengths and weaknesses, and choosing the best one depends on the specific requirements of the task at hand.\n",
            "\n",
            "## 7. **Evaluation**\n",
            "\n",
            "### Performance Metrics\n",
            "\n",
            "- Accuracy: The proportion of correct predictions made by the model.\n",
            "- Loss: A measure of the difference between the predicted and actual outputs.\n",
            "- Precision: The proportion of true positives among the predicted positives.\n",
            "- Recall: The proportion of true positives among the actual positives.\n",
            "- F1 Score: The harmonic mean of precision and recall.\n",
            "\n",
            "### Testing Approaches\n",
            "\n",
            "- Cross-validation: Splitting the dataset into training, validation, and testing sets to evaluate the model's generalization capability.\n",
            "- Grid Search: Systematically searching for the optimal hyperparameters by trying different combinations.\n",
            "\n",
            "### Debugging Strategies\n",
            "\n",
            "- Visualizing the activations and gradients to identify potential issues.\n",
            "- Monitoring the loss and accuracy during training to detect overfitting or underfitting.\n",
            "- Analyzing the predictions made by the model to understand its behavior.\n",
            "\n",
            "### Optimization Opportunities\n",
            "\n",
            "- Increasing the number of epochs or the batch size can potentially improve the model's performance.\n",
            "- Using more powerful hardware, such as GPUs or TPUs, can accelerate the training process.\n",
            "- Fine-tuning the hyperparameters, such as learning rate and dropout rate, can help the model converge faster and perform better.\n",
            "\n",
            "## 8. **Practice Problems**\n",
            "\n",
            "### Concept Verification Questions\n",
            "\n",
            "1. What is the main advantage of LSTM over traditional RNNs?\n",
            "2. Explain the role of the input gate, forget gate, and output gate in an LSTM unit.\n",
            "3. How does the cell state in an LSTM unit differ from the hidden state?\n",
            "\n",
            "### Implementation Challenges\n",
            "\n",
            "1. Implement an LSTM model from scratch using NumPy.\n",
            "2. Modify the LSTM model to handle variable-length sequences.\n",
            "3. Train an LSTM model to predict the next word in a sentence given the previous words.\n",
            "\n",
            "### Problem-Solving Scenarios\n",
            "\n",
            "1. A model is underfitting during training. What could be the reasons, and how might they be addressed?\n",
            "2. The model is producing incorrect predictions for long sequences. How might this be resolved?\n",
            "3. The model is slow to train due to limited computational resources. What strategies could be employed to speed up the training process?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thiss\n",
        "import re\n",
        "\n",
        "def format_text(text):\n",
        "    \"\"\"\n",
        "    Format text with the following rules:\n",
        "    1. Change single asterisks to double asterisks\n",
        "    2. Convert '###' to bullet points\n",
        "    3. Convert dashes to numbered lists, resetting numbers after each bullet point\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to be formatted\n",
        "    Returns:\n",
        "        str: Formatted text\n",
        "    \"\"\"\n",
        "    # Split text into lines for processing\n",
        "    lines = text.split('\\n')\n",
        "    formatted_lines = []\n",
        "\n",
        "    # Initialize counters for each indentation level\n",
        "    number_counters = {}\n",
        "    current_indent = 0\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Skip empty lines but preserve them\n",
        "        if not line.strip():\n",
        "            formatted_lines.append(line)\n",
        "            continue\n",
        "\n",
        "        # Handle single asterisks to double asterisks\n",
        "        # Use negative lookbehind and lookahead to avoid modifying double asterisks\n",
        "        line = re.sub(r'(?<![\\*])\\*(?![\\*])([^\\*]+)(?<![\\*])\\*(?![\\*])', r'**\\1**', line)\n",
        "\n",
        "        # Get the indentation level of the current line\n",
        "        indent = len(line) - len(line.lstrip())\n",
        "\n",
        "        # Convert ### to bullet points\n",
        "        if line.strip().startswith('####'):\n",
        "            # When we encounter a bullet point, reset all numbering counters\n",
        "            number_counters = {}\n",
        "            line = line.replace('####', '•')\n",
        "            current_indent = indent\n",
        "\n",
        "        # Convert ### to bullet points\n",
        "        if line.strip().startswith('###'):\n",
        "            # When we encounter a bullet point, reset all numbering counters\n",
        "            number_counters = {}\n",
        "            line = line.replace('###', '•')\n",
        "            current_indent = indent\n",
        "\n",
        "\n",
        "\n",
        "        if line.strip().startswith('---'):\n",
        "            # When we encounter a bullet point, reset all numbering counters\n",
        "            number_counters = {}\n",
        "            line = line.replace('---', ' ')\n",
        "            current_indent = indent\n",
        "\n",
        "        # Handle numbered lists (lines starting with dash)\n",
        "        elif line.strip().startswith('-'):\n",
        "            # Reset counters for deeper indentation levels when indent changes\n",
        "            if indent > current_indent:\n",
        "                # Keep only counters for less indented levels\n",
        "                number_counters = {k: v for k, v in number_counters.items() if k < indent}\n",
        "\n",
        "            # Initialize or increment counter for this indentation level\n",
        "            if indent not in number_counters:\n",
        "                number_counters[indent] = 1\n",
        "            else:\n",
        "                number_counters[indent] += 1\n",
        "\n",
        "            # Replace dash with the current number for this indentation level\n",
        "            line = re.sub(r'^\\s*-', f\"{' ' * indent}{number_counters[indent]}.\", line, 1)\n",
        "            current_indent = indent\n",
        "        else:\n",
        "            # For non-list lines, keep track of the current indentation\n",
        "            current_indent = indent\n",
        "\n",
        "        formatted_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(formatted_lines)\n",
        "\n",
        "def process_file(input_text):\n",
        "    \"\"\"\n",
        "    Process the entire file and apply formatting\n",
        "\n",
        "    Args:\n",
        "        input_text (str): Content of the input file\n",
        "    Returns:\n",
        "        str: Formatted content\n",
        "    \"\"\"\n",
        "    # Format the text\n",
        "    formatted_text = format_text(input_text)\n",
        "    return formatted_text\n",
        "\n",
        "# Example usage demonstrating the reset behavior\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # Process and print the sample text\n",
        "    result = process_file(clean_answer)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "wLOjWPC9U0W-",
        "outputId": "06ede04c-139c-4534-a78a-7a15379ff502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "## 1. **Concept Overview**\n",
            "\n",
            "• Core Definition and Purpose\n",
            "\n",
            "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem by introducing a memory cell and three types of gates: input gate, forget gate, and output gate. The primary goal of LSTM is to maintain long-term dependencies in sequential data, enabling it to learn and remember relevant information from distant parts of the sequence.\n",
            "\n",
            "• Key Principles\n",
            "\n",
            "The core idea behind LSTM is to control the flow of information into, within, and out of the memory cell. This is achieved through the use of gates, which are essentially nonlinear activation functions that determine whether certain information should be stored, forgotten, or passed on.\n",
            "\n",
            "• Relationship to Broader Computing Concepts\n",
            "\n",
            "LSTM falls under the category of artificial neural networks (ANNs), a subset of machine learning algorithms inspired by biological neural systems. It is specifically designed to handle sequential data, such as time series, natural language, and music, where the order of the data is crucial for understanding and making accurate predictions.\n",
            "\n",
            "## 2. **Technical Components**\n",
            "\n",
            "• Primary Elements and Their Roles\n",
            "\n",
            "1. **Memory Cell**: A central component that stores the long-term information in the network.\n",
            "2. **Input Gate**: Controls the flow of new information into the memory cell.\n",
            "3. **Forget Gate**: Determines what information should be forgotten from the memory cell.\n",
            "4. **Output Gate**: Decides what information should be passed on to the next time step.\n",
            "5. **Cell State**: The internal state of the memory cell, which represents the accumulated information over time.\n",
            "6. **Hidden State**: The output of the LSTM at a given time step, used as input for the next time step.\n",
            "\n",
            "• Relationships and Interactions\n",
            "\n",
            "The interactions between these components occur during the update process, where the input gate, forget gate, and output gate work together to modify the cell state and hidden state based on the current input and previous hidden state.\n",
            "\n",
            "• Implementation Details\n",
            "\n",
            "The LSTM update process can be broken down into several steps, including calculating the input gate, forget gate, and output gate activations, adding the new information to the cell state, and finally passing the updated cell state and hidden state to the next time step.\n",
            "\n",
            "• Core Algorithms/Procedures\n",
            "\n",
            "The LSTM update process can be mathematically represented as follows:\n",
            "\n",
            "1. Calculate input gate activation: $i\\_t = sigmoid(W\\_xi . x\\_t + U\\_hi . h\\_{t-1} + b\\_i)$\n",
            "2. Calculate forget gate activation: $f\\_t = sigmoid(W\\_xf . x\\_t + U\\_hf . h\\_{t-1} + b\\_f)$\n",
            "3. Calculate candidate cell state: $C\\_t = tanh(W\\_xc . x\\_t + U\\_hc . h\\_{t-1} + b\\_c)$\n",
            "4. Update cell state: $C\\_{t-1} = f\\_t ** C\\_{t-1} + i\\_t ** C\\_t$\n",
            "5. Calculate output gate activation: $o\\_t = sigmoid(W\\_xo . x\\_t + U\\_ho . h\\_{t-1} + b\\_o)$\n",
            "6. Calculate hidden state: $h\\_t = o\\_t * tanh(C\\_t)$\n",
            "\n",
            "Here, $W\\_xi$, $W\\_xf$, $W\\_xc$, $U\\_hi$, $U\\_hf$, $U\\_hc$, $b\\_i$, $b\\_f$, $b\\_c$, and $b\\_o$ are weight matrices and biases, and $sigmoid$ denotes the sigmoid activation function.\n",
            "\n",
            "## 3. **Working Mechanism**\n",
            "\n",
            "• Step-by-Step Operational Flow\n",
            "\n",
            "1. At each time step, the input gate determines how much new information should be added to the cell state.\n",
            "2. The forget gate decides what information should be forgotten from the current cell state.\n",
            "3. A candidate cell state is calculated based on the current input and previous hidden state.\n",
            "4. The cell state is updated by combining the old cell state, new information, and the candidate cell state.\n",
            "5. The output gate determines what information should be passed on to the next hidden state.\n",
            "6. The updated cell state is passed through a hyperbolic tangent function to produce the final hidden state.\n",
            "\n",
            "• Critical Processes and Transformations\n",
            "\n",
            "The critical processes involve controlling the flow of information into, within, and out of the memory cell using the input gate, forget gate, and output gate. The transformation of the cell state occurs through the addition of new information and the application of the hyperbolic tangent function.\n",
            "\n",
            "• Resource Management (if applicable)\n",
            "\n",
            "LSTM requires significant computational resources due to its recurrent nature and the presence of multiple gates. Efficient implementations often rely on optimizations like pruning connections, using efficient matrix multiplication libraries, and parallelizing computations.\n",
            "\n",
            "• Exception Handling (if applicable)\n",
            "\n",
            "Exception handling is not typically a concern in the working mechanism of LSTM, as it operates purely as a computational model without explicit error handling logic. However, during training, issues may arise related to convergence, vanishing or exploding gradients, and overfitting, which need to be addressed separately.\n",
            "\n",
            "## 4. **Implementation Example**\n",
            "\n",
            "• Use Case Scenario\n",
            "\n",
            "Consider an LSTM network trained to predict the next word in a sentence given the previous words.\n",
            "\n",
            "• Code Implementation or Technical Design\n",
            "\n",
            "Here's a simplified Python implementation using the Keras library:\n",
            "\n",
            "```python\n",
            "from keras.models import Sequential\n",
            "from keras.layers import LSTM, Dense\n",
            "\n",
            "# Define the model\n",
            "model = Sequential()\n",
            "model.add(LSTM(units=128, return_sequences=True, input_shape=(timesteps, vocab_size)))\n",
            "model.add(Dense(vocab_size))\n",
            "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
            "\n",
            "# Train the model\n",
            "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
            "```\n",
            "\n",
            "• Step-by-Step Execution\n",
            "\n",
            "1. Instantiate a Sequential model and add an LSTM layer with the desired number of units, set `return_sequences` to True if you want the layer to return sequences instead of a single vector, and specify the input shape.\n",
            "2. Add a dense layer for the output layer.\n",
            "3. Compile the model with the appropriate loss function and optimizer.\n",
            "4. Train the model using your training data.\n",
            "\n",
            "• Output Analysis\n",
            "\n",
            "After training, the model will have learned to predict the next word in a sentence given the previous words. You can evaluate its performance on test data and fine-tune the model further to improve its accuracy.\n",
            "\n",
            "## 5. **Best Practices**\n",
            "\n",
            "• Design Considerations\n",
            "\n",
            "1. Choose an appropriate number of LSTM units based on the complexity of the task.\n",
            "2. Experiment with different architectures, such as stacking multiple LSTM layers or using bidirectional LSTMs.\n",
            "3. Regularize the model to prevent overfitting.\n",
            "\n",
            "• Optimization Techniques\n",
            "\n",
            "1. Use dropout to randomly disable some neurons during training, reducing co-dependencies among them.\n",
            "2. Gradient clipping to prevent the gradients from becoming excessively large.\n",
            "3. Use batch normalization to stabilize the network during training.\n",
            "\n",
            "• Common Pitfalls\n",
            "\n",
            "1. Ignoring the vanishing gradient problem by using too few LSTM units or not properly initializing the weights.\n",
            "2. Training the model for insufficient epochs or using a small batch size.\n",
            "3. Not properly handling the sequence length when feeding data to the model.\n",
            "\n",
            "• Performance Implications\n",
            "\n",
            "Performance improvements can be achieved through optimization techniques like those mentioned above, as well as through hardware acceleration like GPUs and TPUs. Additionally, carefully selecting the hyperparameters, such as learning rate, batch size, and number of epochs, can significantly impact the model's performance.\n",
            "\n",
            "## 6. **Applications**\n",
            "\n",
            "• Real-World Use Cases\n",
            "\n",
            "1. Natural Language Processing (NLP): Text generation, sentiment analysis, machine translation, and speech recognition.\n",
            "2. Time Series Forecasting: Stock price prediction, weather forecasting, and traffic prediction.\n",
            "3. Music Generation: Creating new melodies and harmonies based on existing ones.\n",
            "\n",
            "• Industry Applications\n",
            "\n",
            "1. Finance: Predicting stock prices, detecting fraudulent transactions, and risk assessment.\n",
            "2. Healthcare: Diagnosing diseases, monitoring patient health, and drug discovery.\n",
            "3. Entertainment: Generating music, movies, and games.\n",
            "\n",
            "• Integration Patterns\n",
            "\n",
            "LSTMs can be integrated into various software frameworks and libraries, such as TensorFlow, PyTorch, and Keras, making it easy to build and train models for a wide range of applications.\n",
            "\n",
            "• Variations and Alternatives\n",
            "\n",
            "Variants of LSTM include GRU (Gated Recurrent Unit), Echo State Networks, and Simple Recurrent Units. Each has its own strengths and weaknesses, and choosing the best one depends on the specific requirements of the task at hand.\n",
            "\n",
            "## 7. **Evaluation**\n",
            "\n",
            "• Performance Metrics\n",
            "\n",
            "1. Accuracy: The proportion of correct predictions made by the model.\n",
            "2. Loss: A measure of the difference between the predicted and actual outputs.\n",
            "3. Precision: The proportion of true positives among the predicted positives.\n",
            "4. Recall: The proportion of true positives among the actual positives.\n",
            "5. F1 Score: The harmonic mean of precision and recall.\n",
            "\n",
            "• Testing Approaches\n",
            "\n",
            "1. Cross-validation: Splitting the dataset into training, validation, and testing sets to evaluate the model's generalization capability.\n",
            "2. Grid Search: Systematically searching for the optimal hyperparameters by trying different combinations.\n",
            "\n",
            "• Debugging Strategies\n",
            "\n",
            "1. Visualizing the activations and gradients to identify potential issues.\n",
            "2. Monitoring the loss and accuracy during training to detect overfitting or underfitting.\n",
            "3. Analyzing the predictions made by the model to understand its behavior.\n",
            "\n",
            "• Optimization Opportunities\n",
            "\n",
            "1. Increasing the number of epochs or the batch size can potentially improve the model's performance.\n",
            "2. Using more powerful hardware, such as GPUs or TPUs, can accelerate the training process.\n",
            "3. Fine-tuning the hyperparameters, such as learning rate and dropout rate, can help the model converge faster and perform better.\n",
            "\n",
            "## 8. **Practice Problems**\n",
            "\n",
            "• Concept Verification Questions\n",
            "\n",
            "1. What is the main advantage of LSTM over traditional RNNs?\n",
            "2. Explain the role of the input gate, forget gate, and output gate in an LSTM unit.\n",
            "3. How does the cell state in an LSTM unit differ from the hidden state?\n",
            "\n",
            "• Implementation Challenges\n",
            "\n",
            "1. Implement an LSTM model from scratch using NumPy.\n",
            "2. Modify the LSTM model to handle variable-length sequences.\n",
            "3. Train an LSTM model to predict the next word in a sentence given the previous words.\n",
            "\n",
            "• Problem-Solving Scenarios\n",
            "\n",
            "1. A model is underfitting during training. What could be the reasons, and how might they be addressed?\n",
            "2. The model is producing incorrect predictions for long sequences. How might this be resolved?\n",
            "3. The model is slow to train due to limited computational resources. What strategies could be employed to speed up the training process?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thiss\n",
        "# Write the content to a text file\n",
        "file_path = \"content.txt\"\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "id": "zSuBgezD_0M1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "class GroqClient:\n",
        "    def __init__(self, api_key):\n",
        "        self.client = Groq(api_key=api_key)\n",
        "\n",
        "    def get_completion(self, query, max_tokens=2500):\n",
        "        prompt = self._construct_prompt(query)\n",
        "\n",
        "        try:\n",
        "            chat_completion = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a technical visualization expert specialized in extracting image search keywords.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                model=\"mixtral-8x7b-32768\",\n",
        "                temperature=0.5,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=0.7,\n",
        "                stream=False\n",
        "            )\n",
        "            return chat_completion.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"API request failed: {str(e)}\")\n",
        "\n",
        "    def _construct_prompt(self, content):\n",
        "        return f\"\"\"Extract specific image search keywords from the following content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Your task: Generate 5-7 image search keywords that would help find educational diagrams and technical visualizations related to the main concepts in this content.\n",
        "\n",
        "Focus on identifying keywords for:\n",
        "1. System/concept architecture diagrams\n",
        "2. Process flows and sequences\n",
        "3. Component interactions\n",
        "4. Implementation details\n",
        "5. Working mechanisms\n",
        "6. Step-by-step procedures\n",
        "7. Technical examples\n",
        "\n",
        "Keyword Generation Rules:\n",
        "1. Each keyword MUST use hyphens between words\n",
        "2. Each keyword MUST end with exactly ONE of these suffixes:\n",
        "   - -visualization\n",
        "   - -diagram\n",
        "   - -illustration\n",
        "   - -example\n",
        "   - -steps\n",
        "\n",
        "Key Requirements:\n",
        "- Use specific technical terms from the content\n",
        "- Include major concepts and processes\n",
        "- Avoid generic/non-specific terms\n",
        "- Keywords must directly relate to main topics\n",
        "- Each keyword should help find relevant technical diagrams\n",
        "\n",
        "Output Format:\n",
        "ONLY provide a comma-separated list of keywords.\n",
        "Example: concept-architecture-diagram, process-flow-visualization, component-interaction-illustration\n",
        "\n",
        "KEYWORDS (comma-separated):\"\"\"\n",
        "\n",
        "def extract_keywords(response_text):\n",
        "    \"\"\"\n",
        "    Extracts and validates keywords from the API response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        valid_suffixes = [\n",
        "            '-visualization',\n",
        "            '-diagram',\n",
        "            '-illustration',\n",
        "            '-example',\n",
        "            '-steps'\n",
        "        ]\n",
        "\n",
        "        # Get the comma-separated keywords\n",
        "        keywords = response_text.strip()\n",
        "\n",
        "        # Clean up and validate the keywords\n",
        "        cleaned_keywords = []\n",
        "        for keyword in keywords.split(','):\n",
        "            keyword = keyword.strip()\n",
        "\n",
        "            # Validation checks\n",
        "            if (keyword and                # Not empty\n",
        "                '-' in keyword and         # Contains hyphens\n",
        "                any(keyword.endswith(suffix) for suffix in valid_suffixes) and  # Has valid suffix\n",
        "                keyword.count('-') >= 2):  # Has at least 2 hyphens\n",
        "\n",
        "                cleaned_keywords.append(keyword)\n",
        "\n",
        "        # Return 5-7 keywords\n",
        "        cleaned_keywords = cleaned_keywords[:7] if len(cleaned_keywords) > 7 else cleaned_keywords\n",
        "\n",
        "        if len(cleaned_keywords) < 5:\n",
        "            print(f\"Not enough valid keywords found: {len(cleaned_keywords)}\")\n",
        "            return \"\"\n",
        "\n",
        "        return ', '.join(cleaned_keywords)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    # Initialize client with your API key\n",
        "    client = GroqClient(\"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\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Get completion from Groq\n",
        "        response = client.get_completion(result)\n",
        "\n",
        "        # Extract and validate keywords\n",
        "        keywords = extract_keywords(response)\n",
        "\n",
        "        print(\"Generated Image Search Keywords:\")\n",
        "        print(keywords)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "O0olLdMWe9M6",
        "outputId": "4375c2a8-61f9-441f-eb92-4e066490cbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Image Search Keywords:\n",
            "memory-cell-architecture-diagram, input-forget-output-gates-illustration, cell-state-hidden-state-visualization, lstm-unit-sequence-diagram, lstm-update-process-steps, lstm-implementation-example, tanh-sigmoid-activation-function-example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ####################################################################### IMAGE KEYWORD CODE ################################################################\n",
        "# # Function to query Hugging Face Inference API\n",
        "# def query_huggingface_api(prompt, max_length=25000):\n",
        "#     payload = {\n",
        "#         \"inputs\": prompt,\n",
        "#         \"parameters\": {\n",
        "#             \"max_new_tokens\": max_length,\n",
        "#             \"temperature\": 0.5,\n",
        "#             \"top_p\": 0.7,\n",
        "#             \"top_k\": 50,\n",
        "#             \"repetition_penalty\": 1.1,\n",
        "#             \"do_sample\": True,\n",
        "#             \"stop\": [\"<|endoftext|>\"]\n",
        "#         }\n",
        "#      }\n",
        "\n",
        "#     response = requests.post(\n",
        "#         HUGGINGFACE_API_URL,\n",
        "#         headers=headers,\n",
        "#         json=payload\n",
        "#     )\n",
        "\n",
        "#     if response.status_code == 200:\n",
        "#         #print(response.json())\n",
        "#         return response.json()\n",
        "#     else:\n",
        "#         raise Exception(\n",
        "#             f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "#         )\n",
        "\n",
        "# def extract_answer(api_response):\n",
        "#     \"\"\"\n",
        "#     Extracts and validates keywords from the Hugging Face API response.\n",
        "\n",
        "#     Args:\n",
        "#         api_response (dict): The JSON response from the Hugging Face API\n",
        "\n",
        "#     Returns:\n",
        "#         str: Cleaned comma-separated list of valid technical visualization keywords\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Required suffixes for validation\n",
        "#         valid_suffixes = [\n",
        "#             '-visualization',\n",
        "#             '-diagram',\n",
        "#             '-illustration',\n",
        "#             '-example',\n",
        "#             '-steps'\n",
        "#         ]\n",
        "\n",
        "#         # Extract the generated text from the API response\n",
        "#         if isinstance(api_response, list):\n",
        "#             response_text = api_response[0].get('generated_text', '')\n",
        "#         elif isinstance(api_response, dict):\n",
        "#             response_text = api_response.get('generated_text', '')\n",
        "#         else:\n",
        "#             response_text = str(api_response)\n",
        "\n",
        "#         # Find the keywords section\n",
        "#         marker = \"KEYWORDS (comma-separated):\"\n",
        "#         if marker not in response_text:\n",
        "#             print(\"No keyword marker found in response\")\n",
        "#             return \"\"\n",
        "\n",
        "#         # Get everything after the marker\n",
        "#         keywords_section = response_text.split(marker)[-1].strip()\n",
        "\n",
        "#         # Get the first non-empty line\n",
        "#         keyword_lines = [line.strip() for line in keywords_section.split('\\n') if line.strip()]\n",
        "#         if not keyword_lines:\n",
        "#             print(\"No keyword lines found\")\n",
        "#             return \"\"\n",
        "\n",
        "#         # Get the comma-separated keywords\n",
        "#         keywords = keyword_lines[0]\n",
        "\n",
        "#         # Clean up and validate the keywords\n",
        "#         cleaned_keywords = []\n",
        "#         for keyword in keywords.split(','):\n",
        "#             keyword = keyword.strip()\n",
        "\n",
        "#             # Validation checks\n",
        "#             if (keyword and                # Not empty\n",
        "#                 '-' in keyword and         # Contains hyphens\n",
        "#                 any(keyword.endswith(suffix) for suffix in valid_suffixes) and  # Has valid suffix\n",
        "#                 keyword.count('-') >= 2):  # Has at least 2 hyphens (component-type-suffix)\n",
        "\n",
        "#                 cleaned_keywords.append(keyword)\n",
        "\n",
        "#         # Return 5-7 keywords as specified in the prompt\n",
        "#         cleaned_keywords = cleaned_keywords[:7] if len(cleaned_keywords) > 7 else cleaned_keywords\n",
        "\n",
        "#         if len(cleaned_keywords) < 5:\n",
        "#             print(f\"Not enough valid keywords found: {len(cleaned_keywords)}\")\n",
        "#             return \"\"\n",
        "\n",
        "#         return ', '.join(cleaned_keywords)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error extracting keywords: {str(e)}\")\n",
        "#         return \"\""
      ],
      "metadata": {
        "id": "9hmIXAny65MY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ####################################################################### IMAGE KEYWORD CODE ################################################################\n",
        "\n",
        "# def create_image_keyword_prompt(content):\n",
        "#     \"\"\"\n",
        "#     Creates a prompt for generating image search keywords that works with any technical topic.\n",
        "\n",
        "#     Args:\n",
        "#         content (str): Technical content to analyze for image-searchable keywords\n",
        "\n",
        "#     Returns:\n",
        "#         str: Generic prompt that works with any technical content\n",
        "#     \"\"\"\n",
        "#     prompt = f\"\"\"You are a technical visualization expert. Extract specific image search keywords from the following content:\n",
        "\n",
        "# {content}\n",
        "\n",
        "# Your task: Generate 5-7 image search keywords that would help find educational diagrams and technical visualizations related to the main concepts in this content.\n",
        "\n",
        "# Focus on identifying keywords for:\n",
        "# 1. System/concept architecture diagrams\n",
        "# 2. Process flows and sequences\n",
        "# 3. Component interactions\n",
        "# 4. Implementation details\n",
        "# 5. Working mechanisms\n",
        "# 6. Step-by-step procedures\n",
        "# 7. Technical examples\n",
        "\n",
        "# Keyword Generation Rules:\n",
        "# 1. Each keyword MUST use hyphens between words\n",
        "# 2. Each keyword MUST end with exactly ONE of these suffixes:\n",
        "#    - -visualization\n",
        "#    - -diagram\n",
        "#    - -illustration\n",
        "#    - -example\n",
        "#    - -steps\n",
        "\n",
        "# Key Requirements:\n",
        "# - Use specific technical terms from the content\n",
        "# - Include major concepts and processes\n",
        "# - Avoid generic/non-specific terms\n",
        "# - Keywords must directly relate to main topics\n",
        "# - Each keyword should help find relevant technical diagrams\n",
        "\n",
        "# Output Format:\n",
        "# ONLY provide a comma-separated list of keywords. Example:\n",
        "# concept-architecture-diagram, process-flow-visualization, component-interaction-illustration\n",
        "\n",
        "# KEYWORDS (comma-separated):\"\"\"\n",
        "\n",
        "#     return prompt\n",
        "\n",
        "# # Modify the main function to work with your existing setup\n",
        "# def main():\n",
        "#     # Use your existing content and context\n",
        "#     prompt = create_image_keyword_prompt(result)\n",
        "#     #print(prompt)\n",
        "\n",
        "#     # Use your existing API call function\n",
        "#     api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "\n",
        "#     # Extract the keywords\n",
        "#     image_keywords = extract_answer(api_response)\n",
        "\n",
        "#     print(\"Generated Image Search Keywords:\")\n",
        "#     print(image_keywords)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dEGPFXyTQ4jd",
        "outputId": "c9548a38-ad07-42dd-849f-25dfdf3759ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Image Search Keywords:\n",
            "long-short-term-memory-lstm-cell-visualization, lstm-process-flow-steps, lstm-component-interaction-illustration, lstm-implementation-example, working-mechanism-of-lstm-diagram, lstm-training-procedure-steps, lstm-language-modeling-example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from pptx.dml.color import RGBColor\n",
        "\n",
        "def scrape_images(keyword, num_images=2):\n",
        "    \"\"\"Scrape images from GeeksForGeeks using SERP API\"\"\"\n",
        "    api_key = \"41cf19594f02970e20e9362044f5605347e8e04ce0cf4a9614504c087d2bae2e\"\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": keyword,\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        images = []\n",
        "\n",
        "        if \"images_results\" in data:\n",
        "            for image in data[\"images_results\"]:\n",
        "                if 'geeksforgeeks' in image.get('source', '').lower():\n",
        "                    images.append(image[\"original\"])\n",
        "                    if len(images) >= num_images:\n",
        "                        break\n",
        "            return images\n",
        "\n",
        "        return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve images. Status code: {response.status_code}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "budrpW1Lg-eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final code for slides"
      ],
      "metadata": {
        "id": "Y0PQ_l3-bYTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra\n",
        "!pdflatex --version"
      ],
      "metadata": {
        "id": "a4ukERBhpIAk",
        "outputId": "18cd0936-f838-47c4-ee6d-9c25e120b2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,650 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,523 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,907 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,657 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,606 kB]\n",
            "Fetched 20.0 MB in 2s (8,015 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-adf-accanthis fonts-adf-berenis fonts-adf-gillius\n",
            "  fonts-adf-universalis fonts-cabin fonts-cantarell fonts-comfortaa\n",
            "  fonts-croscore fonts-crosextra-caladea fonts-crosextra-carlito\n",
            "  fonts-dejavu-core fonts-dejavu-extra fonts-droid-fallback fonts-ebgaramond\n",
            "  fonts-ebgaramond-extra fonts-font-awesome fonts-freefont-otf\n",
            "  fonts-freefont-ttf fonts-gfs-artemisia fonts-gfs-complutum fonts-gfs-didot\n",
            "  fonts-gfs-neohellenic fonts-gfs-olga fonts-gfs-solomos fonts-go\n",
            "  fonts-junicode fonts-lato fonts-linuxlibertine fonts-lmodern fonts-lobster\n",
            "  fonts-lobstertwo fonts-noto-color-emoji fonts-noto-core fonts-noto-mono\n",
            "  fonts-oflb-asana-math fonts-open-sans fonts-roboto-unhinted fonts-sil-charis\n",
            "  fonts-sil-gentium fonts-sil-gentium-basic fonts-sil-gentiumplus\n",
            "  fonts-sil-gentiumplus-compact fonts-stix fonts-texgyre fonts-urw-base35\n",
            "  libapache-pom-java libcommons-logging-java libcommons-parent-java\n",
            "  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35\n",
            "  libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n",
            "  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils tex-common\n",
            "  tex-gyre texlive-base texlive-binaries texlive-fonts-extra-links\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic tipa\n",
            "  xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fontforge libavalon-framework-java libcommons-logging-java-doc\n",
            "  libexcalibur-logkit-java liblog4j1.2-java ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri ruby-dev bundler\n",
            "  debhelper gv | postscript-viewer perl-tk xpdf | pdf-viewer xzdec cm-super\n",
            "  texlive-fonts-extra-doc texlive-fonts-recommended-doc texlive-latex-base-doc\n",
            "  python3-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-luatex texlive-pstricks dot2tex prerex\n",
            "  texlive-pictures-doc vprerex default-jre-headless tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-adf-accanthis fonts-adf-berenis fonts-adf-gillius\n",
            "  fonts-adf-universalis fonts-cabin fonts-cantarell fonts-comfortaa\n",
            "  fonts-croscore fonts-crosextra-caladea fonts-crosextra-carlito\n",
            "  fonts-dejavu-core fonts-dejavu-extra fonts-droid-fallback fonts-ebgaramond\n",
            "  fonts-ebgaramond-extra fonts-font-awesome fonts-freefont-otf\n",
            "  fonts-freefont-ttf fonts-gfs-artemisia fonts-gfs-complutum fonts-gfs-didot\n",
            "  fonts-gfs-neohellenic fonts-gfs-olga fonts-gfs-solomos fonts-go\n",
            "  fonts-junicode fonts-lato fonts-linuxlibertine fonts-lmodern fonts-lobster\n",
            "  fonts-lobstertwo fonts-noto-color-emoji fonts-noto-core fonts-noto-mono\n",
            "  fonts-oflb-asana-math fonts-open-sans fonts-roboto-unhinted fonts-sil-charis\n",
            "  fonts-sil-gentium fonts-sil-gentium-basic fonts-sil-gentiumplus\n",
            "  fonts-sil-gentiumplus-compact fonts-stix fonts-texgyre fonts-urw-base35\n",
            "  libapache-pom-java libcommons-logging-java libcommons-parent-java\n",
            "  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35\n",
            "  libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n",
            "  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils tex-common\n",
            "  tex-gyre texlive-base texlive-binaries texlive-fonts-extra\n",
            "  texlive-fonts-extra-links texlive-fonts-recommended texlive-latex-base\n",
            "  texlive-latex-extra texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa xfonts-encodings xfonts-utils\n",
            "0 upgraded, 93 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 712 MB of archives.\n",
            "After this operation, 2,087 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.10 [752 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.10 [5,031 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-adf-accanthis all 0.20190904-2 [203 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-adf-berenis all 0.20190904-2 [313 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-adf-gillius all 0.20190904-2 [190 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-adf-universalis all 0.20190904-2 [112 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-cabin all 1.5-3 [141 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-cantarell all 0.303-2 [286 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-comfortaa all 3.001-3 [129 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-croscore all 20201225-1build1 [1,572 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-crosextra-caladea all 20130214-2.1 [82.4 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-crosextra-carlito all 20130920-1.1 [743 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ebgaramond all 0.016+git20210310.42d4f9f2-1 [512 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ebgaramond-extra all 0.016+git20210310.42d4f9f2-1 [2,233 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-font-awesome all 5.0.10+really4.7.0~dfsg-4.1 [516 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-freefont-otf all 20120503-10build1 [3,054 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-artemisia all 1.1-6 [260 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-complutum all 1.1-7 [41.8 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-didot all 1.1-7 [278 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-neohellenic all 1.1-7 [215 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-olga all 1.1-6 [33.5 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-gfs-solomos all 1.1-6 [40.9 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-go all 0~20170330-1 [369 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-junicode all 1.002-2 [828 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-linuxlibertine all 5.3.0-6 [1,627 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lobster all 2.0-2.1 [38.9 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lobstertwo all 2.0-2.1 [93.3 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.042-0ubuntu0.22.04.1 [9,944 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-core all 20201225-1build1 [12.2 MB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-oflb-asana-math all 000.907-7build1 [245 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-open-sans all 1.11-2 [635 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-roboto-unhinted all 2:0~20170802-3 [2,376 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-charis all 6.101-1 [3,973 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentium all 20081126:1.03-4 [245 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentium-basic all 1.102-1.1 [384 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentiumplus all 6.101-1 [8,086 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentiumplus-compact all 5.000-4 [1,514 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.8 [50.1 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.8 [5,113 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-extra all 2021.20220204-1 [484 MB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-stix all 1.1.1-4.1 [589 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-extra-links all 2021.20220204-1 [20.3 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Fetched 712 MB in 25s (28.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 93.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 125003 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.10_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.10_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-adf-accanthis.\n",
            "Preparing to unpack .../13-fonts-adf-accanthis_0.20190904-2_all.deb ...\n",
            "Unpacking fonts-adf-accanthis (0.20190904-2) ...\n",
            "Selecting previously unselected package fonts-adf-berenis.\n",
            "Preparing to unpack .../14-fonts-adf-berenis_0.20190904-2_all.deb ...\n",
            "Unpacking fonts-adf-berenis (0.20190904-2) ...\n",
            "Selecting previously unselected package fonts-adf-gillius.\n",
            "Preparing to unpack .../15-fonts-adf-gillius_0.20190904-2_all.deb ...\n",
            "Unpacking fonts-adf-gillius (0.20190904-2) ...\n",
            "Selecting previously unselected package fonts-adf-universalis.\n",
            "Preparing to unpack .../16-fonts-adf-universalis_0.20190904-2_all.deb ...\n",
            "Unpacking fonts-adf-universalis (0.20190904-2) ...\n",
            "Selecting previously unselected package fonts-cabin.\n",
            "Preparing to unpack .../17-fonts-cabin_1.5-3_all.deb ...\n",
            "Unpacking fonts-cabin (1.5-3) ...\n",
            "Selecting previously unselected package fonts-cantarell.\n",
            "Preparing to unpack .../18-fonts-cantarell_0.303-2_all.deb ...\n",
            "Unpacking fonts-cantarell (0.303-2) ...\n",
            "Selecting previously unselected package fonts-comfortaa.\n",
            "Preparing to unpack .../19-fonts-comfortaa_3.001-3_all.deb ...\n",
            "Unpacking fonts-comfortaa (3.001-3) ...\n",
            "Selecting previously unselected package fonts-croscore.\n",
            "Preparing to unpack .../20-fonts-croscore_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-croscore (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-crosextra-caladea.\n",
            "Preparing to unpack .../21-fonts-crosextra-caladea_20130214-2.1_all.deb ...\n",
            "Unpacking fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Selecting previously unselected package fonts-crosextra-carlito.\n",
            "Preparing to unpack .../22-fonts-crosextra-carlito_20130920-1.1_all.deb ...\n",
            "Unpacking fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../23-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../24-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond.\n",
            "Preparing to unpack .../25-fonts-ebgaramond_0.016+git20210310.42d4f9f2-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond (0.016+git20210310.42d4f9f2-1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond-extra.\n",
            "Preparing to unpack .../26-fonts-ebgaramond-extra_0.016+git20210310.42d4f9f2-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond-extra (0.016+git20210310.42d4f9f2-1) ...\n",
            "Selecting previously unselected package fonts-font-awesome.\n",
            "Preparing to unpack .../27-fonts-font-awesome_5.0.10+really4.7.0~dfsg-4.1_all.deb ...\n",
            "Unpacking fonts-font-awesome (5.0.10+really4.7.0~dfsg-4.1) ...\n",
            "Selecting previously unselected package fonts-freefont-otf.\n",
            "Preparing to unpack .../28-fonts-freefont-otf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-otf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../29-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-gfs-artemisia.\n",
            "Preparing to unpack .../30-fonts-gfs-artemisia_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-artemisia (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-complutum.\n",
            "Preparing to unpack .../31-fonts-gfs-complutum_1.1-7_all.deb ...\n",
            "Unpacking fonts-gfs-complutum (1.1-7) ...\n",
            "Selecting previously unselected package fonts-gfs-didot.\n",
            "Preparing to unpack .../32-fonts-gfs-didot_1.1-7_all.deb ...\n",
            "Unpacking fonts-gfs-didot (1.1-7) ...\n",
            "Selecting previously unselected package fonts-gfs-neohellenic.\n",
            "Preparing to unpack .../33-fonts-gfs-neohellenic_1.1-7_all.deb ...\n",
            "Unpacking fonts-gfs-neohellenic (1.1-7) ...\n",
            "Selecting previously unselected package fonts-gfs-olga.\n",
            "Preparing to unpack .../34-fonts-gfs-olga_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-olga (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-solomos.\n",
            "Preparing to unpack .../35-fonts-gfs-solomos_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-solomos (1.1-6) ...\n",
            "Selecting previously unselected package fonts-go.\n",
            "Preparing to unpack .../36-fonts-go_0~20170330-1_all.deb ...\n",
            "Unpacking fonts-go (0~20170330-1) ...\n",
            "Selecting previously unselected package fonts-junicode.\n",
            "Preparing to unpack .../37-fonts-junicode_1.002-2_all.deb ...\n",
            "Unpacking fonts-junicode (1.002-2) ...\n",
            "Selecting previously unselected package fonts-linuxlibertine.\n",
            "Preparing to unpack .../38-fonts-linuxlibertine_5.3.0-6_all.deb ...\n",
            "Unpacking fonts-linuxlibertine (5.3.0-6) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../39-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-lobster.\n",
            "Preparing to unpack .../40-fonts-lobster_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lobster (2.0-2.1) ...\n",
            "Selecting previously unselected package fonts-lobstertwo.\n",
            "Preparing to unpack .../41-fonts-lobstertwo_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lobstertwo (2.0-2.1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../42-fonts-noto-color-emoji_2.042-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-noto-core.\n",
            "Preparing to unpack .../43-fonts-noto-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../44-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-oflb-asana-math.\n",
            "Preparing to unpack .../45-fonts-oflb-asana-math_000.907-7build1_all.deb ...\n",
            "Unpacking fonts-oflb-asana-math (000.907-7build1) ...\n",
            "Selecting previously unselected package fonts-open-sans.\n",
            "Preparing to unpack .../46-fonts-open-sans_1.11-2_all.deb ...\n",
            "Unpacking fonts-open-sans (1.11-2) ...\n",
            "Selecting previously unselected package fonts-roboto-unhinted.\n",
            "Preparing to unpack .../47-fonts-roboto-unhinted_2%3a0~20170802-3_all.deb ...\n",
            "Unpacking fonts-roboto-unhinted (2:0~20170802-3) ...\n",
            "Selecting previously unselected package fonts-sil-charis.\n",
            "Preparing to unpack .../48-fonts-sil-charis_6.101-1_all.deb ...\n",
            "Unpacking fonts-sil-charis (6.101-1) ...\n",
            "Selecting previously unselected package fonts-sil-gentium.\n",
            "Preparing to unpack .../49-fonts-sil-gentium_20081126%3a1.03-4_all.deb ...\n",
            "Unpacking fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Selecting previously unselected package fonts-sil-gentium-basic.\n",
            "Preparing to unpack .../50-fonts-sil-gentium-basic_1.102-1.1_all.deb ...\n",
            "Unpacking fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus.\n",
            "Preparing to unpack .../51-fonts-sil-gentiumplus_6.101-1_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus (6.101-1) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus-compact.\n",
            "Preparing to unpack .../52-fonts-sil-gentiumplus-compact_5.000-4_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus-compact (5.000-4) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../53-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../54-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../55-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../56-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../57-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../58-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../59-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../60-ruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../61-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../62-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../63-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../64-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../65-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../66-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../67-libruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../68-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../69-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../70-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../71-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../72-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../73-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../74-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../75-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../76-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../77-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../78-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../79-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../80-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-extra.\n",
            "Preparing to unpack .../81-texlive-fonts-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package fonts-stix.\n",
            "Preparing to unpack .../82-fonts-stix_1.1.1-4.1_all.deb ...\n",
            "Unpacking fonts-stix (1.1.1-4.1) ...\n",
            "Selecting previously unselected package texlive-fonts-extra-links.\n",
            "Preparing to unpack .../83-texlive-fonts-extra-links_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-extra-links (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../84-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../85-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../86-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../87-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../88-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../89-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../90-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../91-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../92-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Setting up fonts-gfs-didot (1.1-7) ...\n",
            "Setting up fonts-gfs-artemisia (1.1-6) ...\n",
            "Setting up fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Setting up fonts-cantarell (0.303-2) ...\n",
            "Setting up fonts-ebgaramond (0.016+git20210310.42d4f9f2-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-junicode (1.002-2) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up fonts-adf-berenis (0.20190904-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up fonts-freefont-otf (20120503-10build1) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up fonts-gfs-solomos (1.1-6) ...\n",
            "Setting up fonts-comfortaa (3.001-3) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-sil-gentiumplus-compact (5.000-4) ...\n",
            "Setting up fonts-roboto-unhinted (2:0~20170802-3) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up fonts-open-sans (1.11-2) ...\n",
            "Setting up fonts-sil-gentiumplus (6.101-1) ...\n",
            "Setting up fonts-gfs-neohellenic (1.1-7) ...\n",
            "Setting up fonts-gfs-olga (1.1-6) ...\n",
            "Setting up fonts-oflb-asana-math (000.907-7build1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Setting up fonts-adf-accanthis (0.20190904-2) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Setting up fonts-adf-universalis (0.20190904-2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up fonts-stix (1.1.1-4.1) ...\n",
            "Setting up fonts-sil-charis (6.101-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up fonts-go (0~20170330-1) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up fonts-cabin (1.5-3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up fonts-linuxlibertine (5.3.0-6) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up fonts-croscore (20201225-1build1) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up fonts-gfs-complutum (1.1-7) ...\n",
            "Setting up fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up fonts-ebgaramond-extra (0.016+git20210310.42d4f9f2-1) ...\n",
            "Setting up fonts-lobster (2.0-2.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up fonts-adf-gillius (0.20190904-2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up fonts-noto-core (20201225-1build1) ...\n",
            "Setting up fonts-font-awesome (5.0.10+really4.7.0~dfsg-4.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up fonts-lobstertwo (2.0-2.1) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up texlive-fonts-extra-links (2021.20220204-1) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-extra (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "pdfTeX 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian)\n",
            "kpathsea version 6.3.4/dev\n",
            "Copyright 2021 Han The Thanh (pdfTeX) et al.\n",
            "There is NO warranty.  Redistribution of this software is\n",
            "covered by the terms of both the pdfTeX copyright and\n",
            "the Lesser GNU General Public License.\n",
            "For more information about these matters, see the file\n",
            "named COPYING and the pdfTeX source.\n",
            "Primary author of pdfTeX: Han The Thanh (pdfTeX) et al.\n",
            "Compiled with libpng 1.6.37; using libpng 1.6.37\n",
            "Compiled with zlib 1.2.11; using zlib 1.2.11\n",
            "Compiled with xpdf version 4.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "3v7zTmabilP2",
        "outputId": "fad17c96-b881-4232-c36b-863273f58f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################  with groq ##############################################\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from pptx.dml.color import RGBColor\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "def convert_to_tex(content):\n",
        "    \"\"\"\n",
        "    Convert content to LaTeX with proper section hierarchy and special handling for code blocks and equations.\n",
        "    We insert unique markers (@@@SECTION: ... @@@) that will survive the conversion for easier text extraction.\n",
        "    \"\"\"\n",
        "    tex_content = r\"\"\"\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\\usepackage{listings}\n",
        "\\usepackage{xcolor}\n",
        "\\usepackage{enumitem}\n",
        "\\usepackage{titlesec}\n",
        "\\usepackage{geometry}\n",
        "\\usepackage{verbatim}\n",
        "\n",
        "\\geometry{\n",
        "    a4paper,\n",
        "    left=1in,\n",
        "    right=1in,\n",
        "    top=1in,\n",
        "    bottom=1in\n",
        "}\n",
        "\n",
        "% Format section titles\n",
        "\\titleformat{\\section}\n",
        "  {\\Large\\bfseries}\n",
        "  {}\n",
        "  {0em}\n",
        "  {}\n",
        "\n",
        "% Code listing settings\n",
        "\\lstset{\n",
        "    backgroundcolor=\\color{gray!10},\n",
        "    basicstyle=\\ttfamily\\small,\n",
        "    breaklines=true,\n",
        "    frame=single,\n",
        "    numbers=left,\n",
        "    numberstyle=\\tiny,\n",
        "    keywordstyle=\\color{blue},\n",
        "    commentstyle=\\color{green!60!black},\n",
        "    stringstyle=\\color{red},\n",
        "    tabsize=4,\n",
        "    showspaces=false,\n",
        "    showstringspaces=false\n",
        "}\n",
        "\n",
        "\\begin{document}\n",
        "\\pagestyle{plain}\n",
        "\"\"\"\n",
        "    # Split on \"## \" markers (if present)\n",
        "    sections = content.split(\"## \")\n",
        "    for section in sections[1:]:\n",
        "        if not section.strip():\n",
        "            continue\n",
        "\n",
        "        # Process main section headers with the expected format (e.g. \"1. **Title**\")\n",
        "        title_match = re.match(r'\\d+\\.\\s+\\*\\*(.*?)\\*\\*', section)\n",
        "        if title_match:\n",
        "            section_title = title_match.group(1).strip()\n",
        "            # Insert our unique marker for later extraction:\n",
        "            tex_content += f\"\\\\section*{{@@@SECTION: {section_title} @@@}}\\n\\n\"\n",
        "            section = re.sub(r'\\d+\\.\\s+\\*\\*.*?\\*\\*\\n', '', section)\n",
        "\n",
        "        paragraphs = section.split(\"\\n\")\n",
        "        in_itemize = False\n",
        "        in_paragraph = False\n",
        "        in_code_block = False\n",
        "        in_verbatim = False\n",
        "        code_content = []\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            original_paragraph = paragraph\n",
        "            paragraph = paragraph.rstrip(\"\\n\")\n",
        "            if not paragraph.strip():\n",
        "                if in_itemize:\n",
        "                    tex_content += \"\\\\end{itemize}\\n\"\n",
        "                    in_itemize = False\n",
        "                if in_paragraph:\n",
        "                    tex_content += \"\\n\\n\"\n",
        "                    in_paragraph = False\n",
        "                continue\n",
        "\n",
        "            # Handle verbatim/code blocks\n",
        "            if \"egin{verbatim}\" in paragraph:\n",
        "                in_verbatim = True\n",
        "                tex_content += \"\\\\begin{lstlisting}[language=Python]\\n\"\n",
        "                continue\n",
        "            elif \"\\\\end{verbatim}\" in paragraph:\n",
        "                in_verbatim = False\n",
        "                tex_content += \"\\\\end{lstlisting}\\n\\n\"\n",
        "                continue\n",
        "            elif in_verbatim:\n",
        "                tex_content += paragraph.lstrip() + \"\\n\"\n",
        "                continue\n",
        "            elif paragraph.startswith(\"```python\"):\n",
        "                in_code_block = True\n",
        "                code_content = []\n",
        "                continue\n",
        "            elif paragraph.startswith(\"```\") and in_code_block:\n",
        "                in_code_block = False\n",
        "                tex_content += \"\\\\begin{lstlisting}[language=Python]\\n\"\n",
        "                tex_content += \"\\n\".join(code_content)\n",
        "                tex_content += \"\\n\\\\end{lstlisting}\\n\\n\"\n",
        "                code_content = []\n",
        "                continue\n",
        "            elif in_code_block:\n",
        "                code_content.append(paragraph)\n",
        "                continue\n",
        "\n",
        "            # Handle bullet points and numbered lists\n",
        "            elif paragraph.lstrip().startswith(\"• \"):\n",
        "                if not in_itemize:\n",
        "                    tex_content += \"\\\\begin{itemize}\\n\"\n",
        "                    in_itemize = True\n",
        "                tex_content += f\"\\\\item {paragraph.lstrip()[2:]}\\n\"\n",
        "            elif re.match(r'^\\d+\\.\\s', paragraph):\n",
        "                if not in_itemize:\n",
        "                    tex_content += \"\\\\begin{itemize}\\n\"\n",
        "                    in_itemize = True\n",
        "                paragraph = re.sub(r'^\\d+\\.\\s', '', paragraph)\n",
        "                tex_content += f\"\\\\item {paragraph}\\n\"\n",
        "            else:\n",
        "                if in_itemize:\n",
        "                    tex_content += \"\\\\end{itemize}\\n\"\n",
        "                    in_itemize = False\n",
        "\n",
        "                # Handle equations (display math mode)\n",
        "                if paragraph.count('$') >= 2:\n",
        "                    paragraph = re.sub(r'\\$\\$(.*?)\\$\\$', r'\\\\[\\1\\\\]', paragraph)\n",
        "                    tex_content += paragraph + \"\\n\\n\"\n",
        "                else:\n",
        "                    if not paragraph.startswith(\"\\\\\"):\n",
        "                        if not in_paragraph:\n",
        "                            in_paragraph = True\n",
        "                        tex_content += paragraph + \" \"\n",
        "\n",
        "        if in_itemize:\n",
        "            tex_content += \"\\\\end{itemize}\\n\"\n",
        "        if in_paragraph:\n",
        "            tex_content += \"\\n\\n\"\n",
        "\n",
        "    tex_content += \"\\\\end{document}\"\n",
        "    return tex_content\n",
        "\n",
        "\n",
        "def create_tex_and_compile(tex_content, filename=\"output\"):\n",
        "    \"\"\"\n",
        "    Create and compile LaTeX file.\n",
        "    Returns the path to the generated PDF file.\n",
        "    \"\"\"\n",
        "    filename = os.path.normpath(filename)\n",
        "    tex_file = f\"{filename}.tex\"\n",
        "\n",
        "    with open(tex_file, \"w\", encoding='utf-8') as f:\n",
        "        f.write(tex_content)\n",
        "\n",
        "    # Run pdflatex (you might need to run this twice for proper references)\n",
        "    os.system(f'pdflatex -interaction=nonstopmode \"{tex_file}\"')\n",
        "    pdf_file = f\"{filename}.pdf\"\n",
        "    return pdf_file\n",
        "\n",
        "\n",
        "def scrape_images(keyword, num_images=3):\n",
        "    \"\"\"Scrape images from GeeksForGeeks using SERP API.\"\"\"\n",
        "    api_key = \"df7729cfcc6f85cfea8e18cb9f13ce5180a3ea6e1c22e2e5f3de3ea16bd6e40b\"\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": keyword,\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        images = []\n",
        "        if \"images_results\" in data:\n",
        "            for image in data[\"images_results\"]:\n",
        "                if 'geeksforgeeks' in image.get('source', '').lower():\n",
        "                    images.append(image[\"original\"])\n",
        "                    if len(images) >= num_images:\n",
        "                        break\n",
        "            return images\n",
        "        return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve images. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def create_title_slide(prs, title):\n",
        "    \"\"\"Create the title slide with specific formatting.\"\"\"\n",
        "    title_slide_layout = prs.slide_layouts[0]\n",
        "    slide = prs.slides.add_slide(title_slide_layout)\n",
        "    title_shape = slide.shapes.title\n",
        "    title_shape.text = title\n",
        "\n",
        "    title_frame = title_shape.text_frame\n",
        "    paragraph = title_frame.paragraphs[0]\n",
        "    paragraph.alignment = PP_ALIGN.CENTER\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(44)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "\n",
        "def format_paragraph(paragraph, text, level=0, font_size=17, bold=False):\n",
        "    \"\"\"Helper function to format a paragraph with proper text and styling.\"\"\"\n",
        "    paragraph.text = text\n",
        "    paragraph.level = level\n",
        "    paragraph.alignment = PP_ALIGN.LEFT\n",
        "\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(font_size)\n",
        "    run.font.bold = bold\n",
        "    return paragraph\n",
        "\n",
        "\n",
        "def create_section_slide(prs, title, content):\n",
        "    \"\"\"Create section slides with improved formatting for content.\"\"\"\n",
        "    content_slide_layout = prs.slide_layouts[1]\n",
        "    slide = prs.slides.add_slide(content_slide_layout)\n",
        "\n",
        "    # Format slide title\n",
        "    title_shape = slide.shapes.title\n",
        "    title_frame = title_shape.text_frame\n",
        "    title_para = title_frame.paragraphs[0]\n",
        "    title_para.text = title\n",
        "    title_para.alignment = PP_ALIGN.LEFT\n",
        "    run = title_para.runs[0]\n",
        "    run.font.size = Pt(36)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "    # Format content placeholder\n",
        "    content_shape = slide.placeholders[1]\n",
        "    content_frame = content_shape.text_frame\n",
        "    content_frame.clear()\n",
        "\n",
        "    # Process content by paragraphs\n",
        "    paragraphs = content.split('\\n')\n",
        "    for i, para_text in enumerate(paragraphs):\n",
        "        para_text = para_text.strip()\n",
        "        if not para_text:\n",
        "            continue\n",
        "        if i == 0:\n",
        "            para = content_frame.paragraphs[0]\n",
        "        else:\n",
        "            para = content_frame.add_paragraph()\n",
        "        # Bold if the whole line is wrapped in **\n",
        "        if para_text.startswith('**') and para_text.endswith('**'):\n",
        "            text = para_text.strip('*')\n",
        "            format_paragraph(para, text, level=0, font_size=17, bold=True)\n",
        "        else:\n",
        "            format_paragraph(para, para_text, level=0, font_size=17, bold=False)\n",
        "\n",
        "\n",
        "def create_image_slide(prs, title, image_url):\n",
        "    \"\"\"Create a slide with a single centered image.\"\"\"\n",
        "    blank_slide_layout = prs.slide_layouts[6]\n",
        "    slide = prs.slides.add_slide(blank_slide_layout)\n",
        "\n",
        "    try:\n",
        "        # Add title textbox\n",
        "        title_box = slide.shapes.add_textbox(Inches(1), Inches(0.5), Inches(11), Inches(1))\n",
        "        title_frame = title_box.text_frame\n",
        "        paragraph = title_frame.add_paragraph()\n",
        "        paragraph.text = title\n",
        "        paragraph.alignment = PP_ALIGN.CENTER\n",
        "        run = paragraph.runs[0]\n",
        "        run.font.size = Pt(32)\n",
        "        run.font.bold = True\n",
        "        run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(io.BytesIO(response.content))\n",
        "            if image.format == 'WEBP':\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "            img_path = f\"temp_image_{title.replace(' ', '_')}.png\"\n",
        "            image.save(img_path, 'PNG')\n",
        "\n",
        "            max_width = Inches(10)\n",
        "            max_height = Inches(6)\n",
        "\n",
        "            slide.shapes.add_picture(\n",
        "                img_path,\n",
        "                Inches(1.67),\n",
        "                Inches(2),\n",
        "                width=max_width,\n",
        "                height=max_height\n",
        "            )\n",
        "            os.remove(img_path)\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating image slide: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def split_content_for_slides(content, max_chars=550):\n",
        "    \"\"\"Split content into multiple slides with better handling.\"\"\"\n",
        "    if len(content) <= max_chars:\n",
        "        return [content]\n",
        "\n",
        "    parts = []\n",
        "    current_part = \"\"\n",
        "    # Split by newlines and sentence boundaries\n",
        "    paragraphs = re.split(r'(?<=[.!?])\\s+(?=[A-Z])|(?:\\r?\\n){2,}', content)\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        if len(para) > max_chars:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "            for sentence in sentences:\n",
        "                if len(current_part) + len(sentence) > max_chars:\n",
        "                    if current_part:\n",
        "                        parts.append(current_part.strip())\n",
        "                        current_part = sentence + ' '\n",
        "                else:\n",
        "                    current_part += sentence + ' '\n",
        "        else:\n",
        "            if len(current_part) + len(para) > max_chars:\n",
        "                parts.append(current_part.strip())\n",
        "                current_part = para + '\\n'\n",
        "            else:\n",
        "                current_part += para + '\\n'\n",
        "    if current_part:\n",
        "        parts.append(current_part.strip())\n",
        "    return parts\n",
        "\n",
        "\n",
        "def preprocess_pdf_content(content):\n",
        "    \"\"\"Preprocess PDF content for better slide organization.\"\"\"\n",
        "    # Replace multiple spaces/tabs but preserve newlines.\n",
        "    content = re.sub(r'[ \\t]+', ' ', content)\n",
        "    content = re.sub(r'\\n+', '\\n', content)\n",
        "    # Insert a newline before our section markers if not present.\n",
        "    content = re.sub(r'([^\\n])(@@@SECTION:)', r'\\1\\n\\2', content)\n",
        "    return content\n",
        "\n",
        "\n",
        "def clean_extracted_text(text):\n",
        "    \"\"\"\n",
        "    Clean the extracted text by:\n",
        "    1. Removing leading line numbers and stray replacement characters.\n",
        "    2. Joining broken lines where a sentence is split mid-sentence.\n",
        "    \"\"\"\n",
        "    # Remove common stray characters (like the Unicode replacement char \"�\").\n",
        "    text = text.replace(\"�\", \"\")\n",
        "\n",
        "    # Split text into lines.\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "\n",
        "    # Remove leading numbers (e.g., \"123\" or \"123:\" at the start of each line).\n",
        "    for line in lines:\n",
        "        line = re.sub(r'^\\d+[\\.\\:]*\\s*', '', line)\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    # Now join lines if the current line does not end with typical punctuation\n",
        "    # and the next line starts with a lowercase letter or is very short (e.g., less than 4 words).\n",
        "    joined_lines = []\n",
        "    i = 0\n",
        "    while i < len(cleaned_lines):\n",
        "        current_line = cleaned_lines[i].strip()\n",
        "        # Continue joining subsequent lines as long as the condition holds.\n",
        "        while (i + 1 < len(cleaned_lines)):\n",
        "            next_line = cleaned_lines[i + 1].strip()\n",
        "            # If current line doesn't end with punctuation and next line appears to be a sentence continuation,\n",
        "            # join them.\n",
        "            if (current_line and current_line[-1] not in \".!?:;\" and\n",
        "                (next_line and (next_line[0].islower() or len(next_line.split()) < 4))):\n",
        "                current_line = current_line.rstrip() + \" \" + next_line.lstrip()\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "        joined_lines.append(current_line)\n",
        "        i += 1\n",
        "\n",
        "    # Reconstruct the text.\n",
        "    cleaned_text = \"\\n\".join(joined_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def create_presentation(full_text, query):\n",
        "    \"\"\"Creates the PowerPoint presentation with content and visualizations\"\"\"\n",
        "    prs = Presentation(\"Crop.pptx\")\n",
        "    prs.slide_width = Inches(13.333)\n",
        "    prs.slide_height = Inches(9)\n",
        "\n",
        "    create_title_slide(prs, query.upper())\n",
        "\n",
        "    try:\n",
        "        # Process content\n",
        "        processed_content = preprocess_pdf_content(full_text)\n",
        "        processed_content = clean_extracted_text(processed_content)\n",
        "        sections = re.split(r'@@@SECTION:\\s*(.*?)\\s*@@@', processed_content)\n",
        "        if sections[0].strip() == '':\n",
        "            sections = sections[1:]\n",
        "\n",
        "        images_added = 0\n",
        "        used_keywords = set()\n",
        "        groq_client = GroqClient(GROQ_API_KEY)\n",
        "\n",
        "        # Process each section\n",
        "        for i in range(0, len(sections), 2):\n",
        "            if i + 1 >= len(sections):\n",
        "                break\n",
        "\n",
        "            sec_title = sections[i].strip()\n",
        "            sec_body = sections[i + 1].strip()\n",
        "\n",
        "            # Create content slides\n",
        "            content_parts = split_content_for_slides(sec_body)\n",
        "            for j, part in enumerate(content_parts):\n",
        "                slide_title = f\"{sec_title}\" if j == 0 else f\"{sec_title} (continued)\"\n",
        "                create_section_slide(prs, slide_title, part)\n",
        "\n",
        "            # Generate and add visualization slides\n",
        "            if images_added < 3:\n",
        "                try:\n",
        "                    # Get keywords using Groq\n",
        "                    response = groq_client.get_completion(sec_body)\n",
        "                    keywords = extract_keywords(response)\n",
        "                    available_keywords = [k for k in keywords if k not in used_keywords]\n",
        "\n",
        "                    if available_keywords:\n",
        "                        keyword = available_keywords[0]\n",
        "                        images = scrape_images(keyword, num_images=1)\n",
        "                        if images:\n",
        "                            image_slide_title = f\"{sec_title} - Visualization\"\n",
        "                            if create_image_slide(prs, image_slide_title, images[0]):\n",
        "                                images_added += 1\n",
        "                                used_keywords.add(keyword)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing section {sec_title}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Save the final presentation\n",
        "        output_filename = f\"{query.upper()}.pptx\"\n",
        "        prs.save(output_filename)\n",
        "        print(f\"Presentation created successfully! Images added: {images_added}\")\n",
        "        print(f\"Saved as: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating presentation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text content from a PDF file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text_content = []\n",
        "            for page in pdf_reader.pages:\n",
        "                text_content.append(page.extract_text())\n",
        "            return '\\n\\n'.join(text_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a query/title for the presentation.\n",
        "\n",
        "\n",
        "    # Read the content from a text file (content.txt)\n",
        "    with open('content.txt', 'r', encoding='utf-8') as file:\n",
        "        original_content = file.read()\n",
        "\n",
        "    # Convert the content to LaTeX.\n",
        "    tex_content = convert_to_tex(original_content)\n",
        "    # Optionally, save tex_content to inspect it.\n",
        "    # with open(\"debug_output.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    #     f.write(tex_content)\n",
        "\n",
        "    # Create PDF from the LaTeX file.\n",
        "    pdf_path = create_tex_and_compile(tex_content)\n",
        "\n",
        "    # Extract text from the generated PDF.\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "    # Uncomment next line to debug extracted text.\n",
        "    # print(\"Extracted PDF text:\\n\", extracted_text)\n",
        "\n",
        "    # Create the presentation using the extracted text and query title.\n",
        "    create_presentation(extracted_text, query)\n"
      ],
      "metadata": {
        "id": "CVPCTKMvg-33",
        "outputId": "93edb709-10cb-4d91-d75d-8cdf6b704e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation created successfully! Images added: 0\n",
            "Saved as: LSTM.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from pptx.dml.color import RGBColor\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "def convert_to_tex(content):\n",
        "    \"\"\"\n",
        "    Convert content to LaTeX with proper section hierarchy and special handling for code blocks and equations.\n",
        "    We insert unique markers (@@@SECTION: ... @@@) that will survive the conversion for easier text extraction.\n",
        "    \"\"\"\n",
        "    tex_content = r\"\"\"\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\\usepackage{listings}\n",
        "\\usepackage{xcolor}\n",
        "\\usepackage{enumitem}\n",
        "\\usepackage{titlesec}\n",
        "\\usepackage{geometry}\n",
        "\\usepackage{verbatim}\n",
        "\n",
        "\\geometry{\n",
        "    a4paper,\n",
        "    left=1in,\n",
        "    right=1in,\n",
        "    top=1in,\n",
        "    bottom=1in\n",
        "}\n",
        "\n",
        "% Format section titles\n",
        "\\titleformat{\\section}\n",
        "  {\\Large\\bfseries}\n",
        "  {}\n",
        "  {0em}\n",
        "  {}\n",
        "\n",
        "% Code listing settings\n",
        "\\lstset{\n",
        "    backgroundcolor=\\color{gray!10},\n",
        "    basicstyle=\\ttfamily\\small,\n",
        "    breaklines=true,\n",
        "    frame=single,\n",
        "    numbers=left,\n",
        "    numberstyle=\\tiny,\n",
        "    keywordstyle=\\color{blue},\n",
        "    commentstyle=\\color{green!60!black},\n",
        "    stringstyle=\\color{red},\n",
        "    tabsize=4,\n",
        "    showspaces=false,\n",
        "    showstringspaces=false\n",
        "}\n",
        "\n",
        "\\begin{document}\n",
        "\\pagestyle{plain}\n",
        "\"\"\"\n",
        "    # Split on \"## \" markers (if present)\n",
        "    sections = content.split(\"## \")\n",
        "    for section in sections[1:]:\n",
        "        if not section.strip():\n",
        "            continue\n",
        "\n",
        "        # Process main section headers with the expected format (e.g. \"1. **Title**\")\n",
        "        title_match = re.match(r'\\d+\\.\\s+\\*\\*(.*?)\\*\\*', section)\n",
        "        if title_match:\n",
        "            section_title = title_match.group(1).strip()\n",
        "            # Insert our unique marker for later extraction:\n",
        "            tex_content += f\"\\\\section*{{@@@SECTION: {section_title} @@@}}\\n\\n\"\n",
        "            section = re.sub(r'\\d+\\.\\s+\\*\\*.*?\\*\\*\\n', '', section)\n",
        "\n",
        "        paragraphs = section.split(\"\\n\")\n",
        "        in_itemize = False\n",
        "        in_paragraph = False\n",
        "        in_code_block = False\n",
        "        in_verbatim = False\n",
        "        code_content = []\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            original_paragraph = paragraph\n",
        "            paragraph = paragraph.rstrip(\"\\n\")\n",
        "            if not paragraph.strip():\n",
        "                if in_itemize:\n",
        "                    tex_content += \"\\\\end{itemize}\\n\"\n",
        "                    in_itemize = False\n",
        "                if in_paragraph:\n",
        "                    tex_content += \"\\n\\n\"\n",
        "                    in_paragraph = False\n",
        "                continue\n",
        "\n",
        "            # Handle verbatim/code blocks\n",
        "            if \"egin{verbatim}\" in paragraph:\n",
        "                in_verbatim = True\n",
        "                tex_content += \"\\\\begin{lstlisting}[language=Python]\\n\"\n",
        "                continue\n",
        "            elif \"\\\\end{verbatim}\" in paragraph:\n",
        "                in_verbatim = False\n",
        "                tex_content += \"\\\\end{lstlisting}\\n\\n\"\n",
        "                continue\n",
        "            elif in_verbatim:\n",
        "                tex_content += paragraph.lstrip() + \"\\n\"\n",
        "                continue\n",
        "            elif paragraph.startswith(\"```python\"):\n",
        "                in_code_block = True\n",
        "                code_content = []\n",
        "                continue\n",
        "            elif paragraph.startswith(\"```\") and in_code_block:\n",
        "                in_code_block = False\n",
        "                tex_content += \"\\\\begin{lstlisting}[language=Python]\\n\"\n",
        "                tex_content += \"\\n\".join(code_content)\n",
        "                tex_content += \"\\n\\\\end{lstlisting}\\n\\n\"\n",
        "                code_content = []\n",
        "                continue\n",
        "            elif in_code_block:\n",
        "                code_content.append(paragraph)\n",
        "                continue\n",
        "\n",
        "            # Handle bullet points and numbered lists\n",
        "            elif paragraph.lstrip().startswith(\"• \"):\n",
        "                if not in_itemize:\n",
        "                    tex_content += \"\\\\begin{itemize}\\n\"\n",
        "                    in_itemize = True\n",
        "                tex_content += f\"\\\\item {paragraph.lstrip()[2:]}\\n\"\n",
        "            elif re.match(r'^\\d+\\.\\s', paragraph):\n",
        "                if not in_itemize:\n",
        "                    tex_content += \"\\\\begin{itemize}\\n\"\n",
        "                    in_itemize = True\n",
        "                paragraph = re.sub(r'^\\d+\\.\\s', '', paragraph)\n",
        "                tex_content += f\"\\\\item {paragraph}\\n\"\n",
        "            else:\n",
        "                if in_itemize:\n",
        "                    tex_content += \"\\\\end{itemize}\\n\"\n",
        "                    in_itemize = False\n",
        "\n",
        "                # Handle equations (display math mode)\n",
        "                if paragraph.count('$') >= 2:\n",
        "                    paragraph = re.sub(r'\\$\\$(.*?)\\$\\$', r'\\\\[\\1\\\\]', paragraph)\n",
        "                    tex_content += paragraph + \"\\n\\n\"\n",
        "                else:\n",
        "                    if not paragraph.startswith(\"\\\\\"):\n",
        "                        if not in_paragraph:\n",
        "                            in_paragraph = True\n",
        "                        tex_content += paragraph + \" \"\n",
        "\n",
        "        if in_itemize:\n",
        "            tex_content += \"\\\\end{itemize}\\n\"\n",
        "        if in_paragraph:\n",
        "            tex_content += \"\\n\\n\"\n",
        "\n",
        "    tex_content += \"\\\\end{document}\"\n",
        "    return tex_content\n",
        "\n",
        "\n",
        "def create_tex_and_compile(tex_content, filename=\"output\"):\n",
        "    \"\"\"\n",
        "    Create and compile LaTeX file.\n",
        "    Returns the path to the generated PDF file.\n",
        "    \"\"\"\n",
        "    filename = os.path.normpath(filename)\n",
        "    tex_file = f\"{filename}.tex\"\n",
        "\n",
        "    with open(tex_file, \"w\", encoding='utf-8') as f:\n",
        "        f.write(tex_content)\n",
        "\n",
        "    # Run pdflatex (you might need to run this twice for proper references)\n",
        "    os.system(f'pdflatex -interaction=nonstopmode \"{tex_file}\"')\n",
        "    pdf_file = f\"{filename}.pdf\"\n",
        "    return pdf_file\n",
        "\n",
        "\n",
        "def scrape_images(keyword, num_images=3):\n",
        "    \"\"\"Scrape images from GeeksForGeeks using SERP API.\"\"\"\n",
        "    api_key = \"df7729cfcc6f85cfea8e18cb9f13ce5180a3ea6e1c22e2e5f3de3ea16bd6e40b\"\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": keyword,\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        images = []\n",
        "        if \"images_results\" in data:\n",
        "            for image in data[\"images_results\"]:\n",
        "                if 'geeksforgeeks' in image.get('source', '').lower():\n",
        "                    images.append(image[\"original\"])\n",
        "                    if len(images) >= num_images:\n",
        "                        break\n",
        "            return images\n",
        "        return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve images. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def create_image_keyword_prompt(content):\n",
        "    \"\"\"Creates a prompt for generating image search keywords.\"\"\"\n",
        "    prompt = f\"\"\"You are a technical visualization expert. Extract specific image search keywords from the following content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Your task: Generate 5-7 image search keywords that would help find educational diagrams and technical visualizations related to the main concepts in this content.\n",
        "\n",
        "Focus on identifying keywords for:\n",
        "1. System/concept architecture diagrams\n",
        "2. Process flows and sequences\n",
        "3. Component interactions\n",
        "4. Implementation details\n",
        "5. Working mechanisms\n",
        "6. Step-by-step procedures\n",
        "7. Technical examples\n",
        "\n",
        "Keyword Generation Rules:\n",
        "1. Each keyword MUST use hyphens between words\n",
        "2. Each keyword MUST end with exactly ONE of these suffixes:\n",
        "   - -visualization\n",
        "   - -diagram\n",
        "   - -illustration\n",
        "   - -example\n",
        "   - -steps\n",
        "\n",
        "Key Requirements:\n",
        "- Use specific technical terms from the content\n",
        "- Include major concepts and processes\n",
        "- Avoid generic/non-specific terms\n",
        "- Keywords must directly relate to main topics\n",
        "- Each keyword should help find relevant technical diagrams\n",
        "\n",
        "Output Format:\n",
        "ONLY provide a comma-separated list of keywords. Example:\n",
        "concept-architecture-diagram, process-flow-visualization, component-interaction-illustration\n",
        "\n",
        "KEYWORDS (comma-separated):\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def extract_answer(api_response):\n",
        "    \"\"\"Extracts and validates keywords from the API response.\"\"\"\n",
        "    try:\n",
        "        valid_suffixes = ['-visualization', '-diagram', '-illustration', '-example', '-steps']\n",
        "        if isinstance(api_response, list):\n",
        "            response_text = api_response[0].get('generated_text', '')\n",
        "        elif isinstance(api_response, dict):\n",
        "            response_text = api_response.get('generated_text', '')\n",
        "        else:\n",
        "            response_text = str(api_response)\n",
        "\n",
        "        if \"KEYWORDS (comma-separated):\" not in response_text:\n",
        "            return []\n",
        "\n",
        "        keywords_section = response_text.split(\"KEYWORDS (comma-separated):\")[-1].strip()\n",
        "        keyword_lines = [line.strip() for line in keywords_section.split('\\n') if line.strip()]\n",
        "        if not keyword_lines:\n",
        "            return []\n",
        "\n",
        "        keywords = keyword_lines[0]\n",
        "        cleaned_keywords = []\n",
        "        for keyword in keywords.split(','):\n",
        "            keyword = keyword.strip()\n",
        "            if (keyword and '-' in keyword and\n",
        "                any(keyword.endswith(suffix) for suffix in valid_suffixes) and\n",
        "                keyword.count('-') >= 2):\n",
        "                cleaned_keywords.append(keyword)\n",
        "        cleaned_keywords = list(set(cleaned_keywords))\n",
        "        cleaned_keywords = cleaned_keywords[:7]\n",
        "        return cleaned_keywords if len(cleaned_keywords) >= 3 else []\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def query_huggingface_api(prompt, max_length=25000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.5,\n",
        "            \"top_p\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def create_title_slide(prs, title):\n",
        "    \"\"\"Create the title slide with specific formatting.\"\"\"\n",
        "    title_slide_layout = prs.slide_layouts[0]\n",
        "    slide = prs.slides.add_slide(title_slide_layout)\n",
        "    title_shape = slide.shapes.title\n",
        "    title_shape.text = title\n",
        "\n",
        "    title_frame = title_shape.text_frame\n",
        "    paragraph = title_frame.paragraphs[0]\n",
        "    paragraph.alignment = PP_ALIGN.CENTER\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(44)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "\n",
        "def format_paragraph(paragraph, text, level=0, font_size=17, bold=False):\n",
        "    \"\"\"Helper function to format a paragraph with proper text and styling.\"\"\"\n",
        "    paragraph.text = text\n",
        "    paragraph.level = level\n",
        "    paragraph.alignment = PP_ALIGN.LEFT\n",
        "\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(font_size)\n",
        "    run.font.bold = bold\n",
        "    return paragraph\n",
        "\n",
        "\n",
        "def create_section_slide(prs, title, content):\n",
        "    \"\"\"Create section slides with improved formatting for content.\"\"\"\n",
        "    content_slide_layout = prs.slide_layouts[1]\n",
        "    slide = prs.slides.add_slide(content_slide_layout)\n",
        "\n",
        "    # Format slide title\n",
        "    title_shape = slide.shapes.title\n",
        "    title_frame = title_shape.text_frame\n",
        "    title_para = title_frame.paragraphs[0]\n",
        "    title_para.text = title\n",
        "    title_para.alignment = PP_ALIGN.LEFT\n",
        "    run = title_para.runs[0]\n",
        "    run.font.size = Pt(36)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "    # Format content placeholder\n",
        "    content_shape = slide.placeholders[1]\n",
        "    content_frame = content_shape.text_frame\n",
        "    content_frame.clear()\n",
        "\n",
        "    # Process content by paragraphs\n",
        "    paragraphs = content.split('\\n')\n",
        "    for i, para_text in enumerate(paragraphs):\n",
        "        para_text = para_text.strip()\n",
        "        if not para_text:\n",
        "            continue\n",
        "        if i == 0:\n",
        "            para = content_frame.paragraphs[0]\n",
        "        else:\n",
        "            para = content_frame.add_paragraph()\n",
        "        # Bold if the whole line is wrapped in **\n",
        "        if para_text.startswith('**') and para_text.endswith('**'):\n",
        "            text = para_text.strip('*')\n",
        "            format_paragraph(para, text, level=0, font_size=17, bold=True)\n",
        "        else:\n",
        "            format_paragraph(para, para_text, level=0, font_size=17, bold=False)\n",
        "\n",
        "\n",
        "def create_image_slide(prs, title, image_url):\n",
        "    \"\"\"Create a slide with a single centered image.\"\"\"\n",
        "    blank_slide_layout = prs.slide_layouts[6]\n",
        "    slide = prs.slides.add_slide(blank_slide_layout)\n",
        "\n",
        "    try:\n",
        "        # Add title textbox\n",
        "        title_box = slide.shapes.add_textbox(Inches(1), Inches(0.5), Inches(11), Inches(1))\n",
        "        title_frame = title_box.text_frame\n",
        "        paragraph = title_frame.add_paragraph()\n",
        "        paragraph.text = title\n",
        "        paragraph.alignment = PP_ALIGN.CENTER\n",
        "        run = paragraph.runs[0]\n",
        "        run.font.size = Pt(32)\n",
        "        run.font.bold = True\n",
        "        run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(io.BytesIO(response.content))\n",
        "            if image.format == 'WEBP':\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "            img_path = f\"temp_image_{title.replace(' ', '_')}.png\"\n",
        "            image.save(img_path, 'PNG')\n",
        "\n",
        "            max_width = Inches(10)\n",
        "            max_height = Inches(6)\n",
        "\n",
        "            slide.shapes.add_picture(\n",
        "                img_path,\n",
        "                Inches(1.67),\n",
        "                Inches(2),\n",
        "                width=max_width,\n",
        "                height=max_height\n",
        "            )\n",
        "            os.remove(img_path)\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating image slide: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def split_content_for_slides(content, max_chars=550):\n",
        "    \"\"\"Split content into multiple slides with better handling.\"\"\"\n",
        "    if len(content) <= max_chars:\n",
        "        return [content]\n",
        "\n",
        "    parts = []\n",
        "    current_part = \"\"\n",
        "    # Split by newlines and sentence boundaries\n",
        "    paragraphs = re.split(r'(?<=[.!?])\\s+(?=[A-Z])|(?:\\r?\\n){2,}', content)\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        if len(para) > max_chars:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "            for sentence in sentences:\n",
        "                if len(current_part) + len(sentence) > max_chars:\n",
        "                    if current_part:\n",
        "                        parts.append(current_part.strip())\n",
        "                        current_part = sentence + ' '\n",
        "                else:\n",
        "                    current_part += sentence + ' '\n",
        "        else:\n",
        "            if len(current_part) + len(para) > max_chars:\n",
        "                parts.append(current_part.strip())\n",
        "                current_part = para + '\\n'\n",
        "            else:\n",
        "                current_part += para + '\\n'\n",
        "    if current_part:\n",
        "        parts.append(current_part.strip())\n",
        "    return parts\n",
        "\n",
        "\n",
        "def preprocess_pdf_content(content):\n",
        "    \"\"\"Preprocess PDF content for better slide organization.\"\"\"\n",
        "    # Replace multiple spaces/tabs but preserve newlines.\n",
        "    content = re.sub(r'[ \\t]+', ' ', content)\n",
        "    content = re.sub(r'\\n+', '\\n', content)\n",
        "    # Insert a newline before our section markers if not present.\n",
        "    content = re.sub(r'([^\\n])(@@@SECTION:)', r'\\1\\n\\2', content)\n",
        "    return content\n",
        "\n",
        "\n",
        "def clean_extracted_text(text):\n",
        "    \"\"\"\n",
        "    Clean the extracted text by:\n",
        "    1. Removing leading line numbers and stray replacement characters.\n",
        "    2. Joining broken lines where a sentence is split mid-sentence.\n",
        "    \"\"\"\n",
        "    # Remove common stray characters (like the Unicode replacement char \"�\").\n",
        "    text = text.replace(\"�\", \"\")\n",
        "\n",
        "    # Split text into lines.\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "\n",
        "    # Remove leading numbers (e.g., \"123\" or \"123:\" at the start of each line).\n",
        "    for line in lines:\n",
        "        line = re.sub(r'^\\d+[\\.\\:]*\\s*', '', line)\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    # Now join lines if the current line does not end with typical punctuation\n",
        "    # and the next line starts with a lowercase letter or is very short (e.g., less than 4 words).\n",
        "    joined_lines = []\n",
        "    i = 0\n",
        "    while i < len(cleaned_lines):\n",
        "        current_line = cleaned_lines[i].strip()\n",
        "        # Continue joining subsequent lines as long as the condition holds.\n",
        "        while (i + 1 < len(cleaned_lines)):\n",
        "            next_line = cleaned_lines[i + 1].strip()\n",
        "            # If current line doesn't end with punctuation and next line appears to be a sentence continuation,\n",
        "            # join them.\n",
        "            if (current_line and current_line[-1] not in \".!?:;\" and\n",
        "                (next_line and (next_line[0].islower() or len(next_line.split()) < 4))):\n",
        "                current_line = current_line.rstrip() + \" \" + next_line.lstrip()\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "        joined_lines.append(current_line)\n",
        "        i += 1\n",
        "\n",
        "    # Reconstruct the text.\n",
        "    cleaned_text = \"\\n\".join(joined_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def create_presentation(full_text, query):\n",
        "    \"\"\"Create the full presentation with content-matched images.\"\"\"\n",
        "    # Load an existing PPTX template or create a new Presentation object.\n",
        "    prs = Presentation(\"Crop.pptx\")\n",
        "    prs.slide_width = Inches(13.333)\n",
        "    prs.slide_height = Inches(9)\n",
        "\n",
        "    create_title_slide(prs, query.upper())\n",
        "\n",
        "    try:\n",
        "        processed_content = preprocess_pdf_content(full_text)\n",
        "        # Clean the text further to fix line-break issues.\n",
        "        processed_content = clean_extracted_text(processed_content)\n",
        "        # Split content on our unique section marker.\n",
        "        sections = re.split(r'@@@SECTION:\\s*(.*?)\\s*@@@', processed_content)\n",
        "        if sections[0].strip() == '':\n",
        "            sections = sections[1:]\n",
        "\n",
        "        images_added = 0\n",
        "        used_keywords = set()\n",
        "\n",
        "        # sections should alternate: [title, body, title, body, ...]\n",
        "        for i in range(0, len(sections), 2):\n",
        "            if i + 1 >= len(sections):\n",
        "                break\n",
        "            sec_title = sections[i].strip()\n",
        "            sec_body = sections[i + 1].strip()\n",
        "\n",
        "            content_parts = split_content_for_slides(sec_body)\n",
        "            for j, part in enumerate(content_parts):\n",
        "                slide_title = f\"{sec_title}\" if j == 0 else f\"{sec_title} (continued)\"\n",
        "                create_section_slide(prs, slide_title, part)\n",
        "\n",
        "            if images_added < 3:\n",
        "                # Generate keywords for this section.\n",
        "                prompt = create_image_keyword_prompt(sec_body)\n",
        "                api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "                keywords = extract_answer(api_response)\n",
        "                available_keywords = [k for k in keywords if k not in used_keywords]\n",
        "                if available_keywords:\n",
        "                    keyword = available_keywords[0]\n",
        "                    images = scrape_images(keyword, num_images=1)\n",
        "                    if images:\n",
        "                        image_slide_title = f\"{sec_title} - Visualization\"\n",
        "                        if create_image_slide(prs, image_slide_title, images[0]):\n",
        "                            images_added += 1\n",
        "                            used_keywords.add(keyword)\n",
        "\n",
        "        output_filename = query.upper() + '.pptx'\n",
        "        prs.save(output_filename)\n",
        "        print(\"Presentation created successfully!\")\n",
        "        print(f\"Total images added: {images_added}\")\n",
        "        print(f\"Saved as: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating presentation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text content from a PDF file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text_content = []\n",
        "            for page in pdf_reader.pages:\n",
        "                text_content.append(page.extract_text())\n",
        "            return '\\n\\n'.join(text_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a query/title for the presentation.\n",
        "\n",
        "\n",
        "    # Read the content from a text file (content.txt)\n",
        "    with open('content.txt', 'r', encoding='utf-8') as file:\n",
        "        original_content = file.read()\n",
        "\n",
        "    # Convert the content to LaTeX.\n",
        "    tex_content = convert_to_tex(original_content)\n",
        "    # Optionally, save tex_content to inspect it.\n",
        "    # with open(\"debug_output.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    #     f.write(tex_content)\n",
        "\n",
        "    # Create PDF from the LaTeX file.\n",
        "    pdf_path = create_tex_and_compile(tex_content)\n",
        "\n",
        "    # Extract text from the generated PDF.\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "    # Uncomment next line to debug extracted text.\n",
        "    # print(\"Extracted PDF text:\\n\", extracted_text)\n",
        "\n",
        "    # Create the presentation using the extracted text and query title.\n",
        "    create_presentation(extracted_text, query)\n"
      ],
      "metadata": {
        "id": "XEIgN97be92S",
        "outputId": "da81cac5-a524-4069-c862-e63f7b50fdee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation created successfully!\n",
            "Total images added: 3\n",
            "Saved as: LSTM.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 python-pptx Pillow\n"
      ],
      "metadata": {
        "id": "mpTey3-jwPYP",
        "outputId": "c1d205a4-7c88-4f40-f9ec-30c78392ee44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf\n",
        "!pip install PyPDF2 python-pptx"
      ],
      "metadata": {
        "id": "mTMvIfuQt6yE",
        "outputId": "38cda5ce-6b02-407c-9e7e-ae54b34e2c62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from pptx.dml.color import RGBColor\n",
        "import re\n",
        "\n",
        "def create_title_slide(prs, title):\n",
        "    \"\"\"Create the title slide with specific formatting\"\"\"\n",
        "    title_slide_layout = prs.slide_layouts[0]\n",
        "    slide = prs.slides.add_slide(title_slide_layout)\n",
        "    title_shape = slide.shapes.title\n",
        "    title_shape.text = title\n",
        "\n",
        "    # Format title\n",
        "    title_frame = title_shape.text_frame\n",
        "    paragraph = title_frame.paragraphs[0]\n",
        "    paragraph.alignment = PP_ALIGN.CENTER\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(44)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "def format_paragraph(paragraph, text, level=0, font_size=17):\n",
        "    \"\"\"Helper function to format a paragraph with proper text and styling\"\"\"\n",
        "    paragraph.text = text\n",
        "    paragraph.level = level\n",
        "    paragraph.alignment = PP_ALIGN.LEFT\n",
        "\n",
        "    # Create a new run if none exists\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.font.size = Pt(font_size)\n",
        "\n",
        "    # Make non-bullet point text bold\n",
        "    if level == 0 and text.strip():\n",
        "        run.font.bold = True\n",
        "\n",
        "    return paragraph\n",
        "\n",
        "def create_section_slide(prs, title, content):\n",
        "    \"\"\"Create a content slide with specific formatting\"\"\"\n",
        "    content_slide_layout = prs.slide_layouts[1]\n",
        "    slide = prs.slides.add_slide(content_slide_layout)\n",
        "\n",
        "    # Add and format title\n",
        "    title_shape = slide.shapes.title\n",
        "    title_shape.text = title\n",
        "    title_frame = title_shape.text_frame\n",
        "    paragraph = title_frame.paragraphs[0]\n",
        "    paragraph.alignment = PP_ALIGN.LEFT\n",
        "    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
        "    run.text = title\n",
        "    run.font.size = Pt(36)\n",
        "    run.font.bold = True\n",
        "    run.font.color.rgb = RGBColor(44, 62, 80)\n",
        "\n",
        "    # Add and format content\n",
        "    content_shape = slide.placeholders[1]\n",
        "    content_frame = content_shape.text_frame\n",
        "    content_frame.clear()  # Clear existing text\n",
        "\n",
        "    # Split content into paragraphs and remove empty lines\n",
        "    paragraphs = [p for p in content.split('\\n') if p.strip()]\n",
        "\n",
        "    for i, para_text in enumerate(paragraphs):\n",
        "        # Get or create paragraph\n",
        "        if i == 0:\n",
        "            p = content_frame.paragraphs[0]\n",
        "        else:\n",
        "            p = content_frame.add_paragraph()\n",
        "\n",
        "        # Format the paragraph based on whether it's a bullet point\n",
        "        if para_text.strip().startswith('•'):\n",
        "            format_paragraph(p, para_text.strip()[1:].strip(), level=1)\n",
        "        else:\n",
        "            format_paragraph(p, para_text.strip(), level=0)\n",
        "\n",
        "def create_presentation(content):\n",
        "    \"\"\"Create the full presentation\"\"\"\n",
        "    prs = Presentation(\"/content/Crop.pptx\")\n",
        "\n",
        "    # Set slide size to widescreen\n",
        "    prs.slide_width = Inches(13.333)\n",
        "    prs.slide_height = Inches(9)\n",
        "\n",
        "    # Create title slide\n",
        "    create_title_slide(prs, query.upper())\n",
        "\n",
        "    try:\n",
        "        # Split content into sections using regex\n",
        "        sections = re.split(r'##\\s+\\d+\\.\\s+\\*\\*([^*]+)\\*\\*', content)\n",
        "\n",
        "        # Remove empty first element if it exists\n",
        "        if sections[0].strip() == '':\n",
        "            sections = sections[1:]\n",
        "\n",
        "        # Process sections in pairs (title and content)\n",
        "        for i in range(0, len(sections), 2):\n",
        "            if i + 1 >= len(sections):\n",
        "                break\n",
        "\n",
        "            title = sections[i].strip()\n",
        "            content = sections[i + 1].strip()\n",
        "\n",
        "            # Split long content into multiple slides if needed\n",
        "            content_parts = split_content_for_slides(content)\n",
        "\n",
        "            for j, part in enumerate(content_parts):\n",
        "                slide_title = f\"{title}\" if j == 0 else f\"{title} (continued)\"\n",
        "                create_section_slide(prs, slide_title, part)\n",
        "\n",
        "        # Save the presentation\n",
        "        prs.save(query.upper() + '.pptx')\n",
        "        print(\"Presentation created successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating presentation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def split_content_for_slides(content, max_chars=450):\n",
        "    \"\"\"Split content into multiple slides if it's too long\"\"\"\n",
        "    if len(content) <= max_chars:\n",
        "        return [content]\n",
        "\n",
        "    parts = []\n",
        "    paragraphs = content.split('\\n')\n",
        "    current_part = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_part) + len(para) > max_chars:\n",
        "            if current_part:\n",
        "                parts.append(current_part.strip())\n",
        "                current_part = para + '\\n'\n",
        "        else:\n",
        "            current_part += para + '\\n'\n",
        "\n",
        "    if current_part:\n",
        "        parts.append(current_part.strip())\n",
        "\n",
        "    return parts\n",
        "\n",
        "# Read the content from paste.txt\n",
        "with open('content.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Create the presentation\n",
        "create_presentation(content)"
      ],
      "metadata": {
        "id": "AafTzf9n__Su",
        "outputId": "afef619e-6286-47a6-bd02-27292049c083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proper work is till the above block"
      ],
      "metadata": {
        "id": "OSNpB-Icbjec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################# CODE FOR SLIDES ###################################\n",
        "# from pptx import Presentation\n",
        "# from pptx.util import Inches, Pt\n",
        "# from pptx.enum.text import PP_ALIGN\n",
        "# import re\n",
        "# import traceback\n",
        "# import os\n",
        "\n",
        "# def render_code_block(text_frame, content_shape, slide, code_block):\n",
        "#     code_lines = code_block.strip('```').split('\\n')\n",
        "#     code_lines = [line for line in code_lines if not line.startswith('```')]\n",
        "#     for line in code_lines:\n",
        "#         para = text_frame.add_paragraph()\n",
        "#         para.text = line\n",
        "#         para.font.name = 'Consolas'\n",
        "#         para.font.size = Pt(14)\n",
        "#         para.space_after = 0\n",
        "#         para.space_before = 0\n",
        "\n",
        "# def process_content_with_equations(text_frame, content_shape, slide, points, prs):\n",
        "#     vertical_position = content_shape.top + Inches(0.2)\n",
        "#     for point in points:\n",
        "#         if point.startswith('```'):\n",
        "#             render_code_block(text_frame, content_shape, slide, point)\n",
        "#             continue\n",
        "\n",
        "#         para = text_frame.add_paragraph()\n",
        "\n",
        "#         # Handle bold and italic text\n",
        "#         if point.startswith('**') and point.endswith('**'):\n",
        "#             para.text = point.strip('**')\n",
        "#             para.font.bold = True\n",
        "#         elif point.startswith('*') and point.endswith('*'):\n",
        "#             para.text = point.strip('*')\n",
        "#             para.font.italic = True\n",
        "#         else:\n",
        "#             clean_point = point.lstrip('- ').lstrip('*').strip()\n",
        "#             para.text = clean_point\n",
        "\n",
        "#         para.font.size = Pt(18)\n",
        "#         para.space_after = 0\n",
        "#         para.space_before = 0\n",
        "#         vertical_position += Inches(0.3)\n",
        "\n",
        "#     return vertical_position\n",
        "\n",
        "# def is_heading(line):\n",
        "#     \"\"\"\n",
        "#     Enhanced heading detection that handles multiple formats:\n",
        "#     1. Numbered headings (1., 2., etc.)\n",
        "#     2. Markdown headings (#, ##, etc.)\n",
        "#     3. Bold headings (**...**)\n",
        "#     \"\"\"\n",
        "#     line = line.strip()\n",
        "\n",
        "#     # Pattern for numbered headings with or without bold markers\n",
        "#     numbered_pattern = r'^\\d+\\.\\s*\\*?\\*?(.+?)\\*?\\*?$'\n",
        "\n",
        "#     # Pattern for markdown headings\n",
        "#     markdown_pattern = r'^#{1,6}\\s+(.+)$'\n",
        "\n",
        "#     # Pattern for bold headings\n",
        "#     bold_pattern = r'^\\*\\*(.+)\\*\\*$'\n",
        "\n",
        "#     if re.match(numbered_pattern, line):\n",
        "#         match = re.match(numbered_pattern, line)\n",
        "#         return True, match.group(1).strip('* ')\n",
        "#     elif re.match(markdown_pattern, line):\n",
        "#         match = re.match(markdown_pattern, line)\n",
        "#         return True, match.group(1).strip()\n",
        "#     elif re.match(bold_pattern, line):\n",
        "#         match = re.match(bold_pattern, line)\n",
        "#         return True, match.group(1).strip()\n",
        "\n",
        "#     return False, None\n",
        "\n",
        "# def is_point(line):\n",
        "#     \"\"\"\n",
        "#     Detect if a line is a bullet point or numbered point\n",
        "#     \"\"\"\n",
        "#     line = line.strip()\n",
        "#     # Match bullet points (-, •) or numbered points (1., 2., etc.)\n",
        "#     return bool(re.match(r'^[-•]\\s+(.+)', line) or\n",
        "#                 re.match(r'^\\d+\\.\\s+(.+)', line))\n",
        "\n",
        "# def create_presentation(content, query, template_path=None):\n",
        "#     try:\n",
        "#         if template_path and os.path.exists(template_path):\n",
        "#             prs = Presentation(template_path)\n",
        "#         else:\n",
        "#             prs = Presentation()\n",
        "\n",
        "#         # Create title slide\n",
        "#         title_slide_layout = prs.slide_layouts[0] if prs.slide_layouts and len(prs.slide_layouts) > 0 else None\n",
        "#         if title_slide_layout:\n",
        "#             slide = prs.slides.add_slide(title_slide_layout)\n",
        "#             title = slide.shapes.title\n",
        "#             subtitle = slide.placeholders[1]\n",
        "#             title.text = query\n",
        "#             subtitle.text = \"Comprehensive Technical Guide\"\n",
        "\n",
        "#         sections = []\n",
        "#         current_section = None\n",
        "#         current_points = []\n",
        "\n",
        "#         lines = content.split('\\n')\n",
        "#         in_code_block = False\n",
        "#         current_code_block = []\n",
        "\n",
        "#         for line in lines:\n",
        "#             # Handle code blocks\n",
        "#             if line.startswith('```'):\n",
        "#                 if in_code_block:\n",
        "#                     current_points.append('\\n'.join(current_code_block))\n",
        "#                     current_code_block = []\n",
        "#                     in_code_block = False\n",
        "#                 else:\n",
        "#                     in_code_block = True\n",
        "#                     current_code_block.append(line)\n",
        "#                 continue\n",
        "\n",
        "#             if in_code_block:\n",
        "#                 current_code_block.append(line)\n",
        "#                 continue\n",
        "\n",
        "#             # Skip empty lines\n",
        "#             if not line.strip():\n",
        "#                 continue\n",
        "\n",
        "#             # Check if line is a heading\n",
        "#             is_head, heading_text = is_heading(line)\n",
        "\n",
        "#             if is_head:\n",
        "#                 # Save previous section if it exists\n",
        "#                 if current_section and current_points:\n",
        "#                     sections.append((current_section, current_points))\n",
        "#                 current_section = heading_text\n",
        "#                 current_points = []\n",
        "#             elif is_point(line):\n",
        "#                 if current_section:  # Only add points if we have a current section\n",
        "#                     current_points.append(line)\n",
        "\n",
        "#         # Add the last section\n",
        "#         if current_section and current_points:\n",
        "#             sections.append((current_section, current_points))\n",
        "\n",
        "#         # Create content slides\n",
        "#         for slide_num, (section, points) in enumerate(sections, start=1):\n",
        "#             content_slide_layout = prs.slide_layouts[1] if len(prs.slide_layouts) > 1 else prs.slide_layouts[0]\n",
        "#             slide = prs.slides.add_slide(content_slide_layout)\n",
        "\n",
        "#             title_shape = slide.shapes.title\n",
        "#             title_shape.text = section\n",
        "#             title_shape.text_frame.paragraphs[0].font.size = Pt(24)\n",
        "\n",
        "#             content_shape = slide.shapes.placeholders[1]\n",
        "#             text_frame = content_shape.text_frame\n",
        "#             text_frame.clear()\n",
        "\n",
        "#             process_content_with_equations(text_frame, content_shape, slide, points, prs)\n",
        "\n",
        "#             # Add page numbers\n",
        "#             left = prs.slide_width - Inches(1.5)\n",
        "#             top = prs.slide_height - Inches(0.75)\n",
        "#             textbox = slide.shapes.add_textbox(left, top, Inches(1), Inches(0.5))\n",
        "#             p = textbox.text_frame.add_paragraph()\n",
        "#             p.text = f\"Page {slide_num + 1} of {len(sections) + 1}\"\n",
        "#             p.font.size = Pt(10)\n",
        "#             p.alignment = PP_ALIGN.RIGHT\n",
        "\n",
        "#         # Save presentation\n",
        "#         filename = re.sub(r'[<>:\"/\\\\|?*]', '', query)\n",
        "#         filename = re.sub(r'\\s+', '_', filename).lower() + '.pptx'\n",
        "#         prs.save(filename)\n",
        "#         return filename\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error in presentation creation: {e}\")\n",
        "#         traceback.print_exc()\n",
        "#         return str(e)\n",
        "# def main():\n",
        "\n",
        "#     template_path = '/content/Crop.pptx'\n",
        "#     filename = create_presentation(clean_answer, query, template_path)\n",
        "#     print(f\"Presentation saved as: {filename}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "-r4IulHOy3P8",
        "outputId": "8829ad66-e5f9-4296-c3f6-c835f49c8201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation saved as: lstm.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pptx import Presentation\n",
        "# from pptx.util import Inches, Pt\n",
        "# from pptx.enum.text import PP_ALIGN\n",
        "# import matplotlib.pyplot as plt\n",
        "# import io\n",
        "# import re\n",
        "# import traceback\n",
        "# import os\n",
        "\n",
        "# def render_code_block(text_frame, content_shape, slide, code_block):\n",
        "#     # Clean up code block\n",
        "#     code_lines = code_block.strip('`').split('\\n')\n",
        "\n",
        "#     # Remove code block delimiters\n",
        "#     code_lines = [line for line in code_lines if not line.startswith('```')]\n",
        "\n",
        "#     # Add code as paragraphs\n",
        "#     for line in code_lines:\n",
        "#         para = text_frame.add_paragraph()\n",
        "#         para.text = line\n",
        "#         para.font.name = 'Consolas'\n",
        "#         para.font.size = Pt(14)\n",
        "#         para.space_after = 0\n",
        "#         para.space_before = 0\n",
        "\n",
        "# def process_content_with_equations(text_frame, content_shape, slide, points, prs):\n",
        "#     vertical_position = content_shape.top + Inches(0.2)\n",
        "\n",
        "#     # Process different types of points\n",
        "#     for point in points:\n",
        "#         # Check if it's a code block\n",
        "#         if point.startswith('```'):\n",
        "#             render_code_block(text_frame, content_shape, slide, point)\n",
        "#             continue\n",
        "\n",
        "#         para = text_frame.add_paragraph()\n",
        "\n",
        "#         # Remove markdown formatting\n",
        "#         clean_point = point.lstrip('- ').lstrip('*').lstrip('**').rstrip('*').rstrip('**').strip()\n",
        "\n",
        "#         # Handle bold text\n",
        "#         if point.startswith('**') and point.endswith('**'):\n",
        "#             clean_point = point.strip('**')\n",
        "#             para.font.bold = True\n",
        "\n",
        "#         # Handle italics\n",
        "#         elif point.startswith('*') and point.endswith('*'):\n",
        "#             clean_point = point.strip('*')\n",
        "#             para.font.italic = True\n",
        "\n",
        "#         para.text = clean_point\n",
        "#         para.font.size = Pt(18)\n",
        "#         para.space_after = 0\n",
        "#         para.space_before = 0\n",
        "\n",
        "#         # Add some vertical spacing\n",
        "#         vertical_position += Inches(0.3)\n",
        "\n",
        "#     return vertical_position\n",
        "\n",
        "# def create_presentation(content, query, template_path=None):\n",
        "#     try:\n",
        "#         # Use template if provided, otherwise create a new presentation\n",
        "#         if template_path and os.path.exists(template_path):\n",
        "#             prs = Presentation(template_path)\n",
        "#         else:\n",
        "#             prs = Presentation()\n",
        "\n",
        "#         # Title slide\n",
        "#         title_slide_layout = prs.slide_layouts[0] if prs.slide_layouts and len(prs.slide_layouts) > 0 else None\n",
        "#         if title_slide_layout:\n",
        "#             slide = prs.slides.add_slide(title_slide_layout)\n",
        "#             title = slide.shapes.title\n",
        "#             subtitle = slide.placeholders[1]\n",
        "\n",
        "#             title.text = query\n",
        "#             subtitle.text = \"Comprehensive Technical Guide\"\n",
        "\n",
        "#         # Parse sections\n",
        "#         sections = []\n",
        "#         current_section = None\n",
        "#         current_points = []\n",
        "\n",
        "#         # Split content into lines\n",
        "#         lines = content.split('\\n')\n",
        "\n",
        "#         # Track code block state\n",
        "#         in_code_block = False\n",
        "#         current_code_block = []\n",
        "\n",
        "#         # Iterate through lines to extract sections\n",
        "#         for line in lines:\n",
        "#             # Check for code block start/end\n",
        "#             if line.startswith('```'):\n",
        "#                 if in_code_block:\n",
        "#                     # End of code block\n",
        "#                     current_points.append('\\n'.join(current_code_block))\n",
        "#                     current_code_block = []\n",
        "#                     in_code_block = False\n",
        "#                 else:\n",
        "#                     # Start of code block\n",
        "#                     in_code_block = True\n",
        "#                     current_code_block.append(line)\n",
        "#                 continue\n",
        "\n",
        "#             # If in code block, accumulate lines\n",
        "#             if in_code_block:\n",
        "#                 current_code_block.append(line)\n",
        "#                 continue\n",
        "\n",
        "#             # Check for section headers\n",
        "#             h3_match = re.match(r'^### (.+)$', line)\n",
        "#             h2_match = re.match(r'^## (.+)$', line)\n",
        "\n",
        "#             # Check for bullet points or numbered list\n",
        "#             is_content_line = (\n",
        "#                 line.startswith('- ') or\n",
        "#                 line.startswith('*') or\n",
        "#                 line.startswith('1.')\n",
        "#             )\n",
        "\n",
        "#             if is_content_line:\n",
        "#                 if current_section:\n",
        "#                     current_points.append(line)\n",
        "\n",
        "#             # Detect new sections\n",
        "#             elif h3_match:\n",
        "#                 # Save previous section if exists\n",
        "#                 if current_section and current_points:\n",
        "#                     sections.append((current_section, current_points))\n",
        "\n",
        "#                 # Start new section\n",
        "#                 current_section = h3_match.group(1)\n",
        "#                 current_points = []\n",
        "\n",
        "#             elif h2_match:\n",
        "#                 # Save previous section if exists\n",
        "#                 if current_section and current_points:\n",
        "#                     sections.append((current_section, current_points))\n",
        "\n",
        "#                 # Start new section with H2 header\n",
        "#                 current_section = h2_match.group(1)\n",
        "#                 current_points = []\n",
        "\n",
        "#         # Add last section\n",
        "#         if current_section and current_points:\n",
        "#             sections.append((current_section, current_points))\n",
        "\n",
        "#         # Create slides for each section\n",
        "#         for slide_num, (section, points) in enumerate(sections, start=1):\n",
        "#             # Use content slide layout or create a new slide\n",
        "#             content_slide_layout = prs.slide_layouts[1] if len(prs.slide_layouts) > 1 else prs.slide_layouts[0]\n",
        "#             slide = prs.slides.add_slide(content_slide_layout)\n",
        "\n",
        "#             # Set slide title\n",
        "#             title_shape = slide.shapes.title\n",
        "#             title_shape.text = section\n",
        "#             title_shape.text_frame.paragraphs[0].font.size = Pt(24)\n",
        "\n",
        "#             # Add content\n",
        "#             content_shape = slide.shapes.placeholders[1]\n",
        "#             text_frame = content_shape.text_frame\n",
        "#             text_frame.clear()\n",
        "\n",
        "#             # Process points\n",
        "#             process_content_with_equations(text_frame, content_shape, slide, points, prs)\n",
        "\n",
        "#             # Add slide number\n",
        "#             left = prs.slide_width - Inches(1.5)\n",
        "#             top = prs.slide_height - Inches(0.75)\n",
        "#             textbox = slide.shapes.add_textbox(left, top, Inches(1), Inches(0.5))\n",
        "#             p = textbox.text_frame.add_paragraph()\n",
        "#             p.text = f\"Page {slide_num + 1} of {len(sections) + 1}\"\n",
        "#             p.font.size = Pt(10)\n",
        "#             p.alignment = PP_ALIGN.RIGHT\n",
        "\n",
        "#         # Generate filename\n",
        "#         filename = re.sub(r'[<>:\"/\\|?*]', '', query)\n",
        "#         filename = re.sub(r'\\s+', '_', filename).lower() + '.pptx'\n",
        "#         prs.save(filename)\n",
        "#         return filename\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error in presentation creation: {e}\")\n",
        "#         traceback.print_exc()\n",
        "#         return str(e)\n",
        "\n",
        "# def main():\n",
        "\n",
        "\n",
        "#     # Optional: Specify a template path\n",
        "#     template_path = '/content/Crop.pptx'  # Replace with path to your .pptx template if desired\n",
        "\n",
        "#     filename = create_presentation(clean_answer, query, template_path)\n",
        "#     print(f\"Presentation saved as: {filename}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "j66u7AjDKH_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylatex pygments"
      ],
      "metadata": {
        "id": "Yj6Y06YEPh6Q",
        "outputId": "8daf0f8f-5500-423b-de60-345d47a2f7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pylatex in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.11/dist-packages (from pylatex) (4.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################  .tex code ######################################\n",
        "import os\n",
        "from pylatex import Document, Section, Subsection, Command\n",
        "from pylatex.utils import NoEscape\n",
        "\n",
        "def create_document(clean_answer, query):\n",
        "    # Document setup\n",
        "    geometry_options = {\"margin\": \"1in\"}\n",
        "    doc = Document(geometry_options=geometry_options)\n",
        "\n",
        "    # Add title\n",
        "    doc.preamble.append(Command('title', query))\n",
        "    doc.preamble.append(Command('author', ''))\n",
        "    doc.preamble.append(Command('date', ''))\n",
        "    doc.append(NoEscape(r'\\maketitle'))\n",
        "\n",
        "    # Required packages\n",
        "    doc.packages.append(NoEscape(r'\\usepackage{amsmath}'))\n",
        "    doc.packages.append(NoEscape(r'\\usepackage{listings}'))\n",
        "    doc.packages.append(NoEscape(r'\\usepackage{xcolor}'))\n",
        "\n",
        "    # Code style setup\n",
        "    doc.append(NoEscape(r'''\n",
        "        \\lstset{\n",
        "            language=Python,\n",
        "            basicstyle=\\ttfamily\\footnotesize,\n",
        "            backgroundcolor=\\color{gray!10},\n",
        "            frame=single,\n",
        "            breaklines=true,\n",
        "            numbers=left,\n",
        "            numberstyle=\\tiny,\n",
        "            numbersep=5pt\n",
        "        }\n",
        "    '''))\n",
        "\n",
        "    # Process content\n",
        "    sections = []\n",
        "    current_section = None\n",
        "    current_points = []\n",
        "\n",
        "    for line in clean_answer.split('\\n'):\n",
        "        line = line.strip()\n",
        "        section_match = re.match(r'^(\\d+\\. [^•]+)$', line)\n",
        "        if section_match:\n",
        "            if current_section and current_points:\n",
        "                sections.append((current_section, current_points))\n",
        "            current_section = section_match.group(1)\n",
        "            current_points = []\n",
        "        elif line.startswith('•'):\n",
        "            current_points.append(line)\n",
        "\n",
        "    if current_section and current_points:\n",
        "        sections.append((current_section, current_points))\n",
        "\n",
        "    # Add sections and content\n",
        "    for section_title, points in sections:\n",
        "        with doc.create(Section(section_title.lstrip('0123456789. '))):\n",
        "            for point in points:\n",
        "                content = point.lstrip('•■').strip()\n",
        "                parts = []\n",
        "                current_text = \"\"\n",
        "\n",
        "                # Split text and equations\n",
        "                for part in re.split(r'(\\$[^$]+\\$)', content):\n",
        "                    if part.startswith('$') and part.endswith('$'):\n",
        "                        if current_text.strip():\n",
        "                            parts.append(('text', current_text.strip()))\n",
        "                        parts.append(('equation', part))\n",
        "                        current_text = \"\"\n",
        "                    else:\n",
        "                        current_text += part\n",
        "                if current_text.strip():\n",
        "                    parts.append(('text', current_text.strip()))\n",
        "\n",
        "                # Add content\n",
        "                for part_type, content in parts:\n",
        "                    if part_type == 'text':\n",
        "                        doc.append(content + ' ')\n",
        "                    else:\n",
        "                        doc.append(NoEscape(content + ' '))\n",
        "                doc.append(NoEscape(r'\\\\[0.5em]'))  # Add spacing between points\n",
        "\n",
        "    # Generate PDF\n",
        "    filename = re.sub(r'[<>:\"/\\\\|?*]', '', query)\n",
        "    filename = re.sub(r'\\s+', '_', filename).lower()\n",
        "    doc.generate_pdf(filename, clean_tex=False)\n",
        "    return filename + '.pdf'\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    filename = create_document(clean_answer, query)\n",
        "    print(f\"PDF saved as: {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UmGpSQDePJXC",
        "outputId": "616bf024-0fd6-47be-f787-311bd9a7fd6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF saved as: pointers_in_c++.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx Pillow PyMuPDF matplotlib"
      ],
      "metadata": {
        "id": "cVbVm9QqdRxB",
        "outputId": "1bcc75c9-03ed-4ea7-9a4a-290eb316253e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import re\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import re\n",
        "\n",
        "def render_equation(equation):\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    plt.axis('off')\n",
        "    equation = equation.strip()\n",
        "    plt.text(0.5, 0.5, f'${equation}$', fontsize=36, ha='center', va='center')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return buf\n",
        "\n",
        "def process_content(slide, point, content_shape):\n",
        "    text_frame = content_shape.text_frame\n",
        "    current_x = content_shape.left\n",
        "    current_y = content_shape.top\n",
        "    line_height = Inches(0.4)\n",
        "\n",
        "    parts = re.split(r'(\\$[^$]+\\$|\\`\\`\\`[\\s\\S]+?\\`\\`\\`)', point.lstrip('•■').strip())\n",
        "\n",
        "    for part in parts:\n",
        "        if part.startswith('$') and part.endswith('$'):\n",
        "            eq_buf = render_equation(part.strip('$'))\n",
        "            slide.shapes.add_picture(\n",
        "                eq_buf,\n",
        "                current_x,\n",
        "                current_y,\n",
        "                width=Inches(2.0)\n",
        "            )\n",
        "            current_x += Inches(2.2)\n",
        "        elif part.startswith('```') and part.endswith('```'):\n",
        "            p = text_frame.add_paragraph()\n",
        "            r = p.add_run()\n",
        "            r.text = part.strip('```')\n",
        "            r.font.name = 'Courier New'\n",
        "            r.font.size = Pt(12)\n",
        "            current_y += line_height\n",
        "            current_x = content_shape.left\n",
        "        else:\n",
        "            if part.strip():\n",
        "                p = text_frame.add_paragraph()\n",
        "                r = p.add_run()\n",
        "                r.text = part.strip()\n",
        "                current_y += line_height\n",
        "                current_x = content_shape.left\n",
        "\n",
        "    return current_y + line_height\n",
        "\n",
        "def create_presentation(clean_answer, query, template_path=None):\n",
        "    prs = Presentation(template_path) if template_path else Presentation()\n",
        "\n",
        "    title_slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
        "    title_slide.shapes.title.text = query\n",
        "\n",
        "    sections = []\n",
        "    current_section = None\n",
        "    current_points = []\n",
        "\n",
        "    for line in clean_answer.split('\\n'):\n",
        "        line = line.strip()\n",
        "        section_match = re.match(r'^(\\d+\\. [^•]+)$', line)\n",
        "        if section_match:\n",
        "            if current_section and current_points:\n",
        "                sections.append((current_section, current_points))\n",
        "            current_section = section_match.group(1)\n",
        "            current_points = []\n",
        "        elif line.startswith('•'):\n",
        "            current_points.append(line)\n",
        "\n",
        "    if current_section and current_points:\n",
        "        sections.append((current_section, current_points))\n",
        "\n",
        "    for section_title, points in sections:\n",
        "        slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
        "        slide.shapes.title.text = section_title.strip()\n",
        "        content_shape = slide.shapes.placeholders[1]\n",
        "        vertical_position = content_shape.top\n",
        "\n",
        "        for point in points:\n",
        "            vertical_position = process_content(slide, point, content_shape)\n",
        "\n",
        "    filename = f\"{query.lower().replace(' ', '_')}.pptx\"\n",
        "    prs.save(filename)\n",
        "    return filename\n",
        "\n",
        "def main():\n",
        "    template_path = \"/content/Crop.pptx\"  # Optional\n",
        "\n",
        "\n",
        "    filename = create_presentation(clean_answer, query, template_path)\n",
        "    print(f\"Presentation saved as: {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bgIxH6cAdPmT",
        "outputId": "10905602-9015-4256-ac61-ffcc906729e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation saved as: linear_regression.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create prompt for image key words"
      ],
      "metadata": {
        "id": "PRu5t98eRYsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query Hugging Face Inference API\n",
        "def query_huggingface_api(prompt, max_length=25000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.5,\n",
        "            \"top_p\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "     }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        #print(response.json())\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "def extract_answer(api_response):\n",
        "    \"\"\"\n",
        "    Extracts and validates keywords from the Hugging Face API response.\n",
        "\n",
        "    Args:\n",
        "        api_response (dict): The JSON response from the Hugging Face API\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned comma-separated list of valid technical visualization keywords\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Required suffixes for validation\n",
        "        valid_suffixes = [\n",
        "            '-visualization',\n",
        "            '-diagram',\n",
        "            '-illustration',\n",
        "            '-example',\n",
        "            '-steps'\n",
        "        ]\n",
        "\n",
        "        # Extract the generated text from the API response\n",
        "        if isinstance(api_response, list):\n",
        "            response_text = api_response[0].get('generated_text', '')\n",
        "        elif isinstance(api_response, dict):\n",
        "            response_text = api_response.get('generated_text', '')\n",
        "        else:\n",
        "            response_text = str(api_response)\n",
        "\n",
        "        # Find the keywords section\n",
        "        marker = \"KEYWORDS (comma-separated):\"\n",
        "        if marker not in response_text:\n",
        "            print(\"No keyword marker found in response\")\n",
        "            return \"\"\n",
        "\n",
        "        # Get everything after the marker\n",
        "        keywords_section = response_text.split(marker)[-1].strip()\n",
        "\n",
        "        # Get the first non-empty line\n",
        "        keyword_lines = [line.strip() for line in keywords_section.split('\\n') if line.strip()]\n",
        "        if not keyword_lines:\n",
        "            print(\"No keyword lines found\")\n",
        "            return \"\"\n",
        "\n",
        "        # Get the comma-separated keywords\n",
        "        keywords = keyword_lines[0]\n",
        "\n",
        "        # Clean up and validate the keywords\n",
        "        cleaned_keywords = []\n",
        "        for keyword in keywords.split(','):\n",
        "            keyword = keyword.strip()\n",
        "\n",
        "            # Validation checks\n",
        "            if (keyword and                # Not empty\n",
        "                '-' in keyword and         # Contains hyphens\n",
        "                any(keyword.endswith(suffix) for suffix in valid_suffixes) and  # Has valid suffix\n",
        "                keyword.count('-') >= 2):  # Has at least 2 hyphens (component-type-suffix)\n",
        "\n",
        "                cleaned_keywords.append(keyword)\n",
        "\n",
        "        # Return 5-7 keywords as specified in the prompt\n",
        "        cleaned_keywords = cleaned_keywords[:7] if len(cleaned_keywords) > 7 else cleaned_keywords\n",
        "\n",
        "        if len(cleaned_keywords) < 5:\n",
        "            print(f\"Not enough valid keywords found: {len(cleaned_keywords)}\")\n",
        "            return \"\"\n",
        "\n",
        "        return ', '.join(cleaned_keywords)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {str(e)}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "xVPwSwbea--T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_keyword_prompt(content):\n",
        "    \"\"\"\n",
        "    Creates a prompt for generating image search keywords that works with any technical topic.\n",
        "\n",
        "    Args:\n",
        "        content (str): Technical content to analyze for image-searchable keywords\n",
        "\n",
        "    Returns:\n",
        "        str: Generic prompt that works with any technical content\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are a technical visualization expert. Extract specific image search keywords from the following content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Your task: Generate 5-7 image search keywords that would help find educational diagrams and technical visualizations related to the main concepts in this content.\n",
        "\n",
        "Focus on identifying keywords for:\n",
        "1. System/concept architecture diagrams\n",
        "2. Process flows and sequences\n",
        "3. Component interactions\n",
        "4. Implementation details\n",
        "5. Working mechanisms\n",
        "6. Step-by-step procedures\n",
        "7. Technical examples\n",
        "\n",
        "Keyword Generation Rules:\n",
        "1. Each keyword MUST use hyphens between words\n",
        "2. Each keyword MUST end with exactly ONE of these suffixes:\n",
        "   - -visualization\n",
        "   - -diagram\n",
        "   - -illustration\n",
        "   - -example\n",
        "   - -steps\n",
        "\n",
        "Key Requirements:\n",
        "- Use specific technical terms from the content\n",
        "- Include major concepts and processes\n",
        "- Avoid generic/non-specific terms\n",
        "- Keywords must directly relate to main topics\n",
        "- Each keyword should help find relevant technical diagrams\n",
        "\n",
        "Output Format:\n",
        "ONLY provide a comma-separated list of keywords. Example:\n",
        "concept-architecture-diagram, process-flow-visualization, component-interaction-illustration\n",
        "\n",
        "KEYWORDS (comma-separated):\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Modify the main function to work with your existing setup\n",
        "def main():\n",
        "    # Use your existing content and context\n",
        "    prompt = create_image_keyword_prompt(result)\n",
        "    #print(prompt)\n",
        "\n",
        "    # Use your existing API call function\n",
        "    api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "\n",
        "    # Extract the keywords\n",
        "    image_keywords = extract_answer(api_response)\n",
        "\n",
        "    print(\"Generated Image Search Keywords:\")\n",
        "    print(image_keywords)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZdPWh_hULMMl",
        "outputId": "cbfb70a2-c0dc-4969-fdf4-635c0ce6541c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Image Search Keywords:\n",
            "long-short-term-memory-lstm-visualization, memory-cell-component-illustration, input-gate-process-steps, forget-gate-functionality-diagram, output-gate-role-illustration, lstm-working-mechanism-diagram, language-modeling-example-visualization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "import random\n",
        "import re\n",
        "\n",
        "def create_ppt_from_text(raw_text, templates, main_topic):\n",
        "    # Select a random template for the presentation\n",
        "    selected_template = random.choice(templates)\n",
        "    prs = Presentation(selected_template)\n",
        "\n",
        "    # Add main title slide (Slide 1)\n",
        "    if prs.slides:\n",
        "        slide = prs.slides[0]\n",
        "        title = slide.shapes.title\n",
        "        title.text = main_topic\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(40)\n",
        "\n",
        "    # Split raw text into sections based on headings\n",
        "    sections = split_into_sections(raw_text)\n",
        "\n",
        "    # Process each section and add slides dynamically\n",
        "    for heading, content in sections:\n",
        "        # Slide Layout for Title and Content\n",
        "        slide_layout = prs.slide_layouts[1]\n",
        "        slide = prs.slides.add_slide(slide_layout)\n",
        "\n",
        "        # Add heading as the title of the slide\n",
        "        title_shape = slide.shapes.title\n",
        "        title_shape.text = heading\n",
        "        title_shape.text_frame.paragraphs[0].font.size = Pt(36)\n",
        "\n",
        "        # Check if the content has bullet points (e.g., with dashes or other markers)\n",
        "        if is_bullet_point_section(content):\n",
        "            bullet_points = create_bullet_points_from_list(content)\n",
        "        else:\n",
        "            bullet_points = create_bullet_points(content)\n",
        "\n",
        "        # Add content as the body of the slide\n",
        "        textbox = slide.shapes.placeholders[1].text_frame\n",
        "        textbox.clear()  # Clear default text\n",
        "\n",
        "        # Add each bullet point\n",
        "        for i, point in enumerate(bullet_points, 1):\n",
        "            p = textbox.add_paragraph()\n",
        "            p.text = f\"{i}- {point.strip()}\"\n",
        "            p.font.size = Pt(10)\n",
        "\n",
        "    # Save the presentation\n",
        "    prs.save(f\"/content/{main_topic}.pptx\")\n",
        "\n",
        "\n",
        "def split_into_sections(raw_text):\n",
        "    # Regular expression to match section headings (e.g., **Concept**, **Example**)\n",
        "    section_pattern = r\"\\*\\*(.*?)\\*\\*\"\n",
        "    sections = []\n",
        "\n",
        "    # Find all sections with their titles and content\n",
        "    matches = re.split(r\"(\\*\\*.*?\\*\\*)\", raw_text)\n",
        "    current_heading = None\n",
        "    current_content = \"\"\n",
        "\n",
        "    for match in matches:\n",
        "        if match.startswith(\"**\") and match.endswith(\"**\"):\n",
        "            if current_heading is not None:\n",
        "                sections.append((current_heading, current_content.strip()))\n",
        "            current_heading = match[2:-2]  # Remove ** from the heading\n",
        "            current_content = \"\"\n",
        "        else:\n",
        "            current_content += match\n",
        "\n",
        "    if current_heading is not None:\n",
        "        sections.append((current_heading, current_content.strip()))  # Add the last section\n",
        "\n",
        "    return sections\n",
        "\n",
        "\n",
        "def is_bullet_point_section(content):\n",
        "    # Check if the section contains bullet point markers like dashes or similar\n",
        "    return bool(re.search(r\"^\\s*-|\\s*\\d+\\.\", content, re.MULTILINE))\n",
        "\n",
        "\n",
        "def create_bullet_points_from_list(content):\n",
        "    # Handle sections with existing bullet points (e.g., Advantages, Disadvantages)\n",
        "    lines = content.split('\\n')\n",
        "    bullet_points = [line.strip('-').strip() for line in lines if line.strip()]\n",
        "    return bullet_points\n",
        "\n",
        "\n",
        "def create_bullet_points(content):\n",
        "    # Split content into sentences based on full stops\n",
        "    sentences = content.split('.')\n",
        "    # Remove empty sentences and strip extra spaces\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Example usage\n",
        "main_topic = query.upper()\n",
        "\n",
        "\n",
        "\n",
        "templates = [\"/content/Crop.pptx\"]  # Provide the template path\n",
        "\n",
        "create_ppt_from_text(clean_answer, templates, main_topic)\n"
      ],
      "metadata": {
        "id": "YYQB9GIfq06C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#original\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function to scrape images from GeeksforGeeks using SerpApi\n",
        "def scrape_images(keyword, num_images=2):  # Limit to 1 image\n",
        "    api_key = \"41cf19594f02970e20e9362044f5605347e8e04ce0cf4a9614504c087d2bae2e\"  # Replace with your actual API key\n",
        "\n",
        "\n",
        "    ### 4fb26d3c8969e2792af095f4b95dc1270b0a86736c098c88ddaa72d771b76239    new 0/100\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": keyword,\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        images = []\n",
        "\n",
        "        if \"images_results\" in data:\n",
        "            for image in data[\"images_results\"]:\n",
        "                if 'geeksforgeeks' in image.get('source', '').lower():\n",
        "                    images.append(image[\"original\"])\n",
        "                    if len(images) >= num_images:  # Limit to the desired number of images\n",
        "                        break\n",
        "            return images\n",
        "\n",
        "        return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve images. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def generate_slide_title(text):\n",
        "    # Use TF-IDF vectorizer to extract important terms, focusing on unigrams\n",
        "    tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=10)\n",
        "    tfidf_matrix = tfidf.fit_transform([text])\n",
        "\n",
        "    # Use KMeans clustering to identify key terms\n",
        "    num_clusters = 1  # Only one cluster for the main topic\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(tfidf_matrix)\n",
        "\n",
        "    # Get the terms with the highest TF-IDF scores in the cluster\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "    sorted_items = sorted(zip(kmeans.cluster_centers_[0], feature_names), reverse=True)\n",
        "\n",
        "    # Select the top 3 terms as keywords and create a meaningful title\n",
        "    key_terms = [item[1] for item in sorted_items[:3]]  # Top 3 terms\n",
        "    title = \" \".join(key_terms).title()\n",
        "\n",
        "    # Return the title if it's meaningful; otherwise, fall back to a basic title\n",
        "    if len(title.split()) < 2 or title.lower() in ['summary', 'overview']:  # Avoid trivial titles\n",
        "        return 'Slide Title'\n",
        "    return title\n",
        "\n",
        "\n",
        "def generate_bullet_points(text):\n",
        "    # Split text into sentences and return as bullet points\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    bullets = [sentence for sentence in sentences if len(sentence) > 20]\n",
        "    return bullets\n",
        "\n",
        "import random\n",
        "def create_ppt_from_template(raw_text, templates, main_topic):\n",
        "    selected_template = random.choice(templates)\n",
        "    prs = Presentation(\"/content/Crop.pptx\")\n",
        "\n",
        "    # Add main topic to the first slide\n",
        "    if prs.slides:\n",
        "        slide = prs.slides[0]\n",
        "        title = slide.shapes.title\n",
        "        title.text = main_topic\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(40)\n",
        "\n",
        "    # Improved text segmentation\n",
        "    paragraphs = raw_text.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    slide_sections = []\n",
        "    current_section = []\n",
        "    for paragraph in paragraphs:\n",
        "        if len(current_section) < 3:\n",
        "            current_section.append(paragraph)\n",
        "        else:\n",
        "            slide_sections.append(' '.join(current_section))\n",
        "            current_section = [paragraph]\n",
        "\n",
        "    if current_section:\n",
        "        slide_sections.append(' '.join(current_section))\n",
        "\n",
        "    # Limit to maximum 10 slides\n",
        "    slide_sections = slide_sections[:15]\n",
        "    used_images = set()  # Track used images to prevent repetition\n",
        "\n",
        "    for section in slide_sections:\n",
        "        slide_title = generate_slide_title(section)\n",
        "        bullets = generate_bullet_points(section)\n",
        "\n",
        "        # Add text slide\n",
        "        slide_layout = prs.slide_layouts[1]\n",
        "        slide = prs.slides.add_slide(slide_layout)\n",
        "\n",
        "        title = slide.shapes.title\n",
        "        title.text = slide_title\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(36)\n",
        "\n",
        "        textbox = slide.shapes.placeholders[1].text_frame\n",
        "        textbox.clear()\n",
        "        for bullet in bullets[:3]:  # Limit to 3 bullets per slide\n",
        "            p = textbox.add_paragraph()\n",
        "            p.text = bullet\n",
        "            p.font.size = Pt(20)\n",
        "\n",
        "        # Add corresponding image slide\n",
        "        images = scrape_images(slide_title, num_images=1)\n",
        "        for img_url in images:\n",
        "            if img_url not in used_images:\n",
        "                used_images.add(img_url)\n",
        "                try:\n",
        "                    response = requests.get(img_url)\n",
        "                    response.raise_for_status()\n",
        "\n",
        "                    img = Image.open(BytesIO(response.content))\n",
        "                    image_stream = BytesIO()\n",
        "                    img.save(image_stream, format=\"PNG\")\n",
        "                    image_stream.seek(0)\n",
        "\n",
        "                    img_slide_layout = prs.slide_layouts[6]\n",
        "                    img_slide = prs.slides.add_slide(img_slide_layout)\n",
        "\n",
        "                    slide_width = prs.slide_width\n",
        "                    slide_height = prs.slide_height\n",
        "                    img_width, img_height = img.size\n",
        "\n",
        "                    aspect_ratio = min((slide_width * 0.8) / img_width, (slide_height * 0.8) / img_height)\n",
        "                    new_width = int(img_width * aspect_ratio)\n",
        "                    new_height = int(img_height * aspect_ratio)\n",
        "\n",
        "                    left = (slide_width - new_width) / 2\n",
        "                    top = (slide_height - new_height) / 2\n",
        "\n",
        "                    img_slide.shapes.add_picture(image_stream, left, top, width=new_width, height=new_height)\n",
        "                    break  # Use only one unique image per section\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading image for '{slide_title}': {e}\")\n",
        "\n",
        "    # Save the presentation\n",
        "    prs.save(f\"/content/{main_topic}.pptx\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "templates = [\n",
        "    '/content/Crop.pptx'\n",
        "]  # List of template file paths\n",
        "\n",
        "sentences = answer.split('.')\n",
        "# Join sentences starting from the third sentence (index 2)\n",
        "updated_answer = '.'.join(sentences[2:]).lstrip('.')\n",
        "print(\"Updated Answer:\", updated_answer)\n",
        "\n",
        "raw_text = updated_answer\n",
        "main_topic = query.upper()\n",
        "\n",
        "# Create a presentation using one random template\n",
        "create_ppt_from_template(raw_text, templates, main_topic)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzC9_4uPkq5y",
        "outputId": "bd7e8d9c-ae4c-4fbb-c45f-7768a6961ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Answer: \n",
            "\n",
            "K Nearest Neighbor (KNN) is an instance-based learning algorithm used for both classification and regression tasks. It's a popular method due to its simplicity and effectiveness, especially when dealing with complex or high-dimensional data.\n",
            "\n",
            "Here's how KNN works:\n",
            "\n",
            "1. **Given a new data point**, we want to predict its class or value based on the existing data points.\n",
            "\n",
            "2. **Calculate the distances** between the new data point and all other data points in our dataset. This can be done using various distance measures such as Euclidean, Manhattan, or Minkowski distances.\n",
            "\n",
            "3. **Find the K closest data points**. The number of nearest neighbors, K, is a parameter that needs to be set beforehand. A common choice is K=5.\n",
            "\n",
            "4. **Predict the class or value** of the new data point by taking a vote from the K closest neighbors. For classification problems, the most common class among the K neighbors is chosen. For regression problems, the average value of the K nearest neighbors is taken.\n",
            "\n",
            "Let's take an example to illustrate this:\n",
            "\n",
            "Suppose we have a dataset containing colors of iris flowers (setosa, versicolor, virginica). We want to classify a new flower whose petal length is 5 cm and width is 3 cm.\n",
            "\n",
            "1. Calculate the distances between the new flower and all other flowers in the dataset. Let's say the 5 closest flowers belong to the following classes:\n",
            "   - Setosa: 3, 2, 1 times\n",
            "   - Versicolor: 1 time\n",
            "   - Virginica: 1 time\n",
            "\n",
            "2. Since the majority of the 5 closest flowers are setosa, we predict the new flower is also setosa.\n",
            "\n",
            "It's important to note that the performance of KNN depends on the value of K and the distance measure used. Cross-validation techniques can be employed to find the best K and distance measure for a particular dataset. Also, KNN is sensitive to noise in the data, so it's essential to clean and preprocess the data before applying KNN.\n",
            "\n",
            "In summary, KNN is a powerful and flexible machine learning algorithm that relies on the idea of finding similar instances in the training data to make predictions about new, unseen instances. Its simplicity and effectiveness make it a popular choice for many real-world applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Function to generate a more meaningful title dynamically\n",
        "def generate_slide_title(text, num_topics=1, num_keywords=3):\n",
        "    # Clean the text (remove unnecessary spaces, symbols, etc.)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Vectorize the text using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Apply Latent Dirichlet Allocation (LDA) to extract topics\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "    lda.fit(tfidf_matrix)\n",
        "\n",
        "    # Get the most important words for each topic (top N)\n",
        "    topic_keywords = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_keywords_idx = topic.argsort()[:-num_keywords - 1:-1]\n",
        "        top_keywords = feature_names[top_keywords_idx]\n",
        "        topic_keywords.append(\" \".join(top_keywords))\n",
        "\n",
        "    # Create a title using the most important topic words\n",
        "    title = topic_keywords[0]  # Use the first topic as the title\n",
        "\n",
        "    return title.title()\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"K Nearest Neighbor (KNN) is an instance-based learning algorithm used for both classification and regression tasks. It's a popular method due to its simplicity and effectiveness, especially when dealing with complex or high-dimensional data. In KNN, we calculate the distances between the new data point and all other points in the dataset, find the closest K points, and predict the class or value of the new point.\"\"\"\n",
        "\n",
        "# Generate slide title dynamically\n",
        "slide_title = generate_slide_title(text)\n",
        "print(slide_title)\n",
        "\n"
      ],
      "metadata": {
        "id": "E1fKlxltWnCE",
        "outputId": "140c39dc-8b6d-46d4-efef-6abc69c2d27d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Points Knn Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = answer.split('.')\n",
        "# Join sentences starting from the third sentence (index 2)\n",
        "updated_answer = '.'.join(sentences[2:]).lstrip('.')\n"
      ],
      "metadata": {
        "id": "JMnxn2WtLquv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_huggingface_api(prompt, max_length=30000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(response.json())\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "\n",
        "prompt1 = f\"\"\"\n",
        "You are a question-generation expert. Based on the following content, generate at least 30 questions to assess understanding. Include:\n",
        "- 10 Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\n",
        "- 10 Fill-in-the-blank questions: Provide a sentence with a blank to be filled, the correct answer, and difficulty level.\n",
        "- 15 Short-answer questions: Provide concise questions with answers (1-2 sentences) and indicate the difficulty level.\n",
        "\n",
        "Content:\n",
        "{updated_answer}\n",
        "\n",
        "Ensure the questions vary in difficulty (easy, medium, hard) and cover the entire content comprehensively.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    api_response = query_huggingface_api(prompt1, max_length=25000)\n",
        "    answer = extract_answer(api_response)\n",
        "    print(\"Answer:\", answer)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADQg91vwktFm",
        "outputId": "2e6ddc10-e019-460e-ecf7-a84aca6d34a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"\\nYou are a question-generation expert. Based on the following content, generate at least 30 questions to assess understanding. Include:\\n- 10 Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\\n- 10 Fill-in-the-blank questions: Provide a sentence with a blank to be filled, the correct answer, and difficulty level.\\n- 15 Short-answer questions: Provide concise questions with answers (1-2 sentences) and indicate the difficulty level.\\n\\nContent:\\n\\n\\nKNN (k-Nearest Neighbors) is a machine learning algorithm used for classification and regression tasks. It's a simple yet effective technique for making predictions based on the proximity of data points in a dataset.\\n\\nLet's consider an example to understand KNN better. Suppose we have a dataset of house prices in a city, and each data point includes attributes such as square footage, number of bedrooms, location, etc. Now, we want to predict the price of a new house that doesn't have a labeled price. Using KNN, we can make a prediction by finding the k-closest houses from our dataset to the new house and then taking the average price of those k houses as our prediction.\\n\\nThe number of neighbors (k) used in KNN can be chosen based on the nature of the data and the desired accuracy. A smaller value of k will make the model more sensitive to outliers, while a larger value will make it less so. The choice of k is a trade-off between simplicity and accuracy.\\n\\nTo calculate the distance between two data points, various metrics can be used, such as Euclidean distance, Manhattan distance, or even custom distances tailored to the specific problem at hand.\\n\\nOne important aspect of KNN is that it doesn't require any assumptions about the underlying distribution of the data, making it a non-parametric method. This flexibility allows it to handle complex relationships between variables and perform well on high-dimensional datasets. However, KNN can become computationally expensive when dealing with large datasets due to the need to calculate distances between all pairs of data points.\\n\\nIn summary, KNN is a popular machine learning algorithm that makes predictions by finding the k-closest data points to a new, unlabeled data point and averaging their labels. Its simplicity and flexibility make it a valuable tool in many applications, though its computational cost can be a concern when dealing with large datasets.\\n\\nEnsure the questions vary in difficulty (easy, medium, hard) and cover the entire content comprehensively.\\n\\n---\\n\\n**Multiple-Choice Questions:**\\n\\n1. What type of machine learning algorithm does KNN belong to? (A) Supervised learning (B) Unsupervised learning (C) Semi-supervised learning (D) Reinforcement learning (Correct Answer: A - Difficulty Level: Easy)\\n\\n2. Which of the following is not a use case for KNN? (A) Classifying emails as spam or not spam (B) Predicting house prices based on attributes like square footage, number of bedrooms, and location (C) Training a model to play chess (D) Clustering customers based on their purchasing behavior (Correct Answer: C - Difficulty Level: Easy)\\n\\n3. How does KNN make predictions? (A) By fitting a linear equation to the data (B) By using decision trees (C) By finding the k-closest data points to a new, unlabeled data point and averaging their labels (D) By using neural networks (Correct Answer: C - Difficulty Level: Medium)\\n\\n4. When choosing the number of neighbors (k) in KNN, what happens if you select a smaller value? (A) The model becomes more resistant to outliers (B) The model becomes sensitive to outliers (Correct Answer: B - Difficulty Level: Medium)\\n\\n5. Which metric is NOT typically used to calculate the distance between two data points in KNN? (A) Euclidean distance (B) Manhattan distance (C) Custom distance (D) Chebyshev distance (Correct Answer: D - Difficulty Level: Hard)\\n\\n6. KNN is considered a ___________ method because it doesn't make any assumptions about the underlying distribution of the data. (A) Parametric (B) Non-parametric (C) Linear (D) Logarithmic (Correct Answer: B - Difficulty Level: Medium)\\n\\n7. One drawback of using KNN is its ____________ cost when dealing with large datasets. (A) Calculation (B) Memory (C) Computational (D) Time (Correct Answer: C - Difficulty Level: Easy)\\n\\n8. In the context of KNN, which step involves calculating the distance between all pairs of data points? (A) Finding the k-closest data points (B) Deciding the value of k (C) Selecting the appropriate distance metric (D) Calculating the average label of the k-closest data points (Correct Answer: A - Difficulty Level: Easy)\\n\\n9. What is the purpose of the k parameter in KNN? (A) To determine the number of data points to compare with the new, unlabeled data point (B) To decide the level of sensitivity to outliers (C) To choose the appropriate distance metric (D) To select the type of learning algorithm (Correct Answer: A - Difficulty Level: Medium)\\n\\n10. KNN can be used for both classification and _______ tasks. (A) Linear regression (B) Clustering (C) Time series forecasting (D) Sentiment analysis (Correct Answer: Both A and B - Difficulty Level: Hard)\\n\\n---\\n\\n**Fill-in-the-blank Questions:**\\n\\n1. KNN is a machine learning algorithm used for _______ and _______ tasks. (Answer: Classification and Regression - Difficulty Level: Easy)\\n\\n2. In KNN, we find the k-closest data points to the new, unlabeled data point and take the average of their _______ as our prediction. (Answer: Labels - Difficulty Level: Easy)\\n\\n3. When choosing the number of neighbors (k) in KNN, a smaller value makes the model more sensitive to _______. (Answer: Outliers - Difficulty Level: Medium)\\n\\n4. Euclidean distance, Manhattan distance, and _______ are examples of distance metrics used in KNN. (Answer: Custom distance - Difficulty Level: Medium)\\n\\n5. KNN is a _______ method because it doesn't make any assumptions about the underlying distribution of the data. (Answer: Non-parametric - Difficulty Level: Medium)\\n\\n6. One drawback of using KNN is its _______ cost when dealing with large datasets. (Answer: Computational - Difficulty Level: Easy)\\n\\n7. In KNN, the goal is to find the k-closest data points by calculating the distance between all pairs of data points. (Answer: True - Difficulty Level: Easy)\\n\\n8. The number of neighbors (k) used in KNN can be chosen based on the nature of the data and the desired ________. (Answer: Accuracy - Difficulty Level: Medium)\\n\\n9. KNN can be used for problems where the relationship between variables is complex and non-linear. (Answer: True - Difficulty Level: Medium)\\n\\n10. KNN doesn't require any assumptions about the shape of the data distribution. (Answer: True - Difficulty Level: Easy)\\n\\n---\\n\\n**Short-answer Questions:**\\n\\n1. Explain the concept of K-Nearest Neighbors (KNN) in your own words. (Difficulty Level: Medium)\\n\\n2. What are some common distance metrics used in KNN? (Difficulty Level: Easy)\\n\\n3. Why is KNN considered a non-parametric method? (Difficulty Level: Medium)\\n\\n4. Discuss the role of the k parameter in KNN. (Difficulty Level: Medium)\\n\\n5. How does the choice of k affect the performance of KNN? (Difficulty Level: Medium)\\n\\n6. What are some potential limitations of using KNN? (Difficulty Level: Medium)\\n\\n7. How does KNN handle high-dimensional datasets compared to other algorithms? (Difficulty Level: Medium)\\n\\n8. Can KNN be used for clustering purposes? If yes, explain how. (Difficulty Level: Medium)\\n\\n9. Describe the process of calculating the distance between two data points in KNN. (Difficulty Level: Medium)\\n\\n10. What is the main idea behind using KNN for classification and regression tasks? (Difficulty Level: Easy)\\n\\n11. Why might KNN become computationally expensive when dealing with large datasets? (Difficulty Level: Easy)\\n\\n12. How can custom distances be used in KNN to better suit specific problems? (Difficulty Level: Medium)\\n\\n13. Explain the difference between Euclidean distance and Manhattan distance. (Difficulty Level: Medium)\\n\\n14. Under what circumstances would a smaller value of k be preferred over a larger value in KNN? (Difficulty Level: Medium)\\n\\n15. What factors should be considered when deciding whether to use KNN for a particular problem? (Difficulty Level: Hard)\"}]\n",
            "Answer: True - Difficulty Level: Easy)\n",
            "\n",
            "---\n",
            "\n",
            "**Short-answer Questions:**\n",
            "\n",
            "1. Explain the concept of K-Nearest Neighbors (KNN) in your own words. (Difficulty Level: Medium)\n",
            "\n",
            "2. What are some common distance metrics used in KNN? (Difficulty Level: Easy)\n",
            "\n",
            "3. Why is KNN considered a non-parametric method? (Difficulty Level: Medium)\n",
            "\n",
            "4. Discuss the role of the k parameter in KNN. (Difficulty Level: Medium)\n",
            "\n",
            "5. How does the choice of k affect the performance of KNN? (Difficulty Level: Medium)\n",
            "\n",
            "6. What are some potential limitations of using KNN? (Difficulty Level: Medium)\n",
            "\n",
            "7. How does KNN handle high-dimensional datasets compared to other algorithms? (Difficulty Level: Medium)\n",
            "\n",
            "8. Can KNN be used for clustering purposes? If yes, explain how. (Difficulty Level: Medium)\n",
            "\n",
            "9. Describe the process of calculating the distance between two data points in KNN. (Difficulty Level: Medium)\n",
            "\n",
            "10. What is the main idea behind using KNN for classification and regression tasks? (Difficulty Level: Easy)\n",
            "\n",
            "11. Why might KNN become computationally expensive when dealing with large datasets? (Difficulty Level: Easy)\n",
            "\n",
            "12. How can custom distances be used in KNN to better suit specific problems? (Difficulty Level: Medium)\n",
            "\n",
            "13. Explain the difference between Euclidean distance and Manhattan distance. (Difficulty Level: Medium)\n",
            "\n",
            "14. Under what circumstances would a smaller value of k be preferred over a larger value in KNN? (Difficulty Level: Medium)\n",
            "\n",
            "15. What factors should be considered when deciding whether to use KNN for a particular problem? (Difficulty Level: Hard)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = api_response[0][\"generated_text\"]\n",
        "\n",
        "answer = answer.split('\\n')\n",
        "\n",
        "for line in answer:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TMgknmskx0n",
        "outputId": "678e4d26-413a-4db1-e6a5-ae44a86470a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a question-generation expert. Based on the following content, generate at least 30 questions to assess understanding. Include:\n",
            "- 10 Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\n",
            "- 10 Fill-in-the-blank questions: Provide a sentence with a blank to be filled, the correct answer, and difficulty level.\n",
            "- 15 Short-answer questions: Provide concise questions with answers (1-2 sentences) and indicate the difficulty level.\n",
            "\n",
            "Content:\n",
            "\n",
            "\n",
            "K-Nearest Neighbors (KNN) is a simple and effective machine learning algorithm used for classification and regression tasks. It works by finding the k closest training examples in the feature space of an input sample and using them to predict the class or value of that sample.\n",
            "\n",
            "Let's break it down:\n",
            "1. **Input**: A new, unlabeled data point x with features X1, X2, ..., Xn.\n",
            "2. **Training set**: A collection of labeled data points D = {(x1, y1), (x2, y2), ..., (xm, ym)}, where each data point xi has m features and a corresponding label yi.\n",
            "3. **Distance metric**: A function that measures the similarity between two data points, such as Euclidean distance or Manhattan distance.\n",
            "4. **k**: A positive integer that determines the number of nearest neighbors to consider when making a prediction.\n",
            "\n",
            "Here's how KNN works:\n",
            "\n",
            "1. Calculate the distance between the new data point x and every training example in the dataset.\n",
            "2. Sort the distances in ascending order, and select the k smallest ones.\n",
            "3. For classification problems, count the number of votes for each class among the k nearest neighbors. Predict the class of the new data point as the one with the most votes. For regression problems, take the average of the labels of the k nearest neighbors.\n",
            "\n",
            "Example: Suppose we have a dataset of iris flowers with four features (sepal length, sepal width, petal length, petal width) and three classes (Setosa, Versicolor, Virginica). We want to classify a new flower with the following features: sepal length = 5.0, sepal width = 3.0, petal length = 1.5, petal width = 0.2.\n",
            "\n",
            "To use KNN, we first calculate the distance between this new flower and all the flowers in our dataset. Let's say we choose k=3. After sorting the distances, we find the three nearest neighbors:\n",
            "\n",
            "- Setosa (distance = 0.2)\n",
            "- Setosa (distance = 0.3)\n",
            "- Versicolor (distance = 0.6)\n",
            "\n",
            "Since all three neighbors belong to the Setosa class, we predict that the new flower is also a Setosa.\n",
            "\n",
            "Advantages of KNN:\n",
            "- Easy to understand and implement\n",
            "- Flexible: can be used for both classification and regression tasks\n",
            "- Works well for high-dimensional datasets\n",
            "\n",
            "Disadvantages of KNN:\n",
            "- Computationally expensive for large datasets due to the need to calculate distances between every pair of data points\n",
            "- Sensitive to the choice of k: choosing too small a value may lead to overfitting, while choosing too large a value may reduce accuracy\n",
            "- Requires careful selection of distance metrics and normalization of features to ensure accurate results\n",
            "\n",
            "Ensure the questions vary in difficulty (easy, medium, hard) and cover the entire content comprehensively.\n",
            "\n",
            "---\n",
            "\n",
            "**Multiple-choice questions:**\n",
            "\n",
            "1. [Easy] Which of the following algorithms uses k nearest neighbors to make predictions?\n",
            "   - Naive Bayes Classifier\n",
            "   - Decision Trees\n",
            "   - Support Vector Machines\n",
            "   - K-Nearest Neighbors\n",
            "   (Correct Answer: K-Nearest Neighbors)\n",
            "\n",
            "2. [Medium] What does K-Nearest Neighbors do in a classification problem?\n",
            "   - Predicts the value of the new data point as the average of the labels of the k nearest neighbors\n",
            "   - Predicts the class of the new data point based on the majority vote among the k nearest neighbors\n",
            "   - Uses the Euclidean distance to measure similarity between data points\n",
            "   - Applies principal component analysis (PCA) to reduce the dimensionality of the data before making predictions\n",
            "   (Correct Answer: Predicts the class of the new data point based on the majority vote among the k nearest neighbors)\n",
            "\n",
            "3. [Hard] In K-Nearest Neighbors, what is the role of the 'k' parameter?\n",
            "   - Determines the number of nearest neighbors to consider when making a prediction\n",
            "   - Determines the number of features to consider when making a prediction\n",
            "   - Determines the maximum allowed distance between the new data point and its nearest neighbor\n",
            "   - Determines the type of distance metric to be used when calculating similarity between data points\n",
            "   (Correct Answer: Determines the number of nearest neighbors to consider when making a prediction)\n",
            "\n",
            "4. [Easy] What are the two main steps in the K-Nearest Neighbors algorithm for classification problems?\n",
            "   - Selecting the k neighbors and counting their votes for each class\n",
            "   - Normalizing the features and applying PCA\n",
            "   - Calculating the distance between the new data point and all training examples\n",
            "   - Applying the chosen distance metric and sorting the distances in descending order\n",
            "   (Correct Answer: Selecting the k neighbors and counting their votes for each class)\n",
            "\n",
            "5. [Medium] What is the difference between K-Nearest Neighbors and Support Vector Machines (SVM)?\n",
            "   - K-Nearest Neighbors is a simple and easy-to-implement algorithm, while SVM requires more computational resources and tuning\n",
            "   - K-Nearest Neighbors is a non-parametric method, while SVM is a parametric method\n",
            "   - K-Nearest Neighbors can handle both classification and regression tasks, while SVM is only suitable for classification\n",
            "   - Both K-Nearest Neighbors and SVM require the user to specify the number of nearest neighbors (k) to consider when making a prediction\n",
            "   (Correct Answer: Both K-Nearest Neighbors and SVM require the user to specify the number of nearest neighbors (k) to consider when making a prediction)\n",
            "\n",
            "6. [Hard] In K-Nearest Neighbors, which of the following statements is true about the prediction process for regression problems?\n",
            "   - The prediction is made based on the average of the labels of the k nearest neighbors\n",
            "   - The prediction is made based on the majority vote among the k nearest neighbors\n",
            "   - The prediction is made by finding the kth nearest neighbor and using its label as the predicted value\n",
            "   - The prediction is made by applying the chosen distance metric and sorting the distances in ascending order\n",
            "   (Correct Answer: The prediction is made based on the average of the labels of the k nearest neighbors)\n",
            "\n",
            "7. [Easy] Which of the following is NOT a disadvantage of K-Nearest Neighbors?\n",
            "   - Computationally expensive for large datasets\n",
            "   - Works well for low-dimensional datasets\n",
            "   - Requires careful selection of distance metrics and normalization of features\n",
            "   - Sensitive to the choice of k\n",
            "   (Correct Answer: Works well for low-dimensional datasets)\n",
            "\n",
            "8. [Medium] Which of the following is NOT a step in the K-Nearest Neighbors algorithm for classification problems?\n",
            "   - Calculating the distance between the new data point and all training examples\n",
            "   - Normalizing the features and applying PCA\n",
            "   - Selecting the k neighbors and counting their votes for each class\n",
            "   - Applying the chosen distance metric and sorting the distances in descending order\n",
            "   (Correct Answer: Normalizing the features and applying PCA)\n",
            "\n",
            "9. [Hard] In the example provided, why did the new flower get classified as Setosa?\n",
            "   - Because all three nearest neighbors belonged to the Setosa class\n",
            "   - Because the Euclidean distance was the smallest among all the other classes\n",
            "   - Because the majority vote among the k nearest neighbors was for Setosa\n",
            "   - Because the new flower had the shortest petal length among all the other flowers\n",
            "   (Correct Answer: Because all three nearest neighbors belonged to the Setosa class)\n",
            "\n",
            "10. [Easy] What is the purpose of selecting the k nearest neighbors in K-Nearest Neighbors?\n",
            "   - To reduce the dimensionality of the data\n",
            "   - To increase the generalization ability of the model\n",
            "   - To improve the computational efficiency of the algorithm\n",
            "   - To ensure that the nearest neighbors are from different classes\n",
            "   (Correct Answer: To improve the computational efficiency of the algorithm)\n",
            "\n",
            "---\n",
            "\n",
            "**Fill-in-the-blank questions:**\n",
            "\n",
            "1. [Easy] The _______ algorithm uses k nearest neighbors to make predictions.\n",
            "Answer: K-Nearest Neighbors\n",
            "\n",
            "2. [Medium] In K-Nearest Neighbors, the prediction is made based on the ________ of the k nearest neighbors.\n",
            "Answer: Majority vote (for classification problems) / Average (for regression problems)\n",
            "\n",
            "3. [Hard] One disadvantage of K-Nearest Neighbors is that it is computationally expensive for large datasets due to the need to calculate distances between every ________.\n",
            "Answer: pair of data points\n",
            "\n",
            "4. [Easy] The K-Nearest Neighbors algorithm requires the user to specify the number of nearest neighbors (k) to consider when making a prediction.\n",
            "Blank: ______\n",
            "\n",
            "5. [Medium] In the K-Nearest Neighbors algorithm, after calculating the distances between the new data point and all training examples, the distances are sorted in _______ order.\n",
            "Answer: Ascending\n",
            "\n",
            "6. [Hard] An advantage of K-Nearest Neighbors is that it can handle both _______ and regression tasks.\n",
            "Answer: Classification and regression\n",
            "\n",
            "7. [Easy] When using K-Nearest Neighbors for classification, if there is a tie in the number of votes for each class among the k nearest neighbors, the algorithm will predict the _______ class.\n",
            "Answer: Class with the lowest index\n",
            "\n",
            "8. [Medium] To improve the performance of K-Nearest Neighbors, it is important to carefully select the _______ and normalize the features.\n",
            "Answer: Distance metric\n",
            "\n",
            "9. [Hard] In the given example, suppose the new flower had a petal length of 1.6 instead of 1.5. If k remains the same, which class would it likely belong to? Explain why.\n",
            "Answer: It might belong to the Versicolor class because the new flower would be closer to the third nearest neighbor, which belongs to the Versicolor class.\n",
            "\n",
            "10. [Easy] The K-Nearest Neighbors algorithm is a _______ method, meaning it makes no assumptions about the underlying distribution of the data.\n",
            "Answer: Non-parametric\n",
            "\n",
            "---\n",
            "\n",
            "**Short-answer questions:**\n",
            "\n",
            "1. [Easy] Briefly explain the role of the 'k' parameter in K-Nearest Neighbors.\n",
            "Answer: The 'k' parameter determines the number of nearest neighbors to consider when making a prediction. A larger value of 'k' means considering more neighbors, potentially reducing the risk of misclassifying the new data point but increasing the computational cost. Conversely, a smaller value of 'k' means considering fewer neighbors, potentially increasing the risk of misclassifying the new data point but decreasing the computational cost.\n",
            "\n",
            "2. [Medium] Why is K-Nearest Neighbors sensitive to the choice of 'k'?\n",
            "Answer: K-Nearest Neighbors is sensitive to the choice of 'k' because an incorrect choice of 'k' can lead to overfitting or underfitting, affecting the accuracy of the predictions. Choosing too small a value of 'k' may result in overfitting, while choosing too large a value of 'k' may result in underfitting.\n",
            "\n",
            "3. [Hard] Describe the process of making a prediction using K-Nearest Neighbors for regression problems.\n",
            "Answer: To make a prediction using K-Nearest Neighbors for regression problems, the algorithm counts the number of votes for each possible value among the k nearest neighbors and takes the average of those values as the predicted value. This approach assumes that similar samples have similar values, and the prediction is an estimate of the mean value of the k nearest neighbors.\n",
            "\n",
            "4. [Easy] What is the difference between classification and regression tasks in the context of K-Nearest Neighbors?\n",
            "Answer: In classification tasks, the goal is to predict the class or category of a new data point, whereas in regression tasks, the goal is to predict a continuous numerical value, such as a real number.\n",
            "\n",
            "5. [Medium] Why is K-Nearest Neighbors considered a non-parametric method?\n",
            "Answer: K-Nearest Neighbors is considered a non-parametric method because it makes no assumptions about the underlying distribution of the data, unlike some other machine learning algorithms like linear regression or logistic regression, which assume a specific form for the data distribution.\n",
            "\n",
            "6. [Hard] Explain why K-Nearest Neighbors may be computationally expensive for large datasets.\n",
            "Answer: K-Nearest Neighbors may be computationally expensive for large datasets because it requires calculating the distance between the new data point and every training example in the dataset, which can become time-consuming and resource-intensive as the size of the dataset grows. This operation is known as the \"curse of dimensionality,\" where the complexity of the algorithm increases exponentially with the number of dimensions or features.\n",
            "\n",
            "7. [Easy] What are the advantages of using K-Nearest Neighbors for high-dimensional datasets?\n",
            "Answer: One advantage of using K-Nearest Neighbors for high-dimensional datasets is that it can still perform reasonably well even when the data is sparse or contains many irrelevant features. This is because K-Nearest Neighbors is a non-parametric method that does not rely on explicit assumptions about the data distribution or the relationships between the features.\n",
            "\n",
            "8. [Medium] Why is it important to carefully select the distance metric in K-Nearest Neighbors?\n",
            "Answer: It is important to carefully select the distance metric in K-Nearest Neighbors because the choice of distance metric affects the similarity between data points and, consequently, the predictions made by the algorithm. Different distance metrics may be better suited for different types of data and applications, so it is essential to choose a distance metric that accurately reflects the underlying structure of the data.\n",
            "\n",
            "9. [Hard] How does K-Nearest Neighbors differ from clustering algorithms like K-Means?\n",
            "Answer: K-Nearest Neighbors is a supervised learning algorithm that makes predictions based on labeled data, whereas K-Means is an unsupervised learning algorithm that groups data points into clusters without any prior knowledge of their labels. Additionally, K-Nearest Neighbors is used for both classification and regression tasks, while K-Means is primarily used for clustering tasks.\n",
            "\n",
            "10. [Easy] What are the potential drawbacks of using K-Nearest Neighbors?\n",
            "Answer: Some potential drawbacks of using K-Nearest Neighbors include its sensitivity to the choice of 'k', its computational expense for large datasets, and its requirement for careful selection of the distance metric and normalization of the features to ensure accurate results. Additionally, K-Nearest Neighbors may not scale well to very high-dimensional datasets due to the curse of dimensionality.\n",
            "\n",
            "11. [Medium] In what ways can K-Nearest Neighbors be improved or optimized?\n",
            "Answer: K-Nearest Neighbors can be improved or optimized in several ways, such as by using efficient data structures like kd-trees or ball trees to speed up the search for the k nearest neighbors, by weighting the contributions of the nearest neighbors based on their distance from the new data point, or by using dimensionality reduction techniques like principal component analysis (PCA) to reduce the number of features and alleviate the curse of dimensionality.\n",
            "\n",
            "12. [Hard] What is the concept of overfitting and underfitting in the context of K-Nearest Neighbors?\n",
            "Answer: Overfitting occurs when a machine learning model learns the noise or random fluctuations in the training data rather than the underlying pattern, leading to poor performance on new, unseen data. Underfitting, on the other hand, refers to a situation where the model is too simple to capture the complexity of the data, resulting in poor performance even on the training data. In the context of K-Nearest Neighbors, choosing an incorrect value of 'k' can lead to overfitting or underfitting, affecting the accuracy of the predictions.\n",
            "\n",
            "13. [Easy] What is the role of the new data point in the K-Nearest Neighbors algorithm?\n",
            "Answer: The new data point plays the role of the query or test point in the K-Nearest Neighbors algorithm. Its features are compared to the features of the training data to determine the k nearest neighbors, and these neighbors are then used to make a prediction about the class or value of the new data point.\n",
            "\n",
            "14. [Medium] What are some common distance metrics used in K-Nearest Neighbors?\n",
            "Answer: Some common distance metrics used in K-Nearest Neighbors include the Euclidean distance, Manhattan distance, Minkowski distance, and Mahalanobis distance. Each distance metric measures the similarity between data points in a different way, and the choice of distance metric depends on the nature of the data and the application at hand.\n",
            "\n",
            "15. [Hard] How can K-Nearest Neighbors be applied to image classification tasks?\n",
            "Answer: K-Nearest Neighbors can be applied to image classification tasks by representing each image as a vector of pixel intensities or features, such as color histograms or texture descriptors. Once the images are represented as vectors, they can be compared using a distance metric like Euclidean distance or Manhattan distance to find the k nearest neighbors, and the predicted class of the new image can be determined based on the majority vote among the k nearest neighbors. Alternatively, K-Nearest Neighbors can also be used for image retrieval tasks, where the goal is to find similar images based on their visual characteristics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABO-qH_snG76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}