{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#### 21i1667 , 21i1665 , 21i1662"
      ],
      "metadata": {
        "id": "CksT2KomP6Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain openai  -q\n",
        "!pip install sentence_transformers -q\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install -U langchain-community -q\n",
        "## libraries for images and slides\n",
        "# **\n",
        "!pip install pillow\n",
        "!pip install requests\n",
        "!pip install python-pptx\n",
        "!pip install gensim==3.8.3\n",
        "!pip install keybert\n",
        "!pip install requests Pillow\n",
        "!pip install google_images_download\n",
        "!pip install google-search-results\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt, Inches\n",
        "from PIL import Image\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import requests\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Initialize KeyBERT\n",
        "from keybert import KeyBERT\n",
        "kw_model = KeyBERT()\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "fovbQwT8IxAj",
        "outputId": "4a7ee105-9895-46e9-d074-228fb4cd26f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (11.0.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (3.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.12.2)\n",
            "Collecting gensim==3.8.3\n",
            "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart_open>=1.8.1->gensim==3.8.3) (1.17.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (3.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (4.12.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: google_images_download in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from google_images_download) (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google_images_download) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->google_images_download) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->google_images_download) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->google_images_download) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium->google_images_download) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->google_images_download) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google_images_download) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->google_images_download) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google_images_download) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google_images_download) (0.14.0)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqwKly8lTqzl",
        "outputId": "52d4a5ec-448f-421d-b42d-069e38b769d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents loaded: 7007\n",
            "Number of document chunks created: 35980\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "    # Create a list to hold all the loaded documents\n",
        "    documents = []\n",
        "\n",
        "    # Iterate over each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Construct the full file path\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            # Load the PDF file\n",
        "            pdf_loader = PyPDFLoader(file_path)\n",
        "            documents.extend(pdf_loader.load())  # Append the loaded documents\n",
        "\n",
        "    return documents\n",
        "\n",
        "def split_docs(documents, chunk_size=500, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    return docs\n",
        "\n",
        "# Load documents from the specified directory\n",
        "documents = load_docs(directory)\n",
        "print(f\"Number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "docs = split_docs(documents)\n",
        "print(f\"Number of document chunks created: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Suppress pypdf error logs if desired\n",
        "logging.getLogger(\"pypdf\").setLevel(logging.WARNING)\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                pdf_loader = PyMuPDFLoader(file_path)\n",
        "                loaded_docs = pdf_loader.load()\n",
        "                documents.extend(loaded_docs)\n",
        "                print(f\"Loaded {len(loaded_docs)} pages from {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {file_path}: {e}\")\n",
        "    return documents\n",
        "\n",
        "def split_docs(documents, chunk_size=500, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "# Load documents from the specified directory\n",
        "documents = load_docs(directory)\n",
        "print(f\"Total number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "docs = split_docs(documents)\n",
        "print(f\"Total number of document chunks created: {len(docs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ78iJ3fWWdV",
        "outputId": "aef38db3-93bc-4ef6-a3c4-3af991c8c705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1270 pages from Book - Tony Gaddis - Starting Out with C++.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Data Mining Practical Machine Learning Tools and Techniques 3rd Edition-Manteshbbbb.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 665 pages from Data Mining Practical Machine Learning Tools and Techniques 3rd Edition-Manteshbbbb.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 559 of document /content/data/Database_Systems.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1029 pages from Database_Systems.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Data Structures 1.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 738 pages from Data Structures 1.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 2 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 12 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 20 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 499 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 787 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1161 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1250 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1313 pages from Algo Book.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 801 pages from Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 11 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 29 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 85 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 139 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 141 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 161 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 273 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 299 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 301 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 395 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 419 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 491 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 501 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 587 pages from eisenstein-natural-language-processing.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 40 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 224 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 338 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 340 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 473 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 591 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 604 pages from Data Mining and Analysis by Zaki.pdf\n",
            "Total number of documents loaded: 7007\n",
            "Total number of document chunks created: 35122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t54HueEUW6-Q",
        "outputId": "1e6a7bb2-c140-4776-8d5c-4cac76f0e258",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Document 1:\n",
            "Credits and acknowledgments borrowed from other sources and reproduced, with permission, appear on the \n",
            "Credits page in the endmatter of this textbook.\n",
            "Copyright © 2015, 2012, 2009 Pearson Education, Inc., publishing as Addison-Wesley All rights reserved. \n",
            "Manufactured in the United States of America. This publication is protected by Copyright, and permission \n",
            "should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 2:\n",
            "transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To \n",
            "obtain permission(s) to use material from this work, please submit a written request to Pearson Education, \n",
            "Inc., Permissions Department, One Lake Street, Upper Saddle River, New Jersey 07458 or you may fax your \n",
            "request to 201 236-3290.\n",
            "Many of the designations by manufacturers and sellers to distinguish their products are claimed as trademarks.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 3:\n",
            "Where those designations appear in this book, and the publisher was aware of a trademark claim, the \n",
            "designations have been printed in initial caps or all caps.\n",
            "Library of Congress Cataloging-in-Publication Data\n",
            "Gaddis, Tony.\n",
            " Starting out with C++ : from control structures through objects/Tony Gaddis.—Eighth edition.\n",
            "  pages cm\n",
            " Includes bibliographical references and index.\n",
            " Online the following appendices are available at www.pearsonhighered.com/gaddis: Appendix D:\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 4:\n",
            "Introduction to fl  owcharting; Appendix E: Using UML in class design; Appendix F: Namespaces; Appendix G: \n",
            "Writing managed C++ code for the .net framework; Appendix H: Passing command line arguments; Appendix \n",
            "I: Header fi  le and library function reference; Appendix J: Binary numbers and bitwise operations; Appendix K: \n",
            "Multi-source fi  le programs; Appendix L: Stream member functions for formatting; Appendix M: Introduction\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 5:\n",
            "to Microsoft Visual C++ 2010 express edition; Appendix N: Answers to checkpoints; and Appendix O: \n",
            "Solutions to odd-numbered review questions.\n",
            " ISBN-13: 978-0-13-376939-5\n",
            " ISBN-10: 0-13-376939-9\n",
            " 1. C++ (Computer program language)  I. Title.  II. Title: From control structures through objects.\n",
            "  QA76.73.C153G33 2014b\n",
            "  005.13’3—dc23\n",
            "                                                            2014000213\n",
            "10  9  8  7  6  5  4  3  2  1   Editorial Director: Marcia Horton\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first 5 cleaned document chunks\n",
        "for i, doc in enumerate(docs[5:10]):\n",
        "    print(f\"Cleaned Document {i + 1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGCbi4niFSVX",
        "outputId": "fc750340-b477-4957-db26-5aaa170f237e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cleaned document chunks: 35980\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans a single text string by performing various cleaning steps,\n",
        "    including removing page numbers, headers, and footers.\n",
        "    \"\"\"\n",
        "    # Remove extra whitespaces, tabs, and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove patterns that typically indicate page numbers, e.g., \"Page 1\", \"1 of 10\", etc.\n",
        "    text = re.sub(r'\\bpage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Matches \"Page 1\"\n",
        "    text = re.sub(r'\\b\\d+\\s*(of|/)\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Matches \"1 of 10\" or \"1/10\"\n",
        "\n",
        "    # Remove common header/footer indicators (customize this part based on your documents)\n",
        "    text = re.sub(r'(header|footer|chapter\\s*\\d+|section\\s*\\d+)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove any lines that are too short (likely headers/footers, e.g., under 5 characters)\n",
        "    text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n",
        "\n",
        "    # Remove special characters (except common punctuation marks)\n",
        "    text = re.sub(r'[^\\w\\s.,?!]', '', text)\n",
        "\n",
        "    # Convert to lowercase (optional, depending on use case)\n",
        "    #text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_documents(documents):\n",
        "    \"\"\"\n",
        "    Cleans the text for each document in the list by removing headers, footers, and other noise.\n",
        "    \"\"\"\n",
        "    cleaned_docs = []\n",
        "    for doc in documents:\n",
        "        # Clean the content of the document\n",
        "        cleaned_content = clean_text(doc.page_content)\n",
        "        # Update the cleaned content in the document\n",
        "        doc.page_content = cleaned_content\n",
        "        cleaned_docs.append(doc)\n",
        "\n",
        "    return cleaned_docs\n",
        "\n",
        "# Clean the loaded document chunks\n",
        "cleaned_docs = clean_documents(docs)\n",
        "print(f\"Number of cleaned document chunks: {len(cleaned_docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PiPwt-FaYwl",
        "outputId": "c59f3051-543d-48c8-ec7d-d99a9d4df569",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Document 1:\n",
            "Starting    Eighth .  Gaddis Computer Science  University  Texas  Austin  . Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 2:\n",
            "Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 3:\n",
            " EIGHTH EDITION STARTING     Control Structures through Objects Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 4:\n",
            "  intentionally  blank Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 5:\n",
            " EIGHTH EDITION STARTING     Control Structures through Objects  Gaddis Haywood Community College Boston Columbus Indianapolis    Francisco Upper Saddle River Amsterdam   Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico   Paulo Sydney   Seoul Singapore Taipei Tokyo Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first 5 cleaned document chunks\n",
        "for i, doc in enumerate(cleaned_docs[:5]):\n",
        "    print(f\"Cleaned Document {i + 1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F5GY9voPa0av"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s2p3rUwvOvW",
        "outputId": "84061e22-bd2c-41ef-9e21-70e39e51bccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LXhIY5SrrRec",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vySq5oI5sU5V"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfIpYLV-acks",
        "outputId": "29e98238-16a2-423f-de60-9e243f4df912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to the index: newdata\n",
            "Successfully created or connected to the Langchain index: <langchain_community.vectorstores.pinecone.Pinecone object at 0x7ae105121150>\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
        "\n",
        "#074e0d9a-ab5e-48bf-8eae-9effae335521              This is the API to MYDB Insert this\n",
        "\n",
        "# Prompt for Pinecone API key if not set in the environment\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "# Retrieve the Pinecone API key from environment variables\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Define your index name\n",
        "index_name = \"newdata\"  # Change if desired\n",
        "\n",
        "# Check for existing indexes\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "# Create the index if it does not exist\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # Adjust this to match your embeddings' dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "    # Wait until the index is ready\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        print(\"Waiting for the index to be ready...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Print connection success message\n",
        "print(f\"Successfully connected to the index: {index_name}\")\n",
        "\n",
        "\n",
        "# Now create a Pinecone index for Langchain using the existing index\n",
        "langchain_index = LangchainPinecone.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        "\n",
        ")\n",
        "\n",
        "# Output to verify the index creation\n",
        "print(f\"Successfully created or connected to the Langchain index: {langchain_index}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nrbQeCnCT_P",
        "outputId": "04ea0ade-6b39-4f02-9f1f-ed5e41b9983f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indexes': [{'deletion_protection': 'disabled',\n",
              "              'dimension': 384,\n",
              "              'host': 'pdfdataload-j5j0jq7.svc.aped-4627-b74a.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'pdfdataload',\n",
              "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}},\n",
              "             {'deletion_protection': 'disabled',\n",
              "              'dimension': 3072,\n",
              "              'host': 'pdfdb-j5j0jq7.svc.aped-4627-b74a.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'pdfdb',\n",
              "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}},\n",
              "             {'deletion_protection': 'disabled',\n",
              "              'dimension': 384,\n",
              "              'host': 'newpdfdb-j5j0jq7.svc.aped-4627-b74a.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'newpdfdb',\n",
              "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}},\n",
              "             {'deletion_protection': 'disabled',\n",
              "              'dimension': 384,\n",
              "              'host': 'langchain-chatbot-j5j0jq7.svc.aped-4627-b74a.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'langchain-chatbot',\n",
              "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}},\n",
              "             {'deletion_protection': 'disabled',\n",
              "              'dimension': 384,\n",
              "              'host': 'newdata-j5j0jq7.svc.aped-4627-b74a.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'newdata',\n",
              "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5r7YLpbchAD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# **\n",
        "def get_similar_docs(query, k=20, score=True):\n",
        "    if score:\n",
        "        similar_docs = langchain_index.similarity_search_with_score(query, k=k)  # Use langchain_index\n",
        "    else:\n",
        "        similar_docs = langchain_index.similarity_search(query, k=k)  # Use langchain_index\n",
        "    return similar_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Set the Hugging Face API key directly inside the script\n",
        "HUGGINGFACE_API_TOKEN = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Replace with your chosen model\n",
        "\n",
        "# Construct the API URL\n",
        "HUGGINGFACE_API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "\n",
        "# Set up the headers with the authorization token\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HUGGINGFACE_API_TOKEN}\"\n",
        "}\n",
        "\n",
        "# Example query\n",
        "query = \"What is KNN and how does it work?\"\n",
        "\n",
        "# Assuming you have a function `get_similar_docs` defined\n",
        "similar_docs = get_similar_docs(query)\n",
        "\n",
        "# Prepare the context from similar_docs\n",
        "context = \"\\n\\n\".join([doc[0].page_content for doc in similar_docs])\n",
        "\n",
        "\n",
        "prompt = (\n",
        "    f\"Context:\\n{context}\\n\\n\"\n",
        "    f\"Question: {query}\\n\"\n",
        "    f\"Answer: Imagine you are teaching this topic to someone unfamiliar with it. Provide a detailed, easy-to-understand explanation that includes examples, comparisons, and context to help them fully grasp the concept.\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Function to query Hugging Face Inference API\n",
        "def query_huggingface_api(prompt, max_length=25000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(response.json())\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "# Function to extract answer from API response\n",
        "def extract_answer(api_response):\n",
        "    if isinstance(api_response, list) and len(api_response) > 0:\n",
        "        generated_text = api_response[0].get('generated_text', '')\n",
        "        # Assuming the model appends the answer after \"Answer:\"\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip() if \"Answer:\" in generated_text else generated_text.strip()\n",
        "        if len(answer) > 25000:\n",
        "            answer = answer[:25000] + \"...\"\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No answer generated.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate the answer using Hugging Face Inference API\n",
        "try:\n",
        "    api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "    answer = extract_answer(api_response)\n",
        "    print(\"Answer:\", answer)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))\n",
        "\n"
      ],
      "metadata": {
        "id": "sOJfoO9mCih4",
        "outputId": "87d51c8a-49aa-4e68-9796-09e2f3462c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Context:\\n.  KnuthMorrisPratt algorithm  ,        contains  information    compute .      depend  . Since  array EMhas  mentries, whereas  .entries,    factor    preprocessing   computing  rather  .  preﬁx function   pattern  preﬁx function EMfor  pattern encapsulates knowledge about   \\n\\n.. RECURRENT NEURAL NETWORK LANGUAGE MODELS  .. Kneser smoothing Kneser smoothing  based  absolute discounting,   redistributes  result  probability    different    backoff. Empirical evidence points  Kneser smoothing   state   language modeling Goodman, .  motivate Kneser smoothing, consider  example  recently visited . Which   following   likely Francisco orDuluth ?\\n\\n.  kthen , return return  position   modiﬁed entry  .insertBack , increment variable storing number  entries  return return  position   inserted entry  Algorithm erase Input    Output  foreach position .begin,. .  kthen .erase decrement variable storing number  entries  CodeFragment9. Algorithms  ,, anderase    stored    .\\n\\nAlgorithm . shows  pseudo   kernel means method.  starts   initial random partitioning   points  kclusters.   iteratively updates  cluster assignments  reassigning  point    closest   feature space  . ..  facilitate  distance  putation,  first computes  average kernel value,  ,  squared      ,  \\n\\n positive integer , determines  knumbers  Sthat  closest   median  . . LetXŒ1 andYŒ     arrays,  containing nnumbers already  sorted order.   . algorithm    median   2nelements  arrays XandY. . Professor   consulting    company, which  planning  large pipelinerunning    through     nwells.  company wants  connect\\n\\n             .. Exercises  .   implementation   atRankfunction   Fragment .  class NodeSequence ,  walked   front   . Present   efﬁcient implementation, which walks  whichever      closer  index . . Provide  details   array implementation     . . Suppose     kntotal accesses   elements    \\n\\n.  KnuthMorrisPratt algorithm    ﬁnish  proof   OMPUTE PREFIX FUNCTION computes EMcor rectly.   procedure  OMPUTE PREFIX FUNCTION ,   start    ation   forloop  lines ,    .  condition  enforced  lines         entered,   remains  ineach successive iteration because   . Lines  adjust    becomes\\n\\nnique, called chaining. . introduces  alternative method  resolvingcollisions, called  addressing. Collision resolution  chaining Inchaining ,  place   elements          linked ,  Figure . shows.  jcontains  pointer         stored elements      there    elements,  jcontains .\\n\\n normalized kernel matrix, ,   computed using   kernel function ,  ,, ,, Knhas  diagonal elements  . LetWdenote  diagonal matrix comprising  diagonal element    ,    ,  ............   ,   normalized kernel matrix    expressed compactl    where   diagonal matrix, defined  ,\\n\\n kDmin.   repeat    until   .  9return   procedure computes .    straightforward manner according    inition  equation ..  nested loops beginning  lines    considerall states   characters ,  lines   .     largest ksuch thatP .   starts   largest conceivable value  ,      . .   decreases kuntilPk, which  eventually occur,\\n\\n deﬁning  iforiD1      product      other  miDn1n2SOHniNUL1niC1SOHnk.        \\n\\n.  KnuthMorrisPratt algorithm  . ,       preﬁx  Pthat   sufﬁx  .  ewhile   lines  iterates through  values  ETXŒq0,  although Tifor every ETXŒq0,   never   qsuch   c141DTŒ .   terminates  qreaches ,   course     execute. Therefore, qD0at  ,   . .\\n\\n, meaning    start  dunits  currency ,   trade  drijunits  currency .  sequence  trades  entail  commission, which depends   number oftrades  .     commission    charged     trades.  ,  ckD0for    ,   problem  ﬁnding   sequence  exchanges  currency  currency nexhibits optimal  structure.     commissions ckare arbitrary values,   problem\\n\\n   ofdlgkepairs  numbers derived   niare relatively prime. . Modular arithmetic Informally,   think  modular arithmetic  arithmetic  usual   integers, except     working modulo ,  every result  replaced   element       NUL1gthat  equivalent  , modulo  ,  replaced  xmodn.  informal model sufﬁces   stick   operations\\n\\n ,...,  combined classifier, denoted , predicts  class    point xbymajority voting among  kclasses argmax  ,...,  binary classification, assuming   classes    ,,  combined classifier MKcan  expressed  simply    Bagging   reduce  variance, especially    classifiers  unstable,\\n\\napproach  called  knearest neighbors  approach  density estimation.   kernel density estimation,  density estimation    nonparametric approach. Given ,  number  neighbors,  estimate  density   follows    where   distance   itskth nearest neighbor,     volume   dimensional hypersphere centered  ,  radius . .. \\n\\n.   klnkD. implies . .\\n\\nRandDinduce  kcontingency table ,  called  confusion matrix , defined  follows , ciandya where ,.  count nijdenotes  number  points  predicted class  whose  label  . ,   denotes  number  cases where  classifier agrees    label .  remaining counts ,  ,  cases where  classifier   labels disagree. AccuracyPrecision\\n\\n  .  pseudocode below gives  KnuthMorrisPratt matching algorithm   procedure  ATCHER .    ,  procedure follows  FINITE AUTOMATON MATCHER ,   shall .  ATCHER calls   iliary procedure  OMPUTE PREFIX FUNCTION  compute .  ATCHER .   length length EMDCOMPUTE PREFIX FUNCTION .   number  characters matched 5foriD1ton       right  while  andPŒ  \\n\\n.Dkmodm  example,    table   mD12and    kD100,    .. Since  requires   single division operation, hashing  division  quite .  using  division method,  usually avoid certain values  .   example, mshould    power  , since   ,    .     lowestorder   . Unless     order  patterns  equally\\n\\nQuestion: What is KNN and how does it work?\\nAnswer: Imagine you are teaching this topic to someone unfamiliar with it. Provide a detailed, easy-to-understand explanation that includes examples, comparisons, and context to help them fully grasp the concept.\\n\\nK-Nearest Neighbors (KNN) is a popular machine learning algorithm used for classification and regression tasks. It's a simple yet effective technique based on the idea of finding the 'k' nearest data points to a new, unknown data point and making predictions based on those neighbors.\\n\\nHere's an analogy to help you understand KNN better: Imagine you are lost in a forest, and you want to find your way out. You don't have a map or GPS, but you do have some fellow hikers around you. Instead of relying solely on one person's advice, you ask several people (let's say 'k' people) who seem to be more familiar with the area than others. By taking an average of their directions, you increase the likelihood of reaching the exit. This is essentially what KNN does in the realm of machine learning!\\n\\nIn KNN, we have a set of labeled training data points, each represented by a feature vector (a list of numerical attributes). When encountering a new, unlabeled data point, we calculate its distance to all the existing training points and select the 'k' nearest ones. The 'k' value can vary depending on the specific problem, and choosing an appropriate 'k' is crucial for achieving good results.\\n\\nFor classification problems, we assign the most common class among the 'k' nearest neighbors to the new data point. For regression problems, we take the average of the target values from the 'k' nearest neighbors to make our prediction.\\n\\nLet's illustrate this with an example: Suppose we have a dataset of iris flowers, where each flower is described by four features (sepal length, sepal width, petal length, and petal width), and the class label (setosa, versicolor, or virginica). If we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' nearest ones. Based on the majority class among these 'k' neighbors, we might predict that the new flower belongs to the versicolor class.\\n\\nKNN is particularly useful when dealing with high-dimensional datasets, as it doesn't require complex models or assumptions about the underlying distribution of the data. However, it can be computationally expensive for large datasets due to the need to calculate pairwise distances between data points. To address this issue, various optimizations and alternative distance metrics have been proposed over time.\"}]\n",
            "Answer: Imagine you are teaching this topic to someone unfamiliar with it. Provide a detailed, easy-to-understand explanation that includes examples, comparisons, and context to help them fully grasp the concept.\n",
            "\n",
            "K-Nearest Neighbors (KNN) is a popular machine learning algorithm used for classification and regression tasks. It's a simple yet effective technique based on the idea of finding the 'k' nearest data points to a new, unknown data point and making predictions based on those neighbors.\n",
            "\n",
            "Here's an analogy to help you understand KNN better: Imagine you are lost in a forest, and you want to find your way out. You don't have a map or GPS, but you do have some fellow hikers around you. Instead of relying solely on one person's advice, you ask several people (let's say 'k' people) who seem to be more familiar with the area than others. By taking an average of their directions, you increase the likelihood of reaching the exit. This is essentially what KNN does in the realm of machine learning!\n",
            "\n",
            "In KNN, we have a set of labeled training data points, each represented by a feature vector (a list of numerical attributes). When encountering a new, unlabeled data point, we calculate its distance to all the existing training points and select the 'k' nearest ones. The 'k' value can vary depending on the specific problem, and choosing an appropriate 'k' is crucial for achieving good results.\n",
            "\n",
            "For classification problems, we assign the most common class among the 'k' nearest neighbors to the new data point. For regression problems, we take the average of the target values from the 'k' nearest neighbors to make our prediction.\n",
            "\n",
            "Let's illustrate this with an example: Suppose we have a dataset of iris flowers, where each flower is described by four features (sepal length, sepal width, petal length, and petal width), and the class label (setosa, versicolor, or virginica). If we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' nearest ones. Based on the majority class among these 'k' neighbors, we might predict that the new flower belongs to the versicolor class.\n",
            "\n",
            "KNN is particularly useful when dealing with high-dimensional datasets, as it doesn't require complex models or assumptions about the underlying distribution of the data. However, it can be computationally expensive for large datasets due to the need to calculate pairwise distances between data points. To address this issue, various optimizations and alternative distance metrics have been proposed over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SLIDES AND IMAGES"
      ],
      "metadata": {
        "id": "9NHaBS8pI_d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to scrape images from GeeksforGeeks using SerpApi\n",
        "def scrape_images(keyword, num_images=2):  # Limit to 1 image\n",
        "    api_key = \"41cf19594f02970e20e9362044f5605347e8e04ce0cf4a9614504c087d2bae2e\"  # Replace with your actual API key\n",
        "\n",
        "\n",
        "    ### 4fb26d3c8969e2792af095f4b95dc1270b0a86736c098c88ddaa72d771b76239    new 0/100\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": keyword,\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        images = []\n",
        "\n",
        "        if \"images_results\" in data:\n",
        "            for image in data[\"images_results\"]:\n",
        "                if 'geeksforgeeks' in image.get('source', '').lower():\n",
        "                    images.append(image[\"original\"])\n",
        "                    if len(images) >= num_images:  # Limit to the desired number of images\n",
        "                        break\n",
        "            return images\n",
        "\n",
        "        return []\n",
        "    else:\n",
        "        print(f\"Failed to retrieve images. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def generate_slide_title(text):\n",
        "    # Extract keywords or create a brief title from the text\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), top_n=5)\n",
        "    key_words = [keyword[0] for keyword in keywords]\n",
        "\n",
        "    return \" \".join(key_words[:2]).title()  # Use top 2 keywords as the title\n",
        "\n",
        "def generate_bullet_points(text):\n",
        "    # Split text into sentences and return as bullet points\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    bullets = [sentence for sentence in sentences if len(sentence) > 20]\n",
        "    return bullets\n",
        "\n",
        "import random\n",
        "def create_ppt_from_template(raw_text, templates, main_topic):\n",
        "    selected_template = random.choice(templates)\n",
        "    prs = Presentation(\"/content/template3.pptx\")\n",
        "\n",
        "    # Add main topic to the first slide\n",
        "    if prs.slides:\n",
        "        slide = prs.slides[0]\n",
        "        title = slide.shapes.title\n",
        "        title.text = main_topic\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(40)\n",
        "\n",
        "    # Improved text segmentation\n",
        "    paragraphs = raw_text.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    slide_sections = []\n",
        "    current_section = []\n",
        "    for paragraph in paragraphs:\n",
        "        if len(current_section) < 3:\n",
        "            current_section.append(paragraph)\n",
        "        else:\n",
        "            slide_sections.append(' '.join(current_section))\n",
        "            current_section = [paragraph]\n",
        "\n",
        "    if current_section:\n",
        "        slide_sections.append(' '.join(current_section))\n",
        "\n",
        "    # Limit to maximum 10 slides\n",
        "    slide_sections = slide_sections[:10]\n",
        "    used_images = set()  # Track used images to prevent repetition\n",
        "\n",
        "    for section in slide_sections:\n",
        "        slide_title = generate_slide_title(section)\n",
        "        bullets = generate_bullet_points(section)\n",
        "\n",
        "        # Add text slide\n",
        "        slide_layout = prs.slide_layouts[1]\n",
        "        slide = prs.slides.add_slide(slide_layout)\n",
        "\n",
        "        title = slide.shapes.title\n",
        "        title.text = slide_title\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(36)\n",
        "\n",
        "        textbox = slide.shapes.placeholders[1].text_frame\n",
        "        textbox.clear()\n",
        "        for bullet in bullets[:3]:  # Limit to 3 bullets per slide\n",
        "            p = textbox.add_paragraph()\n",
        "            p.text = bullet\n",
        "            p.font.size = Pt(20)\n",
        "\n",
        "        # Add corresponding image slide\n",
        "        images = scrape_images(slide_title, num_images=1)\n",
        "        for img_url in images:\n",
        "            if img_url not in used_images:\n",
        "                used_images.add(img_url)\n",
        "                try:\n",
        "                    response = requests.get(img_url)\n",
        "                    response.raise_for_status()\n",
        "\n",
        "                    img = Image.open(BytesIO(response.content))\n",
        "                    image_stream = BytesIO()\n",
        "                    img.save(image_stream, format=\"PNG\")\n",
        "                    image_stream.seek(0)\n",
        "\n",
        "                    img_slide_layout = prs.slide_layouts[6]\n",
        "                    img_slide = prs.slides.add_slide(img_slide_layout)\n",
        "\n",
        "                    slide_width = prs.slide_width\n",
        "                    slide_height = prs.slide_height\n",
        "                    img_width, img_height = img.size\n",
        "\n",
        "                    aspect_ratio = min((slide_width * 0.8) / img_width, (slide_height * 0.8) / img_height)\n",
        "                    new_width = int(img_width * aspect_ratio)\n",
        "                    new_height = int(img_height * aspect_ratio)\n",
        "\n",
        "                    left = (slide_width - new_width) / 2\n",
        "                    top = (slide_height - new_height) / 2\n",
        "\n",
        "                    img_slide.shapes.add_picture(image_stream, left, top, width=new_width, height=new_height)\n",
        "                    break  # Use only one unique image per section\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading image for '{slide_title}': {e}\")\n",
        "\n",
        "    # Save the presentation\n",
        "    prs.save(f\"/content/{main_topic}.pptx\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "templates = [\n",
        "    '/content/templates/template.pptx',\n",
        "    '/content/templates/template1.pptx',\n",
        "    '/content/templates/template2.pptx',\n",
        "    '/content/templates/template3.pptx'\n",
        "]  # List of template file paths\n",
        "\n",
        "sentences = answer.split('.')\n",
        "# Join sentences starting from the third sentence (index 2)\n",
        "updated_answer = '.'.join(sentences[2:]).lstrip('.')\n",
        "print(\"Updated Answer:\", updated_answer)\n",
        "\n",
        "raw_text = updated_answer\n",
        "main_topic = query.upper()\n",
        "\n",
        "# Create a presentation using one random template\n",
        "create_ppt_from_template(raw_text, templates, main_topic)\n",
        "\n"
      ],
      "metadata": {
        "id": "UVkXGQOAI98S",
        "outputId": "f17a3eeb-7194-4554-b72b-20756c545efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Answer: \n",
            "\n",
            "Context: In computer operating systems (OS), multiple processes can run simultaneously to improve system performance. However, when two or more processes compete for shared resources such as files, memory, or devices, conflicts can arise that may lead to deadlocks, where each process is waiting for a resource held by another process, causing all of them to be stuck and unable to proceed.\n",
            "\n",
            "Explanation: To understand deadlocks in OS, let's consider a simple example involving two processes, P1 and P2, and three shared resources, R1, R2, and R3. Each process requires specific resources to execute, as shown below:\n",
            "\n",
            "* Process P1: Requires R1 and R2\n",
            "* Process P2: Requires R2 and R3\n",
            "\n",
            "Assuming both processes have already acquired their initial required resources, they will enter a critical section where they perform their main tasks. After completing their work, they release the resources and exit their critical sections. However, suppose that while executing, the processes encounter the following sequence of events:\n",
            "\n",
            "1. P1 requests R2 and waits for it.\n",
            "2. P2 requests R2 and obtains it because P1 is waiting.\n",
            "3. P1 requests R3 and waits for it.\n",
            "4. P2 requests R1 and waits for it.\n",
            "\n",
            "At this point, both processes are blocked, waiting for a resource held by the other process. This situation constitutes a deadlock, and neither process can proceed until one or more of them are terminated.\n",
            "\n",
            "Comparing deadlocks to the database management system example: Although the underlying concepts are similar, the differences between deadlocks in OS and databases lie primarily in the nature of the shared resources and the methods used to resolve deadlocks. In a database system, the shared resources are data items, and the resolution often involves techniques like deadlock prevention or detection. In contrast, in an operating system, the shared resources can be files, memory segments, I/O devices, and so forth, and the resolution may involve techniques like priority scheduling, time slicing, or signal handling.\n",
            "\n",
            "Resolving deadlocks in OS: There are several strategies to address deadlocks in OS, including:\n",
            "\n",
            "1. Deadlock Prevention: Implementing rules to ensure that a deadlock cannot occur, such as requiring processes to acquire resources in a specific order. However, these rules can limit system flexibility and efficiency.\n",
            "2. Deadlock Detection: Periodically checking for the existence of a deadlock and taking appropriate action if one is found. This approach can cause unnecessary process termination, but it allows for maximum system flexibility.\n",
            "3. Resource starvation avoidance: Prioritizing processes or allocating resources dynamically to minimize the likelihood of a deadlock occurring.\n",
            "4. Timeout: Setting a limit on the amount of time a process can wait for a resource before assuming a deadlock has occurred and terminating the process.\n",
            "5. Preemptive resource allocation: Allowing the system to interrupt a process and reclaim its resources if necessary to resolve a deadlock.\n",
            "6. Deadlock recovery: Terminating one or more of the deadlocked processes and restarting them. This can be done using various strategies, such as choosing the process with the lowest priority, the longest execution time, or the shortest remaining work.\n",
            "\n",
            "In summary, deadlocks in operating systems occur when two or more processes are competing for shared resources and each is waiting for a resource held by the other. Deadlocks can lead to significant performance degradation, and various strategies, such as deadlock prevention, detection, and recovery, can be employed to address them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = answer.split('.')\n",
        "# Join sentences starting from the third sentence (index 2)\n",
        "updated_answer = '.'.join(sentences[2:]).lstrip('.')\n",
        "\n",
        "\n",
        "prompt1 = f\"\"\"\n",
        "You are a question-generation expert. Based on the following content, generate a variety of questions to assess understanding. Include:\n",
        "1. Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\n",
        "2. Fill-in-the-blank questions: Provide a sentence with a blank to be filled and indicate the correct answer and difficulty level.\n",
        "3. Short-answer questions: Provide questions that require concise answers (1-2 sentences) and include the correct answer and difficulty level.\n",
        "\n",
        "Content:\n",
        "{updated_answer}\n",
        "\n",
        "Generate the questions and answers with varying difficulty levels like Easy, Medium and Hard. The number of questions should be more than 10.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "try:\n",
        "    api_response = query_huggingface_api(prompt1, max_length=25000)\n",
        "    answer = extract_answer(api_response)\n",
        "    print(\"Answer:\", answer)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmjLxgAjj8ON",
        "outputId": "a8c1b47f-b8ef-4617-87bc-8338c21a0374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"\\nYou are a question-generation expert. Based on the following content, generate a variety of questions to assess understanding. Include:\\n1. Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\\n2. Fill-in-the-blank questions: Provide a sentence with a blank to be filled and indicate the correct answer and difficulty level.\\n3. Short-answer questions: Provide questions that require concise answers (1-2 sentences) and include the correct answer and difficulty level.\\n\\nContent:\\n\\n\\nK-Nearest Neighbors (KNN) is a popular machine learning algorithm used for classification and regression tasks. It's a simple yet effective technique based on the idea of finding the 'k' nearest data points to a new, unknown data point and making predictions based on those neighbors.\\n\\nHere's an analogy to help you understand KNN better: Imagine you are lost in a forest, and you want to find your way out. You don't have a map or GPS, but you do have some fellow hikers around you. Instead of relying solely on one person's advice, you ask several people (let's say 'k' people) who seem to be more familiar with the area than others. By taking an average of their directions, you increase the likelihood of reaching the exit. This is essentially what KNN does in the realm of machine learning!\\n\\nIn KNN, we have a set of labeled training data points, each represented by a feature vector (a list of numerical attributes). When encountering a new, unlabeled data point, we calculate its distance to all the existing training points and select the 'k' nearest ones. The 'k' value can vary depending on the specific problem, and choosing an appropriate 'k' is crucial for achieving good results.\\n\\nFor classification problems, we assign the most common class among the 'k' nearest neighbors to the new data point. For regression problems, we take the average of the target values from the 'k' nearest neighbors to make our prediction.\\n\\nLet's illustrate this with an example: Suppose we have a dataset of iris flowers, where each flower is described by four features (sepal length, sepal width, petal length, and petal width), and the class label (setosa, versicolor, or virginica). If we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' nearest ones. Based on the majority class among these 'k' neighbors, we might predict that the new flower belongs to the versicolor class.\\n\\nKNN is particularly useful when dealing with high-dimensional datasets, as it doesn't require complex models or assumptions about the underlying distribution of the data. However, it can be computationally expensive for large datasets due to the need to calculate pairwise distances between data points. To address this issue, various optimizations and alternative distance metrics have been proposed over time.\\n\\nGenerate the questions and answers with varying difficulty levels like Easy, Medium and Hard. The number of questions should be more than 10.\\n\\nEasy Questions:\\n1. MCQ: What is K-Nearest Neighbors (KNN) used for?\\n   - A) Image recognition\\n   - B) Text analysis\\n   - C) Classification and regression tasks\\n   - D) Time series forecasting\\n   Correct Answer: C\\n\\n2. MCQ: Which of the following best describes the concept of KNN?\\n   - A) Finding the single closest data point to a new, unknown data point\\n   - B) Taking the average of the target values from the 'k' nearest neighbors\\n   - C) Selecting the 'k' nearest data points and making predictions based on them\\n   - D) Calculating the mean distance between all data points\\n   Correct Answer: C\\n\\n3. Fill-in-the-blank: KNN is a popular machine learning algorithm used for _______ and ________ tasks.\\n   - A) Clustering, density estimation\\n   - B) Regression, classification\\n   - C) Principal component analysis, dimensionality reduction\\n   - D) Anomaly detection, time series forecasting\\n   Correct Answer: B\\n\\nMedium Questions:\\n1. MCQ: How does KNN work for regression problems?\\n   - A) We assign the most common class among the 'k' nearest neighbors\\n   - B) We take the average of the target values from the 'k' nearest neighbors\\n   - C) We use decision trees to make predictions based on the 'k' nearest neighbors\\n   - D) We calculate the mean distance between all data points\\n   Correct Answer: B\\n\\n2. Fill-in-the-blank: In the iris flower dataset example, if we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' ______ ones.\\n   - A) closest\\n   - B) farthest\\n   - C) average\\n   - D) majority\\n   Correct Answer: A\\n\\n3. Short-answer: Explain how KNN helps in solving classification problems.\\n   Answer: In KNN, we calculate the distances between a new, unlabeled data point and all existing labeled data points. We then select the 'k' nearest neighbors and assign the most common class among these 'k' neighbors to the new data point. This method helps solve classification problems by making predictions based on similarities between the new data point and its neighbors.\\n   Difficulty Level: Medium\\n\\nHard Questions:\\n1. MCQ: Why is choosing an appropriate 'k' value important in KNN?\\n   - A) Because it affects the speed of the algorithm\\n   - B) Because it determines the complexity of the model\\n   - C) Because it impacts the accuracy of the predictions\\n   - D) Because it decides the dimension of the data\\n   Correct Answer: C\\n\\n2. Fill-in-the-blank: KNN can be computationally expensive for large datasets due to the need to calculate _______ distances between data points.\\n   - A) average\\n   - B) mean\\n   - C) Euclidean\\n   - D) pairwise\\n   Correct Answer: D\\n\\n3. Short-answer: Discuss the main advantage of using KNN for high-dimensional datasets.\\n   Answer: KNN is advantageous for high-dimensional datasets because it doesn't require complex models or assumptions about the underlying distribution of the data. This makes it a flexible and relatively straightforward approach for handling large numbers of dimensions.\\n   Difficulty Level: Hard\"}]\n",
            "Answer: KNN is advantageous for high-dimensional datasets because it doesn't require complex models or assumptions about the underlying distribution of the data. This makes it a flexible and relatively straightforward approach for handling large numbers of dimensions.\n",
            "   Difficulty Level: Hard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = api_response[0][\"generated_text\"] + answer"
      ],
      "metadata": {
        "id": "bYd8xvlQnvqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer.split('\\n')\n"
      ],
      "metadata": {
        "id": "_l_1GdPso8aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in answer:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Yf86-1pqox",
        "outputId": "b5b736d8-7975-492b-82a3-99f9b7326696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a question-generation expert. Based on the following content, generate a variety of questions to assess understanding. Include:\n",
            "1. Multiple-choice questions (MCQs): Provide 4 answer options, one of which is correct. Indicate the correct option and difficulty level (easy, medium, hard).\n",
            "2. Fill-in-the-blank questions: Provide a sentence with a blank to be filled and indicate the correct answer and difficulty level.\n",
            "3. Short-answer questions: Provide questions that require concise answers (1-2 sentences) and include the correct answer and difficulty level.\n",
            "\n",
            "Content:\n",
            "\n",
            "\n",
            "K-Nearest Neighbors (KNN) is a popular machine learning algorithm used for classification and regression tasks. It's a simple yet effective technique based on the idea of finding the 'k' nearest data points to a new, unknown data point and making predictions based on those neighbors.\n",
            "\n",
            "Here's an analogy to help you understand KNN better: Imagine you are lost in a forest, and you want to find your way out. You don't have a map or GPS, but you do have some fellow hikers around you. Instead of relying solely on one person's advice, you ask several people (let's say 'k' people) who seem to be more familiar with the area than others. By taking an average of their directions, you increase the likelihood of reaching the exit. This is essentially what KNN does in the realm of machine learning!\n",
            "\n",
            "In KNN, we have a set of labeled training data points, each represented by a feature vector (a list of numerical attributes). When encountering a new, unlabeled data point, we calculate its distance to all the existing training points and select the 'k' nearest ones. The 'k' value can vary depending on the specific problem, and choosing an appropriate 'k' is crucial for achieving good results.\n",
            "\n",
            "For classification problems, we assign the most common class among the 'k' nearest neighbors to the new data point. For regression problems, we take the average of the target values from the 'k' nearest neighbors to make our prediction.\n",
            "\n",
            "Let's illustrate this with an example: Suppose we have a dataset of iris flowers, where each flower is described by four features (sepal length, sepal width, petal length, and petal width), and the class label (setosa, versicolor, or virginica). If we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' nearest ones. Based on the majority class among these 'k' neighbors, we might predict that the new flower belongs to the versicolor class.\n",
            "\n",
            "KNN is particularly useful when dealing with high-dimensional datasets, as it doesn't require complex models or assumptions about the underlying distribution of the data. However, it can be computationally expensive for large datasets due to the need to calculate pairwise distances between data points. To address this issue, various optimizations and alternative distance metrics have been proposed over time.\n",
            "\n",
            "Generate the questions and answers with varying difficulty levels like Easy, Medium and Hard. The number of questions should be more than 10.\n",
            "\n",
            "Easy Questions:\n",
            "1. MCQ: What is K-Nearest Neighbors (KNN) used for?\n",
            "   - A) Image recognition\n",
            "   - B) Text analysis\n",
            "   - C) Classification and regression tasks\n",
            "   - D) Time series forecasting\n",
            "   Correct Answer: C\n",
            "\n",
            "2. MCQ: Which of the following best describes the concept of KNN?\n",
            "   - A) Finding the single closest data point to a new, unknown data point\n",
            "   - B) Taking the average of the target values from the 'k' nearest neighbors\n",
            "   - C) Selecting the 'k' nearest data points and making predictions based on them\n",
            "   - D) Calculating the mean distance between all data points\n",
            "   Correct Answer: C\n",
            "\n",
            "3. Fill-in-the-blank: KNN is a popular machine learning algorithm used for _______ and ________ tasks.\n",
            "   - A) Clustering, density estimation\n",
            "   - B) Regression, classification\n",
            "   - C) Principal component analysis, dimensionality reduction\n",
            "   - D) Anomaly detection, time series forecasting\n",
            "   Correct Answer: B\n",
            "\n",
            "Medium Questions:\n",
            "1. MCQ: How does KNN work for regression problems?\n",
            "   - A) We assign the most common class among the 'k' nearest neighbors\n",
            "   - B) We take the average of the target values from the 'k' nearest neighbors\n",
            "   - C) We use decision trees to make predictions based on the 'k' nearest neighbors\n",
            "   - D) We calculate the mean distance between all data points\n",
            "   Correct Answer: B\n",
            "\n",
            "2. Fill-in-the-blank: In the iris flower dataset example, if we encounter a new flower with measurements (5.3, 3.0, 1.6, 0.5), we would calculate its distances to all the existing flowers in the dataset and choose the 'k' ______ ones.\n",
            "   - A) closest\n",
            "   - B) farthest\n",
            "   - C) average\n",
            "   - D) majority\n",
            "   Correct Answer: A\n",
            "\n",
            "3. Short-answer: Explain how KNN helps in solving classification problems.\n",
            "   Answer: In KNN, we calculate the distances between a new, unlabeled data point and all existing labeled data points. We then select the 'k' nearest neighbors and assign the most common class among these 'k' neighbors to the new data point. This method helps solve classification problems by making predictions based on similarities between the new data point and its neighbors.\n",
            "   Difficulty Level: Medium\n",
            "\n",
            "Hard Questions:\n",
            "1. MCQ: Why is choosing an appropriate 'k' value important in KNN?\n",
            "   - A) Because it affects the speed of the algorithm\n",
            "   - B) Because it determines the complexity of the model\n",
            "   - C) Because it impacts the accuracy of the predictions\n",
            "   - D) Because it decides the dimension of the data\n",
            "   Correct Answer: C\n",
            "\n",
            "2. Fill-in-the-blank: KNN can be computationally expensive for large datasets due to the need to calculate _______ distances between data points.\n",
            "   - A) average\n",
            "   - B) mean\n",
            "   - C) Euclidean\n",
            "   - D) pairwise\n",
            "   Correct Answer: D\n",
            "\n",
            "3. Short-answer: Discuss the main advantage of using KNN for high-dimensional datasets.\n",
            "   Answer: KNN is advantageous for high-dimensional datasets because it doesn't require complex models or assumptions about the underlying distribution of the data. This makes it a flexible and relatively straightforward approach for handling large numbers of dimensions.\n",
            "   Difficulty Level: HardKNN is advantageous for high-dimensional datasets because it doesn't require complex models or assumptions about the underlying distribution of the data. This makes it a flexible and relatively straightforward approach for handling large numbers of dimensions.\n",
            "   Difficulty Level: Hard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OfoSSWYyvYhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}